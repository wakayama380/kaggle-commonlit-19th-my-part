{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1695442147701,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"VAH4GU3udQLE","outputId":"1f187c1d-fcfe-459d-a1fd-b3cacd145e0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Sep 23 04:09:07 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"VAH4GU3udQLE"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88755,"status":"ok","timestamp":1695442236450,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"y7YWu2B1dPO_","outputId":"c3633274-7c5f-4993-c992-d9cdd3d95bff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"y7YWu2B1dPO_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QniqU23EOe1O"},"outputs":[],"source":["from google.colab import runtime\n","\n"],"id":"QniqU23EOe1O"},{"cell_type":"code","execution_count":null,"metadata":{"id":"16c51a23"},"outputs":[],"source":[],"id":"16c51a23"},{"cell_type":"markdown","metadata":{"id":"d2e3dae0"},"source":["# CFG"],"id":"d2e3dae0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d22dfc09"},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    exp='exp085'\n","    is_exp=False\n","    wandb=False\n","    debug=False\n","    apex=True\n","    print_freq=20\n","    num_workers=4\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=True\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps_rate=0.1\n","    epochs=3\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.98)\n","    batch_size=8\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0, 1, 2, 3]\n","    train=True\n","    awp=False\n","    nth_awp_start_epoch= 3\n","    adv_lr = 1e-4\n","    adv_eps = 1e-2\n","    eval_steps =70\n","    save_strategy='epoch'\n","    pooling='ConcatPooling'\n","    n_layers=10\n","    freeze=True\n","    freeze_top_num_layer=14\n","    lr_weight_decay=0.95\n","    reinit=False\n","\n","\n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]"],"id":"d22dfc09"},{"cell_type":"markdown","metadata":{"id":"NhFDBpBR3vGg"},"source":["# Directory settings"],"id":"NhFDBpBR3vGg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJYVrhufgVPh"},"outputs":[],"source":["# ====================================================\n","# Directory settings\n","# ====================================================\n","import os\n","\n","OUTPUT_DIR = f'/content/drive/MyDrive/Kaggle/outputs/{CFG.exp}/'\n","\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n"],"id":"NJYVrhufgVPh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eedc45a5"},"outputs":[],"source":["# ====================================================\n","# wandb\n","# ====================================================\n","if CFG.wandb:\n","\n","    import wandb\n","\n","    try:\n","        from kaggle_secrets import UserSecretsClient\n","        user_secrets = UserSecretsClient()\n","        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        wandb.login(key=secret_value_0)\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n","\n","\n","    def class2dict(f):\n","        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","    run = wandb.init(project='FB3-Public',\n","                     name=CFG.model,\n","                     config=class2dict(CFG),\n","                     group=CFG.model,\n","                     job_type=\"train\",\n","                     anonymous=anony)"],"id":"eedc45a5"},{"cell_type":"markdown","metadata":{"id":"bec24bf5"},"source":["# Library"],"id":"bec24bf5"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36259,"status":"ok","timestamp":1695442274627,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"f72fdd29","outputId":"2fcb7122-d020-4cea-f01a-7f38fa0ab0e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.31.0\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0)\n","  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.31.0\n","Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.10/dist-packages (0.13.3)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","tokenizers.__version__: 0.13.3\n","transformers.__version__: 4.31.0\n","env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","os.system('pip install iterative-stratification==0.1.7')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","#os.system('pip install -q transformers')\n","!pip install transformers==4.31.0\n","os.system('pip install -q tokenizers')\n","!pip install tokenizers==0.13.3\n","!pip install sentencepiece\n","\n","\n","import tokenizers\n","import transformers\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"f72fdd29"},{"cell_type":"markdown","metadata":{"id":"179b542c"},"source":["# Utils"],"id":"179b542c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9795dd2"},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores\n","\n","\n","def get_logger(filename=OUTPUT_DIR+'train'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything(seed=42)"],"id":"f9795dd2"},{"cell_type":"markdown","metadata":{"id":"fac79d7b"},"source":["# Data Loading"],"id":"fac79d7b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":662},"executionInfo":{"elapsed":4547,"status":"ok","timestamp":1695442279168,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"add36693","outputId":"4fa90a06-e8d4-4a6c-b3a0-fe0cdd1258bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["train.shape: (7165, 8)\n"]},{"output_type":"display_data","data":{"text/plain":["     student_id prompt_id                                               text   content   wording                                    prompt_question               prompt_title                                        prompt_text\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...  0.205683  0.380538  Summarize how the Third Wave developed over su...             The Third Wave  Background \\r\\nThe Third Wave experiment took ...\n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme... -0.548304  0.506755  Summarize the various ways the factory would u...    Excerpt from The Jungle  With one member trimming beef in a cannery, an...\n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...  3.128928  4.231226  In complete sentences, summarize the structure...  Egyptian Social Structure  Egyptian society was structured like a pyramid...\n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we... -0.210614 -0.471415  In complete sentences, summarize the structure...  Egyptian Social Structure  Egyptian society was structured like a pyramid...\n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...  3.272894  3.219757  Summarize how the Third Wave developed over su...             The Third Wave  Background \\r\\nThe Third Wave experiment took ..."],"text/html":["\n","  <div id=\"df-19678975-17aa-43d9-8243-8de2a22e2b98\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19678975-17aa-43d9-8243-8de2a22e2b98')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-19678975-17aa-43d9-8243-8de2a22e2b98 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-19678975-17aa-43d9-8243-8de2a22e2b98');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f69f26cb-5fa9-4f2a-8f63-0a6708f18661\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f69f26cb-5fa9-4f2a-8f63-0a6708f18661')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f69f26cb-5fa9-4f2a-8f63-0a6708f18661 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["test.shape: (4, 6)\n"]},{"output_type":"display_data","data":{"text/plain":["     student_id prompt_id            text prompt_question     prompt_title       prompt_text\n","0  000000ffffff    abc123  Example text 1    Summarize...  Example Title 1  Heading\\nText...\n","1  111111eeeeee    def789  Example text 2    Summarize...  Example Title 2  Heading\\nText...\n","2  222222cccccc    abc123  Example text 3    Summarize...  Example Title 1  Heading\\nText...\n","3  333333dddddd    def789  Example text 4    Summarize...  Example Title 2  Heading\\nText..."],"text/html":["\n","  <div id=\"df-628138e4-f673-453b-9e32-fd353bba2e75\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-628138e4-f673-453b-9e32-fd353bba2e75')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-628138e4-f673-453b-9e32-fd353bba2e75 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-628138e4-f673-453b-9e32-fd353bba2e75');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-223d7f66-fb71-46cc-bb41-fb0b4753b183\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-223d7f66-fb71-46cc-bb41-fb0b4753b183')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-223d7f66-fb71-46cc-bb41-fb0b4753b183 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["submission.shape: (4, 3)\n"]},{"output_type":"display_data","data":{"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"],"text/html":["\n","  <div id=\"df-62de463c-8173-44b2-ae78-a5a817113c55\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62de463c-8173-44b2-ae78-a5a817113c55')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-62de463c-8173-44b2-ae78-a5a817113c55 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-62de463c-8173-44b2-ae78-a5a817113c55');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a90b9e13-82bf-4279-b83b-3d3dec367b53\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a90b9e13-82bf-4279-b83b-3d3dec367b53')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a90b9e13-82bf-4279-b83b-3d3dec367b53 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}],"source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","input_path = '/content/drive/MyDrive/Kaggle/inputs/'\n","train = pd.read_csv(input_path+'summaries_train.csv')\n","test = pd.read_csv(input_path+'summaries_test.csv')\n","submission = pd.read_csv(input_path+'sample_submission.csv')\n","prompt_train = pd.read_csv(input_path+'prompts_train.csv')\n","prompt_test = pd.read_csv(input_path+'prompts_test.csv')\n","train = pd.merge(train,prompt_train,how='left',on='prompt_id')\n","test = pd.merge(test,prompt_test,how='left',on='prompt_id')\n","print(f\"train.shape: {train.shape}\")\n","display(train.head())\n","print(f\"test.shape: {test.shape}\")\n","display(test.head())\n","print(f\"submission.shape: {submission.shape}\")\n","display(submission.head())"],"id":"add36693"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KzAhY4R3zRK"},"outputs":[],"source":["# oof_df=pd.read_pickle(input_path+'oof_df.pkl')"],"id":"2KzAhY4R3zRK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a37a56e4"},"outputs":[],"source":["train['text'] = train['text']+' [SEP] '+train['prompt_text'].str[:6000]\n","test['text'] =  test['text']+' [SEP] '+test['prompt_text'].str[:6000]\n","\n","#################################################\n","# prompt_textも\n","#################################################\n","\n","# # \"text\"列の長さを計算して新しい列\"length\"に追加\n","# train['length'] = train['text'].apply(len)\n","# # \"text\"列の先頭に\"length\"列の値を結合\n","# train['text'] = train['length'].astype(str) + '[SEP]' + train['prompt_question'] + '[SEP]' +train['prompt_title'] + 'summary(' + train['text'] +') [SEP] source of summary('+train['prompt_text']+')'\n","\n","# # \"text\"列の長さを計算して新しい列\"length\"に追加\n","# test['length'] = test['text'].apply(len)\n","# # \"text\"列の先頭に\"length\"列の値を結合\n","# test['text'] = test['length'].astype(str) + '[SEP]' + test['prompt_question'] + '[SEP]' +test['prompt_title'] + 'summary(' + test['text'] +') [SEP] source of summary('+test['prompt_text']+')'\n"],"id":"a37a56e4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTjtOHdZERdZ"},"outputs":[],"source":[],"id":"OTjtOHdZERdZ"},{"cell_type":"markdown","metadata":{"id":"bfcf2f21"},"source":["# CV split"],"id":"bfcf2f21"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1695442279169,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"fe773f94","outputId":"2050f1ba-2b72-460b-c611-7e1076ec5252"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    1103\n","1    2057\n","2    2009\n","3    1996\n","dtype: int64"]},"metadata":{}}],"source":["# ====================================================\n","# CV split\n","# ====================================================\n","# Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","# for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n","#     train.loc[val_index, 'fold'] = int(n)\n","id2fold = {\n","    \"814d6b\": 1,\n","    \"39c16e\": 0,\n","    \"3b9047\": 2,\n","    \"ebad26\": 3,\n","}\n","\n","train[\"fold\"] = train[\"prompt_id\"].map(id2fold)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"],"id":"fe773f94"},{"cell_type":"code","execution_count":null,"metadata":{"id":"26ece1cd"},"outputs":[],"source":["if CFG.debug:\n","    display(train.groupby('fold').size())\n","    train = train.sample(n=3000, random_state=0).reset_index(drop=True)\n","    display(train.groupby('fold').size())"],"id":"26ece1cd"},{"cell_type":"markdown","metadata":{"id":"478fb80d"},"source":["# tokenizer"],"id":"478fb80d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1695442279170,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"GLyHNCkSh0CG","outputId":"37f0dc8f-8276-4476-aa90-4463acc6b5c3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'microsoft/deberta-v3-large'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["CFG.model"],"id":"GLyHNCkSh0CG"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":149,"referenced_widgets":["0a5b39311f434ea486713a0125879fc5","877381a3e38d45c087e9b93e5a9468a4","9e29079cd88542b49b34ae8f2885516b","1512c590b6a842f8a12fc579985a7ae1","714c037a51c642d9b6a01382c9278ecf","5f9305f92f7a4d84bfd8706c9ad46f14","e6354c876344490da54dc6f451c8a174","a1c03f21654b45d7a6134bed41357cb4","86bf46146cea4fc7b5b45c17229147ec","8271313f1d284478ab248a2da8683f4d","af45fd5e4e24436ebcce6578f4716bd9","71d42c3db6494d888f9e561addaab2bd","1c043c2d840d4a208cb3c6e10dc421c6","371bdfe7e857493aba6821de6a10c1db","3c7823b52afa4fb6b2bea46e9e1eddad","69619edb3ed14be9ba13337aeba46a48","ed5176b28e334451bc77782e9c86333e","447a22d2bd3b4b9eba6bdb32ebbe34a1","debed52ef0a6479d961f1ef409d5cb8b","6ae394a0ad954746ab230fddbdd524e2","56a84ce0c903412faca81e2cf3e22b1d","5fc6942fe6234ae8971ac2a1b8fa76ad","66591df0ba614001acf7621c6f152fd5","2e3fbded355743df92881b82de4e9af5","ca441ab9b682474bb4ae7df589e471d6","4ccfef89c58d4f4496d9749d88c698f9","dfae7f292d5a4ab9a27bd91c4e507df8","4b8b9a3f77854935be8ff1db50e4c581","0838dc20191941a888df4ce01400c4b3","54e32399903a403284c70dee18603b10","92e7056f26d34eb0a578f533de8233a2","f94e87f8bd2047b4b55beb82f86588f2","a29def8575704381a5bdd92ca704ccce"]},"executionInfo":{"elapsed":2991,"status":"ok","timestamp":1695442282144,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"1a9ff4aa","outputId":"945c4853-cfa0-41f0-83ee-475764ce1138"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5b39311f434ea486713a0125879fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d42c3db6494d888f9e561addaab2bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66591df0ba614001acf7621c6f152fd5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# ====================================================\n","# tokenizer\n","# ====================================================\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","CFG.tokenizer = tokenizer"],"id":"1a9ff4aa"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1695442282145,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"cQZWV8VaE0xO","outputId":"c5d17f6f-81db-4b39-eac9-e236a5e192bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'In Egypt, there were many occupations and social classes involved in day-to-day living. In many instances if you were at the bottom of the social ladder you could climb up, you didn\\'t have to stay a peasant you could work to bring your status up. Everyone worshipped the gods Ra, Osiris, and Isis, but also they would worship their pharaohs like gods as well. Under the pharaohs were the priests, they had the responsibility to entertain or please the said god. The Chain of Command was placed to keep everyone in check, not one person could handle all the civilians and treasures without any aid. Like the tax collector, called a vizier like stated they were in charge of collecting the peoples\\' tax. They were also one of the rare instances who were able to read and write, that\\'s how they were granted \"vizier\" Also the soldiers did many things as they would fight in wars or \"quelled domestic uprisings\". They were in charge of getting the slaves, farmers, and peasants to build palaces or the famous ancient pyramids. More skilled hardworking workers had occupations of craftsmen or women and physicians. This would mostly make up the middle-class people. The creative craftsmen would often make jewelry, papyrus products, pottery, tools, and many useful things people may need . Of course, you would need merchants to sell the goods to people who would pay for it. [SEP] Egyptian society was structured like a pyramid. At the top were the gods, such as Ra, Osiris, and Isis. Egyptians believed that the gods controlled the universe. Therefore, it was important to keep them happy. They could make the Nile overflow, cause famine, or even bring death. \\r\\nThe Egyptians also elevated some human beings to gods. Their leaders, called pharaohs, were believed to be gods in human form. They had absolute power over their subjects. After pharaohs died, huge stone pyramids were built as their tombs. Pharaohs were buried in chambers within the pyramids. \\r\\nBecause the people of Egypt believed that their pharaohs were gods, they entrusted their rulers with many responsibilities. Protection was at the top of the list. The pharaoh directed the army in case of a foreign threat or an internal conflict. All laws were enacted at the discretion of the pharaoh. Each farmer paid taxes in the form of grains, which were stored in the pharaoh’s warehouses. This grain was used to feed the people in the event of a famine. \\r\\nThe Chain of Command \\r\\nNo single person could manage all these duties without assistance. The pharaoh appointed a chief minister called a vizier as a supervisor. The vizier ensured that taxes were collected. \\r\\nWorking with the vizier were scribes who kept government records. These high-level employees had mastered a rare skill in ancient Egypt — they could read and write. \\r\\nNoble Aims \\r\\nRight below the pharaoh in status were powerful nobles and priests. Only nobles could hold government posts; in these positions they profited from tributes paid to the pharaoh. Priests were responsible for pleasing the gods. \\r\\nNobles enjoyed great status and also grew wealthy from donations to the gods. All Egyptians—from pharaohs to farmers—gave gifts to the gods. \\r\\nSoldier On \\r\\nSoldiers fought in wars or quelled domestic uprisings. During long periods of peace, soldiers also supervised the peasants, farmers, and slaves who were involved in building such structures as pyramids and palaces. \\r\\nSkilled workers such as physicians and craftsmen/women made up the middle class. Craftsmen made and sold jewelry, pottery, papyrus products, tools, and other useful things. \\r\\nNaturally, there were people needed to buy goods from artisans and traders. These were the merchants and storekeepers who sold these goods to the public. \\r\\nThe Bottom of the Heap \\r\\nAt the bottom of the social structure were slaves and farmers. Slavery became the fate of those captured as prisoners of war. In addition to being forced to work on building projects, slaves toiled at the discretion of the pharaoh or nobles. \\r\\nFarmers tended the fields, raised animals, kept canals and reservoirs in good order, worked in the stone quarries, and built the royal monuments. Farmers paid taxes that could amount to as much as 60% of their yearly harvest—that’s a lot of hay! \\r\\nSocial mobility was not impossible. A small number of peasants and farmers moved up the economic ladder. Families saved money to send their sons to village schools to learn trades. These schools were run by priests or by artisans. Boys who learned to read and write could become scribes, then go on to gain employment in the government. It was possible for a boy born on a farm to work his way up into the higher ranks of the government. Bureaucracy proved lucrative.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["train['text'].iloc[2]"],"id":"cQZWV8VaE0xO"},{"cell_type":"markdown","metadata":{"id":"0e4568b4"},"source":["# Dataset"],"id":"0e4568b4"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["77c711c1675d4b968c516b6f25e71d53","65087da746474689b75dab6ab58a967f","f7faaa32f3784e2ca65438aa82a4e421","9bcf9da4d98041f2bc278718a7ced7f8","d7d0b6b337b348c5944f72f1c38ed316","ec7c52f1d38b4977b605143407edc4fa","8b5ea1d9280b4a85b3221d1083283cb7","e07b8238de69476da49c018164b9b38f","d9e119bd0c934f84a986ceddf7e8d8d1","8cb944dfbf29460d9d73c0f4b32e0453","8f610050b6174716b46278339ba61860"]},"executionInfo":{"elapsed":18678,"status":"ok","timestamp":1695442300812,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"5d1060ed","outputId":"1cfd8d7c-a3ff-4189-a913-84acd83b8d34"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/7165 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c711c1675d4b968c516b6f25e71d53"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["max_len: 1024\n","INFO:__main__:max_len: 1024\n"]}],"source":["# ====================================================\n","# Define max_len\n","# ====================================================\n","lengths = []\n","tk0 = tqdm(train['text'].fillna(\"\").values, total=len(train))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","    lengths.append(length)\n","CFG.max_len = max(lengths) + 2 # cls & sep\n","CFG.max_len=1024\n","LOGGER.info(f\"max_len: {CFG.max_len}\")"],"id":"5d1060ed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bd231fe"},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text):\n","    inputs = cfg.tokenizer.encode_plus(\n","        text,\n","        return_tensors=None,\n","        add_special_tokens=True,\n","        max_length=CFG.max_len,\n","        pad_to_max_length=True,\n","        truncation=True\n","    )\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['text'].values\n","        self.labels = df[cfg.target_cols].values\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item])\n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        return inputs, label\n","\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:,:mask_len]\n","    return inputs"],"id":"2bd231fe"},{"cell_type":"markdown","metadata":{"id":"694f45ea"},"source":["# Model"],"id":"694f45ea"},{"cell_type":"code","execution_count":null,"metadata":{"id":"be0cf2ca"},"outputs":[],"source":["#ref:https://github.com/shu421/kagglib/blob/main/nlp/model.py\n","# ====================================================\n","# Model\n","# ====================================================\n","\n","def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","\n","    for parameter in module.parameters():\n","        parameter.requires_grad = False\n","# =====================================================\n","# Pooling\n","# =====================================================\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","\n","\n","class AttentionPooling(nn.Module):\n","    \"\"\"\n","    Usage:\n","        self.pool = AttentionPooling(self.config.hidden_size)\n","    \"\"\"\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_dim, in_dim),\n","            nn.LayerNorm(in_dim),\n","            nn.GELU(),\n","            nn.Linear(in_dim, 1),\n","        )\n","\n","        self._init_weights(self.attention)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        w = self.attention(last_hidden_state).float()\n","        w[attention_mask == 0] = float(\"-inf\")\n","        w = torch.softmax(w, 1)\n","        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n","        return attention_embeddings\n","\n","\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n","        )\n","\n","    def forward(self, all_hidden_states):\n","        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","        return weighted_average[:, 0]\n","\n","class LSTMPooling(nn.Module):\n","    def __init__(self, num_layers, hidden_size, hiddendim_lstm, dropout_rate, is_lstm=True):\n","        super(LSTMPooling, self).__init__()\n","        self.num_hidden_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.hiddendim_lstm = hiddendim_lstm\n","\n","        if is_lstm:\n","            self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n","        else:\n","            self.lstm = nn.GRU(self.hidden_size, self.hiddendim_lstm, batch_first=True, bidirectional=True)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, all_hidden_states):\n","        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n","                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n","        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n","        out, _ = self.lstm(hidden_states, None)\n","        out = self.dropout(out[:, -1, :])\n","        return out\n","\n","class ConcatPooling(nn.Module):\n","    def __init__(self, n_layers=4):\n","        super(ConcatPooling, self, ).__init__()\n","\n","        self.n_layers = n_layers\n","\n","    def forward(self, all_hidden_states):\n","        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n","        concatenate_pooling = concatenate_pooling[:, 0]\n","        return concatenate_pooling\n","\n","# GeM\n","class GeMText(nn.Module):\n","    def __init__(self, dim=1, cfg=None, p=3, eps=1e-6):\n","        super(GeMText, self).__init__()\n","        self.dim = dim\n","        self.p = Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","        self.feat_mult = 1\n","        # x seeems last hidden state\n","\n","    def forward(self, x, attention_mask):\n","        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n","        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n","        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n","        ret = ret.pow(1 / self.p)\n","        return ret\n","\n","# ===========================================\n","# custom Model\n","# ===========================================\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","            self.config.hidden_dropout = 0.\n","            self.config.hidden_dropout_prob = 0.\n","            self.config.attention_dropout = 0.\n","            self.config.attention_probs_dropout_prob = 0.\n","            LOGGER.info(self.config)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel(self.config)\n","        if self.cfg.gradient_checkpointing:\n","            self.model.gradient_checkpointing_enable()\n","        if cfg.pooling =='LSTMPooling':\n","            self.pool =  LSTMPooling(self.config.num_hidden_layers,\n","                                       self.config.hidden_size,\n","                                       self.cfg.hidden_size,\n","                                       0.1,\n","                                       is_lstm=True\n","                           )\n","            self.fc = nn.Linear(self.cfg.hidden_size, 2)\n","        elif cfg.pooling == \"GeM\":\n","            self.pool = GeMText()\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","        elif cfg.pooling=='ConcatPooling':\n","            self.pool = ConcatPooling(n_layers=cfg.n_layers)\n","            self.fc = nn.Linear(cfg.n_layers*self.config.hidden_size, 2)\n","        elif cfg.pooling=='WeightedLayerPooling':\n","            self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","\n","\n","        self._init_weights(self.fc)\n","\n","\n","        # Freeze\n","        if self.cfg.freeze:\n","            freeze(self.model.encoder.layer[:self.cfg.freeze_top_num_layer])\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        if self.cfg.pooling=='MeanPooling':\n","          feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling in ['GRUPooling', 'LSTMPooling']:\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling=='GeM':\n","          feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling == 'ConcatPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling == 'WeightedLayerPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(feature)\n","        return output\n","\n","\n","# initialize layer\n","def reinit_bert(model):\n","    \"\"\"_summary_\n","\n","    Args:\n","        model (AutoModel): _description_\n","\n","    Returns:\n","        model (AutoModel): _description_\n","\n","    Usage:\n","        model = reinit_bert(model)\n","    \"\"\"\n","    for layer in model.model.encoder.layer[-1:]:\n","        for module in layer.modules():\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","    return model"],"id":"be0cf2ca"},{"cell_type":"markdown","metadata":{"id":"31b35d08"},"source":["# Loss"],"id":"31b35d08"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac3909be"},"outputs":[],"source":["# ====================================================\n","# Loss\n","# ====================================================\n","class RMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.MSELoss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","\n","class WeightedSmoothL1Loss(nn.Module):\n","    def __init__(self,weights = torch.tensor([0.5, 1.2], device = device )):\n","        super(WeightedSmoothL1Loss, self).__init__()\n","        self.weights=weights\n","\n","    def forward(self, inputs, targets):\n","        \"\"\"\n","        inputs: ネットワークの出力 (予測値)\n","        targets: 正解ラベル\n","        weights: 各サンプルに対する重み\n","        \"\"\"\n","        # Smooth L1 損失を計算\n","        loss = nn.SmoothL1Loss(reduction='none')(inputs, targets)\n","\n","        # 重みを適用して損失を計算\n","        weighted_loss = torch.mean(loss * self.weights)\n","\n","        return weighted_loss\n","\n","\n","class MCRMSELoss(nn.Module):\n","    def __init__(self):\n","        super(MCRMSELoss, self).__init__()\n","\n","    def forward(self, y_true, y_pred):\n","        colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=0)\n","        return torch.mean(torch.sqrt(colwise_mse), dim=0)"],"id":"ac3909be"},{"cell_type":"markdown","metadata":{"id":"lKO48EXzL8mV"},"source":["# AWP"],"id":"lKO48EXzL8mV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zMdI9u5L-HP"},"outputs":[],"source":["from torch import Tensor\n","from torch.nn import Module\n","from torch.optim import Optimizer\n","from torch.nn.modules.loss import _Loss\n","\n","class AWP:\n","    def __init__(\n","        self,\n","        model: Module,\n","        criterion: _Loss,\n","        optimizer: Optimizer,\n","        apex: bool,\n","        adv_param: str=\"weight\",\n","        adv_lr: float=1.0,\n","        adv_eps: float=0.01\n","    ) -> None:\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.apex = apex\n","        self.backup = {}\n","        self.backup_eps = {}\n","\n","    def attack_backward(self, inputs: dict, label: Tensor) -> Tensor:\n","        with torch.cuda.amp.autocast(enabled=self.apex):\n","            self._save()\n","            self._attack_step() # モデルを近傍の悪い方へ改変\n","            y_preds = self.model(inputs)\n","            adv_loss = self.criterion(\n","                y_preds.view(-1, 1), label.view(-1, 1))\n","            mask = (label.view(-1, 1) != -1)\n","            adv_loss = torch.masked_select(adv_loss, mask).mean()\n","            self.optimizer.zero_grad()\n","        return adv_loss\n","\n","    def _attack_step(self) -> None:\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    # 直前に損失関数に通してパラメータの勾配を取得できるようにしておく必要あり\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(\n","                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","\n","    def _save(self) -> None:\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self) -> None:\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"],"id":"1zMdI9u5L-HP"},{"cell_type":"markdown","metadata":{"id":"d6344760"},"source":["# Helpler functions"],"id":"d6344760"},{"cell_type":"code","execution_count":null,"metadata":{"id":"978896ec"},"outputs":[],"source":["# ====================================================\n","# Helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","\n","    if CFG.awp and epoch+1 >= CFG.nth_awp_start_epoch:\n","        LOGGER.info(f'AWP training with epoch {epoch+1}')\n","    model.train()\n","    awp = AWP(\n","            model,\n","            criterion,\n","            optimizer,\n","            CFG.apex,\n","            adv_lr=CFG.adv_lr,\n","            adv_eps=CFG.adv_eps\n","        )\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if CFG.awp and CFG.nth_awp_start_epoch <= epoch+1:\n","            loss = awp.attack_backward(inputs, labels)\n","            scaler.scale(loss).backward()\n","            awp._restore()\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader),\n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions\n","\n","\n","\n","\n","\n","# def train_fn_by_step(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, now_step):\n","\n","#     # if CFG.awp and epoch+1 >= CFG.nth_awp_start_epoch:\n","#     #     LOGGER.info(f'AWP training with epoch {epoch+1}')\n","#     model.train()\n","#     # awp = AWP(\n","#     #         model,\n","#     #         criterion,\n","#     #         optimizer,\n","#     #         CFG.apex,\n","#     #         adv_lr=CFG.adv_lr,\n","#     #         adv_eps=CFG.adv_eps\n","#     #     )\n","#     if now_step==0:\n","#       scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","#       losses = AverageMeter()\n","#       start = end = time.time()\n","#       global_step = 0\n","#     for step, (inputs, labels) in enumerate(train_loader):\n","#         if now_step>step:\n","#           continue\n","#         inputs = collate(inputs)\n","#         for k, v in inputs.items():\n","#             inputs[k] = v.to(device)\n","#         labels = labels.to(device)\n","#         batch_size = labels.size(0)\n","#         with torch.cuda.amp.autocast(enabled=CFG.apex):\n","#             y_preds = model(inputs)\n","#             loss = criterion(y_preds, labels)\n","#         if CFG.gradient_accumulation_steps > 1:\n","#             loss = loss / CFG.gradient_accumulation_steps\n","#         losses.update(loss.item(), batch_size)\n","#         scaler.scale(loss).backward()\n","#         grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","#         # if CFG.awp and CFG.nth_awp_start_epoch <= epoch+1:\n","#         #     loss = awp.attack_backward(inputs, labels)\n","#         #     scaler.scale(loss).backward()\n","#         #     awp._restore()\n","\n","#         if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","#             scaler.step(optimizer)\n","#             scaler.update()\n","#             optimizer.zero_grad()\n","#             global_step += 1\n","#             if CFG.batch_scheduler:\n","#                 scheduler.step()\n","#         end = time.time()\n","#         if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","#             print('Epoch: [{0}][{1}/{2}] '\n","#                   'Elapsed {remain:s} '\n","#                   'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","#                   'Grad: {grad_norm:.4f}  '\n","#                   'LR: {lr:.8f}  '\n","#                   .format(epoch+1, step, len(train_loader),\n","#                           remain=timeSince(start, float(step+1)/len(train_loader)),\n","#                           loss=losses,\n","#                           grad_norm=grad_norm,\n","#                           lr=scheduler.get_lr()[0]))\n","\n","#         if CFG.wandb:\n","#             wandb.log({f\"[fold{fold}] loss\": losses.val,\n","#                        f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","#         if step%CFG.eval_steps==0:\n","#           return losses.avg, step+1 ,epoch\n","\n","#     return losses.avg, step+1 ,epoch+1\n","\n","\n"],"id":"978896ec"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUz1bDR9nNsa"},"outputs":[],"source":[],"id":"rUz1bDR9nNsa"},{"cell_type":"markdown","metadata":{"id":"fdb6b5d6"},"source":["# train loop"],"id":"fdb6b5d6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9088253a"},"outputs":[],"source":["# ====================================================\n","# train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","\n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    def get_optimizer_grouped_parameters(cfg, model):\n","        \"\"\"Layerwise Learning Rate Decay\"\"\"\n","        model_type = \"model\"\n","        no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if model_type not in n],\n","                \"lr\": cfg.decoder_lr,\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        num_layers = model.config.num_hidden_layers\n","        layers = [getattr(model, model_type).embeddings] + list(\n","            getattr(model, model_type).encoder.layer\n","        )\n","        layers.reverse()\n","        lr = cfg.encoder_lr\n","        for layer in layers:\n","            optimizer_grouped_parameters += [\n","                {\n","                    \"params\": [\n","                        p\n","                        for n, p in layer.named_parameters()\n","                        if not any(nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\": cfg.weight_decay,\n","                    \"lr\": lr,\n","                },\n","                {\n","                    \"params\": [\n","                        p\n","                        for n, p in layer.named_parameters()\n","                        if any(nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\": 0.0,\n","                    \"lr\": lr,\n","                },\n","            ]\n","\n","            lr *= cfg.lr_weight_decay\n","        return optimizer_grouped_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr,\n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    # optimizer_parameters = get_optimizer_grouped_parameters(CFG,model)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","\n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","\n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = MCRMSELoss()\n","    #nn.SmoothL1Loss(reduction='mean')\n","    # WeightedSmoothL1Loss(reduction='mean') #\n","\n","    best_score = np.inf\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","\n","        # scoring\n","        score, scores = get_score(valid_labels, predictions)\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] epoch\": epoch+1,\n","                       f\"[fold{fold}] avg_train_loss\": avg_loss,\n","                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                       f\"[fold{fold}] score\": score})\n","\n","        if best_score > score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# ====================================================\n","# train loop by steps\n","# ====================================================\n","def train_loop_steps(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    if CFG.reinit:\n","      model=reinit_bert(model)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","\n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr,\n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","\n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","\n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = MCRMSELoss()\n","    #nn.SmoothL1Loss(reduction='mean')\n","    # WeightedSmoothL1Loss(reduction='mean') #\n","\n","    best_score = np.inf\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        model.train()\n","        # if CFG.awp and epoch+1 >= CFG.nth_awp_start_epoch:\n","        #   LOGGER.info(f'AWP training with epoch {epoch+1}')\n","\n","        # awp = AWP(\n","        #     model,\n","        #     criterion,\n","        #     optimizer,\n","        #     CFG.apex,\n","        #     adv_lr=CFG.adv_lr,\n","        #     adv_eps=CFG.adv_eps\n","        #     )\n","        scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","        losses = AverageMeter()\n","        start = end = time.time()\n","        global_step = 0\n","        for step, (inputs, labels) in enumerate(tqdm(train_loader)):\n","\n","            inputs = collate(inputs)\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","            labels = labels.to(device)\n","            batch_size = labels.size(0)\n","            with torch.cuda.amp.autocast(enabled=CFG.apex):\n","                y_preds = model(inputs)\n","                loss = criterion(y_preds, labels)\n","            if CFG.gradient_accumulation_steps > 1:\n","                loss = loss / CFG.gradient_accumulation_steps\n","            losses.update(loss.item(), batch_size)\n","            scaler.scale(loss).backward()\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                global_step += 1\n","                if CFG.batch_scheduler:\n","                    scheduler.step()\n","            end = time.time()\n","            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","                print('Epoch: [{0}][{1}/{2}] '\n","                      'Elapsed {remain:s} '\n","                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                      'Grad: {grad_norm:.4f}  '\n","                      'LR: {lr:.8f}  '\n","                      .format(epoch+1, step, len(train_loader),\n","                              remain=timeSince(start, float(step+1)/len(train_loader)),\n","                              loss=losses,\n","                              grad_norm=grad_norm,\n","                              lr=scheduler.get_lr()[0]))\n","            if CFG.wandb:\n","                wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                          f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","\n","            if (step % CFG.eval_steps==0 and step!=0) or step == (len(train_loader)-1):\n","\n","                  # valid\n","                  losses_val = AverageMeter()\n","                  model.eval()\n","                  preds = []\n","\n","                  for val_step, (inputs, labels) in enumerate(valid_loader):\n","\n","                      inputs = collate(inputs)\n","                      for k, v in inputs.items():\n","                          inputs[k] = v.to(device)\n","                      labels = labels.to(device)\n","                      batch_size = labels.size(0)\n","                      with torch.no_grad():\n","                          y_preds = model(inputs)\n","                          loss = criterion(y_preds, labels)\n","                      if CFG.gradient_accumulation_steps > 1:\n","                          loss = loss / CFG.gradient_accumulation_steps\n","                      losses_val.update(loss.item(), batch_size)\n","                      preds.append(y_preds.to('cpu').numpy())\n","\n","                      if val_step % CFG.print_freq == 0 or val_step == (len(valid_loader)-1):\n","                          print('EVAL: [{0}/{1}] '\n","                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                                .format(val_step, len(valid_loader),\n","                                        loss=losses_val))\n","                  predictions = np.concatenate(preds)\n","\n","                  # scoring\n","                  score, scores = get_score(valid_labels, predictions)\n","\n","                  elapsed = time.time() - start_time\n","\n","\n","\n","                  if best_score > score:\n","                      best_score = score\n","                      LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","                      torch.save({'model': model.state_dict(),\n","                                  'predictions': predictions},\n","                                  OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","\n","                  LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {losses.avg:.4f}  avg_val_loss: {losses_val.avg:.4f}')\n","                  LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n","                  # if CFG.wandb:\n","                  #     wandb.log({f\"[fold{fold}] epoch\": epoch+1,\n","                  #                f\"[fold{fold}] avg_train_loss\": avg_loss,\n","                  #                f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                  #                f\"[fold{fold}] score\": score})\n","                  model.train()\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds\n","\n","\n","\n","\n","\n","\n","# ====================================================\n","# train loop by steps\n","# ====================================================\n","def prediction(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    if CFG.reinit:\n","      model=reinit_bert(model)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","    model.eval()\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds"],"id":"9088253a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["eb42bd5eb2ab4cb8a5bdcbf31b632602","c6e1cfe3e37244a7b190c5ed863a55ba","ab936b3f9fbf43cd867d7d3aa7947c9c","818b81d13d0142be902def03f557d8c1","8a242c84367f4ad89285e14f94255724","59ae62b2cccb4b59984540fd8b55152d","5a9f27babd1541648ce0f4598e993a5f","286797c24d774d6f851e61dcc000ce43","c82a5aac200447609c97694f289cd24d","12bb2eebf65444679af262c234f756ef","ee9fb1dfcc8b4cabb9a89885ae59d83d"]},"id":"fde1c8af","outputId":"9d359d97-8f3c-4164-a24b-50dba77b625f"},"outputs":[{"output_type":"stream","name":"stderr","text":["========== fold: 0 training ==========\n","INFO:__main__:========== fold: 0 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb42bd5eb2ab4cb8a5bdcbf31b632602"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/757] Elapsed 0m 5s (remain 64m 56s) Loss: 0.9748(0.9748) Grad: inf  LR: 0.00000009  \n","Epoch: [1][20/757] Elapsed 0m 24s (remain 14m 4s) Loss: 0.9563(1.2253) Grad: 150186.1250  LR: 0.00000185  \n","Epoch: [1][40/757] Elapsed 0m 43s (remain 12m 35s) Loss: 0.7831(1.0917) Grad: 93232.7812  LR: 0.00000361  \n","Epoch: [1][60/757] Elapsed 1m 1s (remain 11m 47s) Loss: 0.6848(1.0423) Grad: 226087.3281  LR: 0.00000537  \n","Epoch: [1][80/757] Elapsed 1m 20s (remain 11m 15s) Loss: 0.9426(0.9807) Grad: 198894.4688  LR: 0.00000714  \n","Epoch: [1][100/757] Elapsed 1m 40s (remain 10m 49s) Loss: 0.6510(0.9217) Grad: 127112.2500  LR: 0.00000890  \n","Epoch: [1][120/757] Elapsed 1m 59s (remain 10m 26s) Loss: 0.6135(0.8865) Grad: 209649.1250  LR: 0.00001066  \n","Epoch: [1][140/757] Elapsed 2m 18s (remain 10m 4s) Loss: 0.6788(0.8372) Grad: 136354.7969  LR: 0.00001242  \n","Epoch: [1][160/757] Elapsed 2m 37s (remain 9m 42s) Loss: 0.6104(0.8120) Grad: 154004.0625  LR: 0.00001419  \n","Epoch: [1][180/757] Elapsed 2m 56s (remain 9m 21s) Loss: 0.6055(0.7930) Grad: 122600.8047  LR: 0.00001595  \n","Epoch: [1][200/757] Elapsed 3m 15s (remain 9m 0s) Loss: 0.4366(0.7685) Grad: 112884.1328  LR: 0.00001771  \n","Epoch: [1][220/757] Elapsed 3m 34s (remain 8m 40s) Loss: 0.7112(0.7471) Grad: 161214.4844  LR: 0.00001947  \n","Epoch: [1][240/757] Elapsed 3m 53s (remain 8m 20s) Loss: 0.5694(0.7281) Grad: 83415.1406  LR: 0.00002000  \n","Epoch: [1][260/757] Elapsed 4m 12s (remain 8m 0s) Loss: 0.6019(0.7156) Grad: 72714.0938  LR: 0.00001999  \n","Epoch: [1][280/757] Elapsed 4m 31s (remain 7m 39s) Loss: 0.6479(0.7042) Grad: 156575.2969  LR: 0.00001997  \n","Epoch: [1][300/757] Elapsed 4m 49s (remain 7m 19s) Loss: 0.4350(0.6929) Grad: 148457.2031  LR: 0.00001994  \n","Epoch: [1][320/757] Elapsed 5m 8s (remain 6m 59s) Loss: 0.4977(0.6827) Grad: 124382.0547  LR: 0.00001990  \n","Epoch: [1][340/757] Elapsed 5m 28s (remain 6m 40s) Loss: 0.8504(0.6762) Grad: 134248.7344  LR: 0.00001985  \n","Epoch: [1][360/757] Elapsed 5m 47s (remain 6m 20s) Loss: 0.3891(0.6638) Grad: 66367.6953  LR: 0.00001979  \n","Epoch: [1][380/757] Elapsed 6m 6s (remain 6m 1s) Loss: 0.4292(0.6574) Grad: 106856.7266  LR: 0.00001972  \n","Epoch: [1][400/757] Elapsed 6m 25s (remain 5m 42s) Loss: 0.5134(0.6491) Grad: 63525.2500  LR: 0.00001965  \n","Epoch: [1][420/757] Elapsed 6m 44s (remain 5m 22s) Loss: 0.4906(0.6426) Grad: 122849.4766  LR: 0.00001956  \n","Epoch: [1][440/757] Elapsed 7m 3s (remain 5m 3s) Loss: 0.3770(0.6360) Grad: 154819.5781  LR: 0.00001946  \n","Epoch: [1][460/757] Elapsed 7m 22s (remain 4m 44s) Loss: 0.2981(0.6285) Grad: 94219.3984  LR: 0.00001936  \n","Epoch: [1][480/757] Elapsed 7m 41s (remain 4m 25s) Loss: 0.6482(0.6250) Grad: 139458.3594  LR: 0.00001925  \n","Epoch: [1][500/757] Elapsed 8m 0s (remain 4m 5s) Loss: 0.4704(0.6211) Grad: 132917.4375  LR: 0.00001913  \n","Epoch: [1][520/757] Elapsed 8m 19s (remain 3m 46s) Loss: 0.5478(0.6161) Grad: 72782.9062  LR: 0.00001900  \n","Epoch: [1][540/757] Elapsed 8m 38s (remain 3m 27s) Loss: 0.3900(0.6123) Grad: 86665.7422  LR: 0.00001886  \n","Epoch: [1][560/757] Elapsed 8m 57s (remain 3m 7s) Loss: 0.4303(0.6063) Grad: 111869.1797  LR: 0.00001871  \n","Epoch: [1][580/757] Elapsed 9m 16s (remain 2m 48s) Loss: 0.7016(0.6032) Grad: 63635.9805  LR: 0.00001856  \n","Epoch: [1][600/757] Elapsed 9m 36s (remain 2m 29s) Loss: 0.5529(0.6005) Grad: 149347.5625  LR: 0.00001840  \n","Epoch: [1][620/757] Elapsed 9m 55s (remain 2m 10s) Loss: 0.4375(0.5983) Grad: 77602.0156  LR: 0.00001823  \n","Epoch: [1][640/757] Elapsed 10m 14s (remain 1m 51s) Loss: 0.4430(0.5952) Grad: 27739.9297  LR: 0.00001805  \n","Epoch: [1][660/757] Elapsed 10m 32s (remain 1m 31s) Loss: 0.4196(0.5923) Grad: 64569.7109  LR: 0.00001786  \n","Epoch: [1][680/757] Elapsed 10m 52s (remain 1m 12s) Loss: 0.5073(0.5878) Grad: 70634.3047  LR: 0.00001767  \n","Epoch: [1][700/757] Elapsed 11m 10s (remain 0m 53s) Loss: 0.3763(0.5840) Grad: 130916.4844  LR: 0.00001747  \n","Epoch: [1][720/757] Elapsed 11m 30s (remain 0m 34s) Loss: 0.4753(0.5807) Grad: 106977.8672  LR: 0.00001726  \n","Epoch: [1][740/757] Elapsed 11m 49s (remain 0m 15s) Loss: 0.4691(0.5790) Grad: 157847.2031  LR: 0.00001704  \n","Epoch: [1][756/757] Elapsed 12m 4s (remain 0m 0s) Loss: 0.5113(0.5767) Grad: 118346.8281  LR: 0.00001687  \n","EVAL: [0/69] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6187(0.6187) \n","EVAL: [20/69] Elapsed 0m 21s (remain 0m 48s) Loss: 0.4734(0.5777) \n","EVAL: [40/69] Elapsed 0m 41s (remain 0m 28s) Loss: 0.5964(0.5540) \n","EVAL: [60/69] Elapsed 1m 2s (remain 0m 8s) Loss: 0.5045(0.5497) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5767  avg_val_loss: 0.5579  time: 795s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.5767  avg_val_loss: 0.5579  time: 795s\n","Epoch 1 - Score: 0.5715  Scores: [0.47966466298863125, 0.6632378429796326]\n","INFO:__main__:Epoch 1 - Score: 0.5715  Scores: [0.47966466298863125, 0.6632378429796326]\n","Epoch 1 - Save Best Score: 0.5715 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.5715 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [68/69] Elapsed 1m 10s (remain 0m 0s) Loss: 0.7843(0.5579) \n","Epoch: [2][0/757] Elapsed 0m 1s (remain 14m 53s) Loss: 0.4016(0.4016) Grad: inf  LR: 0.00001686  \n","Epoch: [2][20/757] Elapsed 0m 19s (remain 11m 32s) Loss: 0.3237(0.4524) Grad: 79624.4453  LR: 0.00001663  \n","Epoch: [2][40/757] Elapsed 0m 38s (remain 11m 12s) Loss: 0.5124(0.4329) Grad: 93206.5703  LR: 0.00001640  \n","Epoch: [2][60/757] Elapsed 0m 57s (remain 10m 57s) Loss: 0.4392(0.4265) Grad: 126868.1406  LR: 0.00001616  \n","Epoch: [2][80/757] Elapsed 1m 16s (remain 10m 41s) Loss: 0.3316(0.4359) Grad: 64573.6328  LR: 0.00001591  \n","Epoch: [2][100/757] Elapsed 1m 35s (remain 10m 21s) Loss: 0.4507(0.4384) Grad: 59470.0273  LR: 0.00001566  \n","Epoch: [2][120/757] Elapsed 1m 54s (remain 10m 3s) Loss: 0.2208(0.4442) Grad: 36672.7461  LR: 0.00001541  \n","Epoch: [2][140/757] Elapsed 2m 13s (remain 9m 45s) Loss: 0.3404(0.4501) Grad: 70698.7422  LR: 0.00001515  \n","Epoch: [2][160/757] Elapsed 2m 33s (remain 9m 26s) Loss: 0.3590(0.4466) Grad: 99477.6875  LR: 0.00001488  \n","Epoch: [2][180/757] Elapsed 2m 52s (remain 9m 7s) Loss: 0.3273(0.4467) Grad: 61253.8242  LR: 0.00001461  \n","Epoch: [2][200/757] Elapsed 3m 10s (remain 8m 48s) Loss: 0.4264(0.4434) Grad: 85785.0078  LR: 0.00001433  \n","Epoch: [2][220/757] Elapsed 3m 30s (remain 8m 29s) Loss: 0.3230(0.4412) Grad: 35128.5742  LR: 0.00001406  \n","Epoch: [2][240/757] Elapsed 3m 49s (remain 8m 10s) Loss: 0.4539(0.4388) Grad: 82791.7812  LR: 0.00001377  \n","Epoch: [2][260/757] Elapsed 4m 8s (remain 7m 51s) Loss: 0.5056(0.4401) Grad: 89151.5938  LR: 0.00001349  \n","Epoch: [2][280/757] Elapsed 4m 27s (remain 7m 32s) Loss: 0.3857(0.4400) Grad: 55071.6172  LR: 0.00001320  \n","Epoch: [2][300/757] Elapsed 4m 46s (remain 7m 13s) Loss: 0.3185(0.4381) Grad: 76195.4531  LR: 0.00001291  \n","Epoch: [2][320/757] Elapsed 5m 5s (remain 6m 55s) Loss: 0.4979(0.4377) Grad: 40522.8945  LR: 0.00001261  \n","Epoch: [2][340/757] Elapsed 5m 24s (remain 6m 36s) Loss: 0.5178(0.4379) Grad: 110631.9844  LR: 0.00001231  \n","Epoch: [2][360/757] Elapsed 5m 43s (remain 6m 16s) Loss: 0.4587(0.4376) Grad: 113743.5234  LR: 0.00001201  \n","Epoch: [2][380/757] Elapsed 6m 2s (remain 5m 57s) Loss: 0.4226(0.4360) Grad: 96660.8359  LR: 0.00001171  \n","Epoch: [2][400/757] Elapsed 6m 21s (remain 5m 38s) Loss: 0.4272(0.4363) Grad: 87098.6875  LR: 0.00001141  \n","Epoch: [2][420/757] Elapsed 6m 40s (remain 5m 19s) Loss: 0.4318(0.4359) Grad: 96900.4531  LR: 0.00001110  \n","Epoch: [2][440/757] Elapsed 6m 59s (remain 5m 0s) Loss: 0.5388(0.4349) Grad: 113474.5547  LR: 0.00001080  \n","Epoch: [2][460/757] Elapsed 7m 18s (remain 4m 41s) Loss: 0.3311(0.4360) Grad: 118990.9453  LR: 0.00001049  \n","Epoch: [2][480/757] Elapsed 7m 37s (remain 4m 22s) Loss: 0.3312(0.4356) Grad: 63487.5703  LR: 0.00001018  \n","Epoch: [2][500/757] Elapsed 7m 56s (remain 4m 3s) Loss: 0.4096(0.4346) Grad: 146796.9531  LR: 0.00000988  \n","Epoch: [2][520/757] Elapsed 8m 15s (remain 3m 44s) Loss: 0.3849(0.4357) Grad: 74883.8125  LR: 0.00000957  \n","Epoch: [2][540/757] Elapsed 8m 35s (remain 3m 25s) Loss: 0.4298(0.4350) Grad: 96625.1562  LR: 0.00000926  \n","Epoch: [2][560/757] Elapsed 8m 54s (remain 3m 6s) Loss: 0.5070(0.4342) Grad: 90158.7188  LR: 0.00000896  \n","Epoch: [2][580/757] Elapsed 9m 13s (remain 2m 47s) Loss: 0.3614(0.4344) Grad: 69755.5391  LR: 0.00000865  \n","Epoch: [2][600/757] Elapsed 9m 32s (remain 2m 28s) Loss: 0.3608(0.4342) Grad: 87429.2734  LR: 0.00000835  \n","Epoch: [2][620/757] Elapsed 9m 51s (remain 2m 9s) Loss: 0.3860(0.4344) Grad: 70775.9375  LR: 0.00000805  \n","Epoch: [2][640/757] Elapsed 10m 10s (remain 1m 50s) Loss: 0.4618(0.4334) Grad: 59319.6172  LR: 0.00000775  \n","Epoch: [2][660/757] Elapsed 10m 29s (remain 1m 31s) Loss: 0.3641(0.4322) Grad: 58784.8789  LR: 0.00000745  \n","Epoch: [2][680/757] Elapsed 10m 48s (remain 1m 12s) Loss: 0.4617(0.4315) Grad: 65628.7734  LR: 0.00000715  \n","Epoch: [2][700/757] Elapsed 11m 7s (remain 0m 53s) Loss: 0.4426(0.4306) Grad: 119730.6719  LR: 0.00000686  \n","Epoch: [2][720/757] Elapsed 11m 26s (remain 0m 34s) Loss: 0.5048(0.4305) Grad: 36334.0742  LR: 0.00000657  \n","Epoch: [2][740/757] Elapsed 11m 45s (remain 0m 15s) Loss: 0.4368(0.4301) Grad: 157940.3906  LR: 0.00000628  \n","Epoch: [2][756/757] Elapsed 12m 1s (remain 0m 0s) Loss: 0.3733(0.4294) Grad: 63326.6172  LR: 0.00000606  \n","EVAL: [0/69] Elapsed 0m 1s (remain 1m 31s) Loss: 0.5592(0.5592) \n","EVAL: [20/69] Elapsed 0m 21s (remain 0m 48s) Loss: 0.4741(0.5896) \n","EVAL: [40/69] Elapsed 0m 41s (remain 0m 28s) Loss: 0.6337(0.5707) \n","EVAL: [60/69] Elapsed 1m 2s (remain 0m 8s) Loss: 0.4831(0.5670) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4294  avg_val_loss: 0.5744  time: 792s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4294  avg_val_loss: 0.5744  time: 792s\n","Epoch 2 - Score: 0.5879  Scores: [0.5590123063004532, 0.6167541623192853]\n","INFO:__main__:Epoch 2 - Score: 0.5879  Scores: [0.5590123063004532, 0.6167541623192853]\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [68/69] Elapsed 1m 10s (remain 0m 0s) Loss: 0.8276(0.5744) \n","Epoch: [3][0/757] Elapsed 0m 1s (remain 14m 44s) Loss: 0.5870(0.5870) Grad: inf  LR: 0.00000604  \n","Epoch: [3][20/757] Elapsed 0m 20s (remain 11m 42s) Loss: 0.2080(0.3798) Grad: 106175.1328  LR: 0.00000576  \n","Epoch: [3][40/757] Elapsed 0m 39s (remain 11m 24s) Loss: 0.3672(0.3880) Grad: 68542.9688  LR: 0.00000549  \n","Epoch: [3][60/757] Elapsed 0m 58s (remain 11m 2s) Loss: 0.3862(0.3802) Grad: 48643.7930  LR: 0.00000521  \n","Epoch: [3][80/757] Elapsed 1m 17s (remain 10m 44s) Loss: 0.3180(0.3779) Grad: 65737.9141  LR: 0.00000495  \n","Epoch: [3][100/757] Elapsed 1m 36s (remain 10m 26s) Loss: 0.4420(0.3778) Grad: 100990.8125  LR: 0.00000468  \n","Epoch: [3][120/757] Elapsed 1m 55s (remain 10m 6s) Loss: 0.4851(0.3807) Grad: 104368.6094  LR: 0.00000443  \n","Epoch: [3][140/757] Elapsed 2m 14s (remain 9m 47s) Loss: 0.2618(0.3756) Grad: 95742.5547  LR: 0.00000417  \n","Epoch: [3][160/757] Elapsed 2m 32s (remain 9m 25s) Loss: 0.3995(0.3724) Grad: 70606.0078  LR: 0.00000393  \n","Epoch: [3][180/757] Elapsed 2m 51s (remain 9m 6s) Loss: 0.4309(0.3672) Grad: 52625.6055  LR: 0.00000369  \n","Epoch: [3][200/757] Elapsed 3m 10s (remain 8m 48s) Loss: 0.2981(0.3668) Grad: 106363.2031  LR: 0.00000345  \n","Epoch: [3][220/757] Elapsed 3m 30s (remain 8m 29s) Loss: 0.3053(0.3654) Grad: 117852.0000  LR: 0.00000322  \n","Epoch: [3][240/757] Elapsed 3m 49s (remain 8m 10s) Loss: 0.2553(0.3645) Grad: 39388.0820  LR: 0.00000300  \n","Epoch: [3][260/757] Elapsed 4m 8s (remain 7m 52s) Loss: 0.2739(0.3621) Grad: 40594.1406  LR: 0.00000278  \n","Epoch: [3][280/757] Elapsed 4m 27s (remain 7m 32s) Loss: 0.2692(0.3598) Grad: 64354.4336  LR: 0.00000257  \n","Epoch: [3][300/757] Elapsed 4m 46s (remain 7m 13s) Loss: 0.6117(0.3594) Grad: 137623.7969  LR: 0.00000237  \n","Epoch: [3][320/757] Elapsed 5m 5s (remain 6m 55s) Loss: 0.3257(0.3602) Grad: 34661.1562  LR: 0.00000218  \n","Epoch: [3][340/757] Elapsed 5m 24s (remain 6m 36s) Loss: 0.3247(0.3593) Grad: 98501.7500  LR: 0.00000199  \n","Epoch: [3][360/757] Elapsed 5m 43s (remain 6m 17s) Loss: 0.4368(0.3575) Grad: 107232.0078  LR: 0.00000181  \n","Epoch: [3][380/757] Elapsed 6m 2s (remain 5m 57s) Loss: 0.4220(0.3554) Grad: 37803.7188  LR: 0.00000164  \n","Epoch: [3][400/757] Elapsed 6m 21s (remain 5m 38s) Loss: 0.3007(0.3547) Grad: 83283.8594  LR: 0.00000147  \n","Epoch: [3][420/757] Elapsed 6m 40s (remain 5m 19s) Loss: 0.4255(0.3550) Grad: 68686.9688  LR: 0.00000132  \n","Epoch: [3][440/757] Elapsed 6m 59s (remain 5m 0s) Loss: 0.3394(0.3548) Grad: 81031.7891  LR: 0.00000117  \n","Epoch: [3][460/757] Elapsed 7m 18s (remain 4m 41s) Loss: 0.3125(0.3528) Grad: 51634.3281  LR: 0.00000103  \n","Epoch: [3][480/757] Elapsed 7m 37s (remain 4m 22s) Loss: 0.3959(0.3528) Grad: 107765.6406  LR: 0.00000090  \n","Epoch: [3][500/757] Elapsed 7m 56s (remain 4m 3s) Loss: 0.3499(0.3537) Grad: 69160.0859  LR: 0.00000077  \n","Epoch: [3][520/757] Elapsed 8m 15s (remain 3m 44s) Loss: 0.2848(0.3524) Grad: 76400.2031  LR: 0.00000066  \n","Epoch: [3][540/757] Elapsed 8m 34s (remain 3m 25s) Loss: 0.3716(0.3513) Grad: 92787.4844  LR: 0.00000056  \n","Epoch: [3][560/757] Elapsed 8m 53s (remain 3m 6s) Loss: 0.4148(0.3502) Grad: 68410.2188  LR: 0.00000046  \n","Epoch: [3][580/757] Elapsed 9m 13s (remain 2m 47s) Loss: 0.3260(0.3500) Grad: 60670.5195  LR: 0.00000037  \n","Epoch: [3][600/757] Elapsed 9m 31s (remain 2m 28s) Loss: 0.3022(0.3505) Grad: 83680.7188  LR: 0.00000029  \n","Epoch: [3][620/757] Elapsed 9m 50s (remain 2m 9s) Loss: 0.2658(0.3505) Grad: 46413.5000  LR: 0.00000022  \n","Epoch: [3][640/757] Elapsed 10m 10s (remain 1m 50s) Loss: 0.3362(0.3505) Grad: 83597.4297  LR: 0.00000016  \n","Epoch: [3][660/757] Elapsed 10m 28s (remain 1m 31s) Loss: 0.2981(0.3492) Grad: 36372.6914  LR: 0.00000011  \n","Epoch: [3][680/757] Elapsed 10m 48s (remain 1m 12s) Loss: 0.4112(0.3491) Grad: 78588.7031  LR: 0.00000007  \n","Epoch: [3][700/757] Elapsed 11m 6s (remain 0m 53s) Loss: 0.4009(0.3490) Grad: 28226.4336  LR: 0.00000004  \n","Epoch: [3][720/757] Elapsed 11m 25s (remain 0m 34s) Loss: 0.3656(0.3487) Grad: 81990.1172  LR: 0.00000002  \n","Epoch: [3][740/757] Elapsed 11m 44s (remain 0m 15s) Loss: 0.3428(0.3482) Grad: 74569.5938  LR: 0.00000000  \n","Epoch: [3][756/757] Elapsed 12m 0s (remain 0m 0s) Loss: 0.4357(0.3479) Grad: 68085.4297  LR: 0.00000000  \n","EVAL: [0/69] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6466(0.6466) \n","EVAL: [20/69] Elapsed 0m 21s (remain 0m 48s) Loss: 0.5442(0.6561) \n","EVAL: [40/69] Elapsed 0m 41s (remain 0m 28s) Loss: 0.7004(0.6320) \n","EVAL: [60/69] Elapsed 1m 2s (remain 0m 8s) Loss: 0.5216(0.6275) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3479  avg_val_loss: 0.6349  time: 791s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3479  avg_val_loss: 0.6349  time: 791s\n","Epoch 3 - Score: 0.6497  Scores: [0.643583714184997, 0.6557286684897378]\n","INFO:__main__:Epoch 3 - Score: 0.6497  Scores: [0.643583714184997, 0.6557286684897378]\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [68/69] Elapsed 1m 10s (remain 0m 0s) Loss: 0.9113(0.6349) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 0 result ==========\n","INFO:__main__:========== fold: 0 result ==========\n","Score: 0.5715  Scores: [0.47966466298863125, 0.6632378429796326]\n","INFO:__main__:Score: 0.5715  Scores: [0.47966466298863125, 0.6632378429796326]\n","========== fold: 1 training ==========\n","INFO:__main__:========== fold: 1 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/638] Elapsed 0m 1s (remain 13m 5s) Loss: 1.6800(1.6800) Grad: inf  LR: 0.00000010  \n","Epoch: [1][20/638] Elapsed 0m 20s (remain 9m 56s) Loss: 1.1454(1.5692) Grad: 2435191.5000  LR: 0.00000220  \n","Epoch: [1][40/638] Elapsed 0m 39s (remain 9m 34s) Loss: 1.4742(1.3475) Grad: 429932.9062  LR: 0.00000429  \n","Epoch: [1][60/638] Elapsed 0m 58s (remain 9m 14s) Loss: 0.9481(1.1867) Grad: 435629.2188  LR: 0.00000639  \n","Epoch: [1][80/638] Elapsed 1m 17s (remain 8m 54s) Loss: 0.6854(1.0880) Grad: 733302.1250  LR: 0.00000848  \n","Epoch: [1][100/638] Elapsed 1m 36s (remain 8m 34s) Loss: 0.8548(1.0043) Grad: 506429.3125  LR: 0.00001058  \n","Epoch: [1][120/638] Elapsed 1m 56s (remain 8m 15s) Loss: 0.5819(0.9560) Grad: 885084.6250  LR: 0.00001267  \n","Epoch: [1][140/638] Elapsed 2m 15s (remain 7m 56s) Loss: 0.5935(0.9189) Grad: 513662.9688  LR: 0.00001476  \n","Epoch: [1][160/638] Elapsed 2m 34s (remain 7m 37s) Loss: 0.4687(0.8809) Grad: 173814.6406  LR: 0.00001686  \n","Epoch: [1][180/638] Elapsed 2m 53s (remain 7m 18s) Loss: 0.9120(0.8715) Grad: 208679.1250  LR: 0.00001895  \n","Epoch: [1][200/638] Elapsed 3m 12s (remain 6m 58s) Loss: 0.6066(0.8569) Grad: 500310.4062  LR: 0.00002000  \n","Epoch: [1][220/638] Elapsed 3m 31s (remain 6m 38s) Loss: 0.6010(0.8384) Grad: 353124.5625  LR: 0.00001999  \n","Epoch: [1][240/638] Elapsed 3m 50s (remain 6m 19s) Loss: 0.7100(0.8193) Grad: 317684.5000  LR: 0.00001996  \n","Epoch: [1][260/638] Elapsed 4m 9s (remain 6m 0s) Loss: 0.5878(0.8098) Grad: 507270.8125  LR: 0.00001992  \n","Epoch: [1][280/638] Elapsed 4m 28s (remain 5m 41s) Loss: 0.6061(0.7945) Grad: 234899.3750  LR: 0.00001987  \n","Epoch: [1][300/638] Elapsed 4m 47s (remain 5m 22s) Loss: 0.5396(0.7770) Grad: 129691.5938  LR: 0.00001980  \n","Epoch: [1][320/638] Elapsed 5m 6s (remain 5m 2s) Loss: 0.7124(0.7720) Grad: 207153.7344  LR: 0.00001972  \n","Epoch: [1][340/638] Elapsed 5m 25s (remain 4m 43s) Loss: 0.6958(0.7628) Grad: 112390.2734  LR: 0.00001963  \n","Epoch: [1][360/638] Elapsed 5m 44s (remain 4m 24s) Loss: 0.7698(0.7547) Grad: 184946.7188  LR: 0.00001952  \n","Epoch: [1][380/638] Elapsed 6m 3s (remain 4m 5s) Loss: 0.4816(0.7455) Grad: 103558.9922  LR: 0.00001941  \n","Epoch: [1][400/638] Elapsed 6m 22s (remain 3m 46s) Loss: 0.6604(0.7409) Grad: 193402.6719  LR: 0.00001928  \n","Epoch: [1][420/638] Elapsed 6m 41s (remain 3m 27s) Loss: 0.5393(0.7324) Grad: 155855.1406  LR: 0.00001913  \n","Epoch: [1][440/638] Elapsed 7m 1s (remain 3m 8s) Loss: 0.7633(0.7269) Grad: 188950.4688  LR: 0.00001898  \n","Epoch: [1][460/638] Elapsed 7m 20s (remain 2m 49s) Loss: 0.3169(0.7159) Grad: 158891.1250  LR: 0.00001881  \n","Epoch: [1][480/638] Elapsed 7m 39s (remain 2m 29s) Loss: 0.6779(0.7097) Grad: 153188.5312  LR: 0.00001864  \n","Epoch: [1][500/638] Elapsed 7m 58s (remain 2m 10s) Loss: 0.5054(0.7006) Grad: 92773.3672  LR: 0.00001845  \n","Epoch: [1][520/638] Elapsed 8m 17s (remain 1m 51s) Loss: 0.6920(0.6936) Grad: 94314.4688  LR: 0.00001825  \n","Epoch: [1][540/638] Elapsed 8m 36s (remain 1m 32s) Loss: 0.5821(0.6874) Grad: 164183.5781  LR: 0.00001803  \n","Epoch: [1][560/638] Elapsed 8m 56s (remain 1m 13s) Loss: 0.2710(0.6834) Grad: 95538.9219  LR: 0.00001781  \n","Epoch: [1][580/638] Elapsed 9m 15s (remain 0m 54s) Loss: 0.6158(0.6783) Grad: 77668.4688  LR: 0.00001758  \n","Epoch: [1][600/638] Elapsed 9m 34s (remain 0m 35s) Loss: 0.4904(0.6714) Grad: 41366.0938  LR: 0.00001734  \n","Epoch: [1][620/638] Elapsed 9m 52s (remain 0m 16s) Loss: 0.5037(0.6691) Grad: 188788.4844  LR: 0.00001708  \n","Epoch: [1][637/638] Elapsed 10m 9s (remain 0m 0s) Loss: 0.5903(0.6637) Grad: 120726.3906  LR: 0.00001686  \n","EVAL: [0/129] Elapsed 0m 1s (remain 2m 39s) Loss: 0.5271(0.5271) \n","EVAL: [20/129] Elapsed 0m 23s (remain 2m 0s) Loss: 0.4631(0.5315) \n","EVAL: [40/129] Elapsed 0m 44s (remain 1m 36s) Loss: 0.4762(0.5093) \n","EVAL: [60/129] Elapsed 1m 6s (remain 1m 14s) Loss: 0.5671(0.5135) \n","EVAL: [80/129] Elapsed 1m 28s (remain 0m 52s) Loss: 0.4627(0.5085) \n","EVAL: [100/129] Elapsed 1m 49s (remain 0m 30s) Loss: 0.6407(0.5090) \n","EVAL: [120/129] Elapsed 2m 11s (remain 0m 8s) Loss: 0.5007(0.5064) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.6637  avg_val_loss: 0.5068  time: 749s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.6637  avg_val_loss: 0.5068  time: 749s\n","Epoch 1 - Score: 0.5143  Scores: [0.4414963980881938, 0.5871557290733892]\n","INFO:__main__:Epoch 1 - Score: 0.5143  Scores: [0.4414963980881938, 0.5871557290733892]\n","Epoch 1 - Save Best Score: 0.5143 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.5143 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [128/129] Elapsed 2m 19s (remain 0m 0s) Loss: 0.4914(0.5068) \n","Epoch: [2][0/638] Elapsed 0m 1s (remain 13m 16s) Loss: 0.4445(0.4445) Grad: inf  LR: 0.00001685  \n","Epoch: [2][20/638] Elapsed 0m 20s (remain 9m 52s) Loss: 0.5173(0.4455) Grad: 93588.6562  LR: 0.00001658  \n","Epoch: [2][40/638] Elapsed 0m 39s (remain 9m 32s) Loss: 0.4843(0.4625) Grad: 50520.9961  LR: 0.00001630  \n","Epoch: [2][60/638] Elapsed 0m 58s (remain 9m 13s) Loss: 0.5409(0.4575) Grad: 120754.2031  LR: 0.00001601  \n","Epoch: [2][80/638] Elapsed 1m 17s (remain 8m 54s) Loss: 0.3620(0.4478) Grad: 56771.1055  LR: 0.00001572  \n","Epoch: [2][100/638] Elapsed 1m 36s (remain 8m 34s) Loss: 0.5528(0.4624) Grad: 81326.6328  LR: 0.00001541  \n","Epoch: [2][120/638] Elapsed 1m 55s (remain 8m 14s) Loss: 0.3423(0.4555) Grad: 131003.4297  LR: 0.00001510  \n","Epoch: [2][140/638] Elapsed 2m 14s (remain 7m 55s) Loss: 0.4167(0.4625) Grad: 71652.0156  LR: 0.00001479  \n","Epoch: [2][160/638] Elapsed 2m 33s (remain 7m 35s) Loss: 0.4150(0.4680) Grad: 101659.4375  LR: 0.00001447  \n","Epoch: [2][180/638] Elapsed 2m 52s (remain 7m 16s) Loss: 0.4851(0.4678) Grad: 125428.9922  LR: 0.00001414  \n","Epoch: [2][200/638] Elapsed 3m 12s (remain 6m 57s) Loss: 0.3795(0.4697) Grad: 149406.8750  LR: 0.00001380  \n","Epoch: [2][220/638] Elapsed 3m 31s (remain 6m 38s) Loss: 0.4668(0.4669) Grad: 118191.4609  LR: 0.00001346  \n","Epoch: [2][240/638] Elapsed 3m 50s (remain 6m 19s) Loss: 0.3632(0.4686) Grad: 98906.1953  LR: 0.00001312  \n","Epoch: [2][260/638] Elapsed 4m 9s (remain 6m 0s) Loss: 0.3746(0.4683) Grad: 123019.2344  LR: 0.00001277  \n","Epoch: [2][280/638] Elapsed 4m 28s (remain 5m 41s) Loss: 0.5254(0.4693) Grad: 90551.5156  LR: 0.00001242  \n","Epoch: [2][300/638] Elapsed 4m 47s (remain 5m 22s) Loss: 0.3948(0.4705) Grad: 48200.9688  LR: 0.00001206  \n","Epoch: [2][320/638] Elapsed 5m 7s (remain 5m 3s) Loss: 0.5461(0.4717) Grad: 102416.1250  LR: 0.00001170  \n","Epoch: [2][340/638] Elapsed 5m 26s (remain 4m 44s) Loss: 0.3934(0.4698) Grad: 63255.2539  LR: 0.00001134  \n","Epoch: [2][360/638] Elapsed 5m 45s (remain 4m 25s) Loss: 0.4865(0.4701) Grad: 79344.8047  LR: 0.00001098  \n","Epoch: [2][380/638] Elapsed 6m 4s (remain 4m 5s) Loss: 0.5516(0.4701) Grad: 94702.6016  LR: 0.00001062  \n","Epoch: [2][400/638] Elapsed 6m 23s (remain 3m 46s) Loss: 0.4829(0.4717) Grad: 119021.1875  LR: 0.00001026  \n","Epoch: [2][420/638] Elapsed 6m 42s (remain 3m 27s) Loss: 0.7173(0.4731) Grad: 121573.6719  LR: 0.00000989  \n","Epoch: [2][440/638] Elapsed 7m 2s (remain 3m 8s) Loss: 0.3578(0.4719) Grad: 119590.7500  LR: 0.00000953  \n","Epoch: [2][460/638] Elapsed 7m 21s (remain 2m 49s) Loss: 0.3393(0.4696) Grad: 142168.4062  LR: 0.00000916  \n","Epoch: [2][480/638] Elapsed 7m 40s (remain 2m 30s) Loss: 0.3798(0.4690) Grad: 64202.2930  LR: 0.00000880  \n","Epoch: [2][500/638] Elapsed 7m 59s (remain 2m 11s) Loss: 0.3831(0.4679) Grad: 91020.5547  LR: 0.00000844  \n","Epoch: [2][520/638] Elapsed 8m 18s (remain 1m 52s) Loss: 0.4575(0.4677) Grad: 71604.7266  LR: 0.00000808  \n","Epoch: [2][540/638] Elapsed 8m 37s (remain 1m 32s) Loss: 0.2984(0.4676) Grad: 91142.1250  LR: 0.00000772  \n","Epoch: [2][560/638] Elapsed 8m 57s (remain 1m 13s) Loss: 0.3976(0.4664) Grad: 113658.5156  LR: 0.00000737  \n","Epoch: [2][580/638] Elapsed 9m 16s (remain 0m 54s) Loss: 0.5417(0.4634) Grad: 74963.7109  LR: 0.00000702  \n","Epoch: [2][600/638] Elapsed 9m 35s (remain 0m 35s) Loss: 0.4717(0.4637) Grad: 79938.3828  LR: 0.00000668  \n","Epoch: [2][620/638] Elapsed 9m 54s (remain 0m 16s) Loss: 0.4788(0.4627) Grad: 85233.7500  LR: 0.00000633  \n","Epoch: [2][637/638] Elapsed 10m 10s (remain 0m 0s) Loss: 0.3601(0.4607) Grad: 98617.2891  LR: 0.00000605  \n","EVAL: [0/129] Elapsed 0m 1s (remain 2m 41s) Loss: 0.6226(0.6226) \n","EVAL: [20/129] Elapsed 0m 23s (remain 2m 0s) Loss: 0.5413(0.5598) \n","EVAL: [40/129] Elapsed 0m 44s (remain 1m 36s) Loss: 0.5393(0.5485) \n","EVAL: [60/129] Elapsed 1m 6s (remain 1m 14s) Loss: 0.5752(0.5482) \n","EVAL: [80/129] Elapsed 1m 28s (remain 0m 52s) Loss: 0.5683(0.5403) \n","EVAL: [100/129] Elapsed 1m 49s (remain 0m 30s) Loss: 0.5825(0.5382) \n","EVAL: [120/129] Elapsed 2m 11s (remain 0m 8s) Loss: 0.5326(0.5369) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4607  avg_val_loss: 0.5366  time: 750s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4607  avg_val_loss: 0.5366  time: 750s\n","Epoch 2 - Score: 0.5434  Scores: [0.5260186443579409, 0.5607561135421593]\n","INFO:__main__:Epoch 2 - Score: 0.5434  Scores: [0.5260186443579409, 0.5607561135421593]\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [128/129] Elapsed 2m 19s (remain 0m 0s) Loss: 0.5220(0.5366) \n","Epoch: [3][0/638] Elapsed 0m 1s (remain 12m 52s) Loss: 0.3095(0.3095) Grad: inf  LR: 0.00000603  \n","Epoch: [3][20/638] Elapsed 0m 20s (remain 9m 56s) Loss: 0.5830(0.4038) Grad: 68285.4062  LR: 0.00000570  \n","Epoch: [3][40/638] Elapsed 0m 39s (remain 9m 34s) Loss: 0.3382(0.4007) Grad: 97288.1484  LR: 0.00000537  \n","Epoch: [3][60/638] Elapsed 0m 58s (remain 9m 14s) Loss: 0.2624(0.3871) Grad: 45091.5430  LR: 0.00000505  \n","Epoch: [3][80/638] Elapsed 1m 17s (remain 8m 54s) Loss: 0.4567(0.3910) Grad: 235516.1875  LR: 0.00000474  \n","Epoch: [3][100/638] Elapsed 1m 36s (remain 8m 33s) Loss: 0.4573(0.3948) Grad: 64896.7969  LR: 0.00000443  \n","Epoch: [3][120/638] Elapsed 1m 55s (remain 8m 14s) Loss: 0.2847(0.3961) Grad: 53695.7266  LR: 0.00000413  \n","Epoch: [3][140/638] Elapsed 2m 14s (remain 7m 55s) Loss: 0.5074(0.3951) Grad: 86904.8281  LR: 0.00000384  \n","Epoch: [3][160/638] Elapsed 2m 34s (remain 7m 36s) Loss: 0.5227(0.3937) Grad: 67471.1250  LR: 0.00000356  \n","Epoch: [3][180/638] Elapsed 2m 53s (remain 7m 17s) Loss: 0.2712(0.3940) Grad: 86779.8672  LR: 0.00000329  \n","Epoch: [3][200/638] Elapsed 3m 12s (remain 6m 58s) Loss: 0.3589(0.3937) Grad: 55645.1016  LR: 0.00000302  \n","Epoch: [3][220/638] Elapsed 3m 31s (remain 6m 38s) Loss: 0.4865(0.3937) Grad: 73874.0078  LR: 0.00000276  \n","Epoch: [3][240/638] Elapsed 3m 50s (remain 6m 19s) Loss: 0.4709(0.3919) Grad: 67615.5391  LR: 0.00000252  \n","Epoch: [3][260/638] Elapsed 4m 9s (remain 6m 0s) Loss: 0.4455(0.3916) Grad: 40804.2461  LR: 0.00000228  \n","Epoch: [3][280/638] Elapsed 4m 28s (remain 5m 41s) Loss: 0.2933(0.3904) Grad: 44106.6211  LR: 0.00000205  \n","Epoch: [3][300/638] Elapsed 4m 47s (remain 5m 22s) Loss: 0.3202(0.3878) Grad: 86446.3672  LR: 0.00000184  \n","Epoch: [3][320/638] Elapsed 5m 6s (remain 5m 3s) Loss: 0.5697(0.3884) Grad: 53843.2148  LR: 0.00000163  \n","Epoch: [3][340/638] Elapsed 5m 26s (remain 4m 43s) Loss: 0.5664(0.3892) Grad: 52938.5039  LR: 0.00000144  \n","Epoch: [3][360/638] Elapsed 5m 45s (remain 4m 24s) Loss: 0.4160(0.3885) Grad: 78076.5078  LR: 0.00000126  \n","Epoch: [3][380/638] Elapsed 6m 4s (remain 4m 5s) Loss: 0.3174(0.3882) Grad: 83358.8594  LR: 0.00000108  \n","Epoch: [3][400/638] Elapsed 6m 23s (remain 3m 46s) Loss: 0.3816(0.3866) Grad: 94643.0156  LR: 0.00000093  \n","Epoch: [3][420/638] Elapsed 6m 42s (remain 3m 27s) Loss: 0.3225(0.3855) Grad: 79451.5703  LR: 0.00000078  \n","Epoch: [3][440/638] Elapsed 7m 1s (remain 3m 8s) Loss: 0.4185(0.3861) Grad: 103216.5781  LR: 0.00000064  \n","Epoch: [3][460/638] Elapsed 7m 20s (remain 2m 49s) Loss: 0.3744(0.3849) Grad: 68473.6406  LR: 0.00000052  \n","Epoch: [3][480/638] Elapsed 7m 40s (remain 2m 30s) Loss: 0.3993(0.3833) Grad: 67967.3984  LR: 0.00000041  \n","Epoch: [3][500/638] Elapsed 7m 59s (remain 2m 11s) Loss: 0.4012(0.3821) Grad: 60107.4883  LR: 0.00000031  \n","Epoch: [3][520/638] Elapsed 8m 18s (remain 1m 51s) Loss: 0.3748(0.3815) Grad: 53016.9453  LR: 0.00000023  \n","Epoch: [3][540/638] Elapsed 8m 37s (remain 1m 32s) Loss: 0.3348(0.3808) Grad: 51421.5547  LR: 0.00000016  \n","Epoch: [3][560/638] Elapsed 8m 56s (remain 1m 13s) Loss: 0.3811(0.3811) Grad: 68202.7969  LR: 0.00000010  \n","Epoch: [3][580/638] Elapsed 9m 15s (remain 0m 54s) Loss: 0.2775(0.3810) Grad: 89649.8906  LR: 0.00000006  \n","Epoch: [3][600/638] Elapsed 9m 34s (remain 0m 35s) Loss: 0.3330(0.3806) Grad: 60617.3633  LR: 0.00000002  \n","Epoch: [3][620/638] Elapsed 9m 53s (remain 0m 16s) Loss: 0.4982(0.3802) Grad: 45645.1875  LR: 0.00000001  \n","Epoch: [3][637/638] Elapsed 10m 9s (remain 0m 0s) Loss: 0.3461(0.3800) Grad: 61210.8047  LR: 0.00000000  \n","EVAL: [0/129] Elapsed 0m 1s (remain 2m 40s) Loss: 0.4659(0.4659) \n","EVAL: [20/129] Elapsed 0m 23s (remain 2m 0s) Loss: 0.4344(0.4795) \n","EVAL: [40/129] Elapsed 0m 44s (remain 1m 36s) Loss: 0.4541(0.4692) \n","EVAL: [60/129] Elapsed 1m 6s (remain 1m 14s) Loss: 0.5163(0.4694) \n","EVAL: [80/129] Elapsed 1m 28s (remain 0m 52s) Loss: 0.4688(0.4616) \n","EVAL: [100/129] Elapsed 1m 49s (remain 0m 30s) Loss: 0.5765(0.4596) \n","EVAL: [120/129] Elapsed 2m 11s (remain 0m 8s) Loss: 0.4399(0.4587) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3800  avg_val_loss: 0.4580  time: 749s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3800  avg_val_loss: 0.4580  time: 749s\n","Epoch 3 - Score: 0.4668  Scores: [0.4015583756193256, 0.5320486421358627]\n","INFO:__main__:Epoch 3 - Score: 0.4668  Scores: [0.4015583756193256, 0.5320486421358627]\n","Epoch 3 - Save Best Score: 0.4668 Model\n","INFO:__main__:Epoch 3 - Save Best Score: 0.4668 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [128/129] Elapsed 2m 19s (remain 0m 0s) Loss: 0.4115(0.4580) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 1 result ==========\n","INFO:__main__:========== fold: 1 result ==========\n","Score: 0.4668  Scores: [0.4015583756193256, 0.5320486421358627]\n","INFO:__main__:Score: 0.4668  Scores: [0.4015583756193256, 0.5320486421358627]\n","========== fold: 2 training ==========\n","INFO:__main__:========== fold: 2 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/644] Elapsed 0m 1s (remain 13m 35s) Loss: 1.0684(1.0684) Grad: nan  LR: 0.00000010  \n","Epoch: [1][20/644] Elapsed 0m 20s (remain 10m 4s) Loss: 0.5241(1.0239) Grad: 126936.6953  LR: 0.00000218  \n","Epoch: [1][40/644] Elapsed 0m 39s (remain 9m 38s) Loss: 0.6040(1.0034) Grad: 133394.3750  LR: 0.00000425  \n","Epoch: [1][60/644] Elapsed 0m 58s (remain 9m 19s) Loss: 0.8906(0.9609) Grad: 74216.2188  LR: 0.00000632  \n","Epoch: [1][80/644] Elapsed 1m 17s (remain 8m 59s) Loss: 0.8159(0.9036) Grad: 107826.9609  LR: 0.00000839  \n","Epoch: [1][100/644] Elapsed 1m 36s (remain 8m 40s) Loss: 0.6547(0.8605) Grad: 156958.7344  LR: 0.00001047  \n","Epoch: [1][120/644] Elapsed 1m 56s (remain 8m 21s) Loss: 0.8403(0.8242) Grad: 160177.0156  LR: 0.00001254  \n","Epoch: [1][140/644] Elapsed 2m 14s (remain 8m 1s) Loss: 0.8006(0.7941) Grad: 166899.1719  LR: 0.00001461  \n","Epoch: [1][160/644] Elapsed 2m 34s (remain 7m 42s) Loss: 0.4486(0.7733) Grad: 89758.5078  LR: 0.00001668  \n","Epoch: [1][180/644] Elapsed 2m 53s (remain 7m 23s) Loss: 0.6351(0.7526) Grad: 194000.9375  LR: 0.00001876  \n","Epoch: [1][200/644] Elapsed 3m 12s (remain 7m 4s) Loss: 0.6439(0.7290) Grad: 208484.6406  LR: 0.00002000  \n","Epoch: [1][220/644] Elapsed 3m 31s (remain 6m 45s) Loss: 0.5393(0.7151) Grad: 87017.9297  LR: 0.00001999  \n","Epoch: [1][240/644] Elapsed 3m 50s (remain 6m 25s) Loss: 0.5936(0.7025) Grad: 87902.4688  LR: 0.00001996  \n","Epoch: [1][260/644] Elapsed 4m 9s (remain 6m 6s) Loss: 0.4942(0.6890) Grad: 217265.2344  LR: 0.00001992  \n","Epoch: [1][280/644] Elapsed 4m 28s (remain 5m 47s) Loss: 0.4535(0.6781) Grad: 174089.4062  LR: 0.00001987  \n","Epoch: [1][300/644] Elapsed 4m 47s (remain 5m 28s) Loss: 0.5944(0.6692) Grad: 155295.7188  LR: 0.00001981  \n","Epoch: [1][320/644] Elapsed 5m 7s (remain 5m 9s) Loss: 0.5703(0.6598) Grad: 161514.4219  LR: 0.00001973  \n","Epoch: [1][340/644] Elapsed 5m 26s (remain 4m 49s) Loss: 0.6053(0.6529) Grad: 99418.2578  LR: 0.00001965  \n","Epoch: [1][360/644] Elapsed 5m 45s (remain 4m 30s) Loss: 0.4972(0.6454) Grad: 79485.1641  LR: 0.00001954  \n","Epoch: [1][380/644] Elapsed 6m 4s (remain 4m 11s) Loss: 0.5961(0.6378) Grad: 99978.0000  LR: 0.00001943  \n","Epoch: [1][400/644] Elapsed 6m 23s (remain 3m 52s) Loss: 0.4851(0.6326) Grad: 71242.6406  LR: 0.00001930  \n","Epoch: [1][420/644] Elapsed 6m 42s (remain 3m 33s) Loss: 0.6022(0.6269) Grad: 181746.3594  LR: 0.00001916  \n","Epoch: [1][440/644] Elapsed 7m 1s (remain 3m 14s) Loss: 0.6014(0.6209) Grad: 172347.2812  LR: 0.00001901  \n","Epoch: [1][460/644] Elapsed 7m 20s (remain 2m 54s) Loss: 0.3645(0.6139) Grad: 56141.4102  LR: 0.00001885  \n","Epoch: [1][480/644] Elapsed 7m 39s (remain 2m 35s) Loss: 0.3282(0.6074) Grad: 75958.6641  LR: 0.00001868  \n","Epoch: [1][500/644] Elapsed 7m 59s (remain 2m 16s) Loss: 0.4450(0.6023) Grad: 109139.5000  LR: 0.00001849  \n","Epoch: [1][520/644] Elapsed 8m 18s (remain 1m 57s) Loss: 0.5214(0.5978) Grad: 94429.8672  LR: 0.00001830  \n","Epoch: [1][540/644] Elapsed 8m 37s (remain 1m 38s) Loss: 0.5836(0.5940) Grad: 105591.3906  LR: 0.00001809  \n","Epoch: [1][560/644] Elapsed 8m 56s (remain 1m 19s) Loss: 0.5241(0.5917) Grad: 89141.6797  LR: 0.00001787  \n","Epoch: [1][580/644] Elapsed 9m 15s (remain 1m 0s) Loss: 0.6367(0.5877) Grad: 106461.0312  LR: 0.00001764  \n","Epoch: [1][600/644] Elapsed 9m 34s (remain 0m 41s) Loss: 0.5232(0.5830) Grad: 126670.3047  LR: 0.00001741  \n","Epoch: [1][620/644] Elapsed 9m 54s (remain 0m 22s) Loss: 0.6830(0.5805) Grad: 59877.3867  LR: 0.00001716  \n","Epoch: [1][640/644] Elapsed 10m 13s (remain 0m 2s) Loss: 0.4676(0.5779) Grad: 120947.1797  LR: 0.00001690  \n","Epoch: [1][643/644] Elapsed 10m 16s (remain 0m 0s) Loss: 0.4964(0.5774) Grad: 113629.3984  LR: 0.00001686  \n","EVAL: [0/126] Elapsed 0m 1s (remain 2m 56s) Loss: 0.8454(0.8454) \n","EVAL: [20/126] Elapsed 0m 23s (remain 1m 56s) Loss: 0.6322(0.6796) \n","EVAL: [40/126] Elapsed 0m 45s (remain 1m 34s) Loss: 0.7277(0.6691) \n","EVAL: [60/126] Elapsed 1m 7s (remain 1m 11s) Loss: 0.7957(0.6676) \n","EVAL: [80/126] Elapsed 1m 29s (remain 0m 49s) Loss: 0.7309(0.6715) \n","EVAL: [100/126] Elapsed 1m 50s (remain 0m 27s) Loss: 0.6836(0.6732) \n","EVAL: [120/126] Elapsed 2m 12s (remain 0m 5s) Loss: 0.7899(0.6706) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5774  avg_val_loss: 0.6705  time: 754s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.5774  avg_val_loss: 0.6705  time: 754s\n","Epoch 1 - Score: 0.6829  Scores: [0.5077849531973613, 0.8580032447558141]\n","INFO:__main__:Epoch 1 - Score: 0.6829  Scores: [0.5077849531973613, 0.8580032447558141]\n","Epoch 1 - Save Best Score: 0.6829 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.6829 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [125/126] Elapsed 2m 17s (remain 0m 0s) Loss: 0.5499(0.6705) \n","Epoch: [2][0/644] Elapsed 0m 1s (remain 13m 7s) Loss: 0.5725(0.5725) Grad: inf  LR: 0.00001685  \n","Epoch: [2][20/644] Elapsed 0m 20s (remain 10m 0s) Loss: 0.6008(0.4650) Grad: 125468.5938  LR: 0.00001658  \n","Epoch: [2][40/644] Elapsed 0m 39s (remain 9m 39s) Loss: 0.5785(0.4506) Grad: 95512.3984  LR: 0.00001631  \n","Epoch: [2][60/644] Elapsed 0m 58s (remain 9m 19s) Loss: 0.2442(0.4535) Grad: 84089.7969  LR: 0.00001602  \n","Epoch: [2][80/644] Elapsed 1m 17s (remain 9m 0s) Loss: 0.3686(0.4442) Grad: 70140.7891  LR: 0.00001573  \n","Epoch: [2][100/644] Elapsed 1m 36s (remain 8m 41s) Loss: 0.5703(0.4432) Grad: 105063.7109  LR: 0.00001543  \n","Epoch: [2][120/644] Elapsed 1m 56s (remain 8m 21s) Loss: 0.5250(0.4474) Grad: 140400.1719  LR: 0.00001512  \n","Epoch: [2][140/644] Elapsed 2m 15s (remain 8m 2s) Loss: 0.5055(0.4419) Grad: 113954.2812  LR: 0.00001481  \n","Epoch: [2][160/644] Elapsed 2m 34s (remain 7m 42s) Loss: 0.5532(0.4407) Grad: 138775.6875  LR: 0.00001449  \n","Epoch: [2][180/644] Elapsed 2m 53s (remain 7m 23s) Loss: 0.4278(0.4445) Grad: 58716.8789  LR: 0.00001417  \n","Epoch: [2][200/644] Elapsed 3m 12s (remain 7m 4s) Loss: 0.4290(0.4431) Grad: 98766.7578  LR: 0.00001384  \n","Epoch: [2][220/644] Elapsed 3m 31s (remain 6m 45s) Loss: 0.5030(0.4449) Grad: 127332.2266  LR: 0.00001350  \n","Epoch: [2][240/644] Elapsed 3m 50s (remain 6m 26s) Loss: 0.4915(0.4435) Grad: 41826.1680  LR: 0.00001316  \n","Epoch: [2][260/644] Elapsed 4m 10s (remain 6m 6s) Loss: 0.4082(0.4439) Grad: 113253.5859  LR: 0.00001281  \n","Epoch: [2][280/644] Elapsed 4m 29s (remain 5m 47s) Loss: 0.5077(0.4441) Grad: 42483.9805  LR: 0.00001247  \n","Epoch: [2][300/644] Elapsed 4m 47s (remain 5m 28s) Loss: 0.3750(0.4443) Grad: 65521.2148  LR: 0.00001211  \n","Epoch: [2][320/644] Elapsed 5m 7s (remain 5m 9s) Loss: 0.3124(0.4403) Grad: 64100.0469  LR: 0.00001176  \n","Epoch: [2][340/644] Elapsed 5m 26s (remain 4m 49s) Loss: 0.4162(0.4380) Grad: 77552.6094  LR: 0.00001140  \n","Epoch: [2][360/644] Elapsed 5m 45s (remain 4m 30s) Loss: 0.7114(0.4396) Grad: 84115.8516  LR: 0.00001105  \n","Epoch: [2][380/644] Elapsed 6m 4s (remain 4m 11s) Loss: 0.4924(0.4387) Grad: 101949.1641  LR: 0.00001069  \n","Epoch: [2][400/644] Elapsed 6m 23s (remain 3m 52s) Loss: 0.4828(0.4363) Grad: 95992.7188  LR: 0.00001032  \n","Epoch: [2][420/644] Elapsed 6m 42s (remain 3m 33s) Loss: 0.4422(0.4347) Grad: 27764.8340  LR: 0.00000996  \n","Epoch: [2][440/644] Elapsed 7m 1s (remain 3m 14s) Loss: 0.4458(0.4329) Grad: 50105.7539  LR: 0.00000960  \n","Epoch: [2][460/644] Elapsed 7m 20s (remain 2m 54s) Loss: 0.3457(0.4321) Grad: 125054.2188  LR: 0.00000924  \n","Epoch: [2][480/644] Elapsed 7m 39s (remain 2m 35s) Loss: 0.3127(0.4323) Grad: 94935.8125  LR: 0.00000888  \n","Epoch: [2][500/644] Elapsed 7m 58s (remain 2m 16s) Loss: 0.4710(0.4314) Grad: 107633.1719  LR: 0.00000852  \n","Epoch: [2][520/644] Elapsed 8m 18s (remain 1m 57s) Loss: 0.5167(0.4326) Grad: 113867.6719  LR: 0.00000817  \n","Epoch: [2][540/644] Elapsed 8m 37s (remain 1m 38s) Loss: 0.3335(0.4332) Grad: 94218.5938  LR: 0.00000782  \n","Epoch: [2][560/644] Elapsed 8m 56s (remain 1m 19s) Loss: 0.4779(0.4333) Grad: 60283.3555  LR: 0.00000746  \n","Epoch: [2][580/644] Elapsed 9m 15s (remain 1m 0s) Loss: 0.4062(0.4327) Grad: 44040.5234  LR: 0.00000712  \n","Epoch: [2][600/644] Elapsed 9m 34s (remain 0m 41s) Loss: 0.2940(0.4313) Grad: 95908.4609  LR: 0.00000677  \n","Epoch: [2][620/644] Elapsed 9m 53s (remain 0m 21s) Loss: 0.3840(0.4301) Grad: 101112.7109  LR: 0.00000643  \n","Epoch: [2][640/644] Elapsed 10m 12s (remain 0m 2s) Loss: 0.3731(0.4295) Grad: 83267.8125  LR: 0.00000610  \n","Epoch: [2][643/644] Elapsed 10m 15s (remain 0m 0s) Loss: 0.4815(0.4299) Grad: 152110.8750  LR: 0.00000605  \n","EVAL: [0/126] Elapsed 0m 1s (remain 2m 55s) Loss: 0.8017(0.8017) \n","EVAL: [20/126] Elapsed 0m 23s (remain 1m 55s) Loss: 0.5623(0.6041) \n","EVAL: [40/126] Elapsed 0m 45s (remain 1m 34s) Loss: 0.7116(0.5997) \n","EVAL: [60/126] Elapsed 1m 7s (remain 1m 11s) Loss: 0.7825(0.5967) \n","EVAL: [80/126] Elapsed 1m 29s (remain 0m 49s) Loss: 0.6764(0.6002) \n","EVAL: [100/126] Elapsed 1m 50s (remain 0m 27s) Loss: 0.6267(0.5988) \n","EVAL: [120/126] Elapsed 2m 12s (remain 0m 5s) Loss: 0.7311(0.5992) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4299  avg_val_loss: 0.5986  time: 753s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4299  avg_val_loss: 0.5986  time: 753s\n","Epoch 2 - Score: 0.6138  Scores: [0.5544364813182208, 0.6732520693955895]\n","INFO:__main__:Epoch 2 - Score: 0.6138  Scores: [0.5544364813182208, 0.6732520693955895]\n","Epoch 2 - Save Best Score: 0.6138 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.6138 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [125/126] Elapsed 2m 17s (remain 0m 0s) Loss: 0.5015(0.5986) \n","Epoch: [3][0/644] Elapsed 0m 1s (remain 13m 16s) Loss: 0.3554(0.3554) Grad: inf  LR: 0.00000603  \n","Epoch: [3][20/644] Elapsed 0m 20s (remain 10m 3s) Loss: 0.2949(0.3789) Grad: 117566.0078  LR: 0.00000570  \n","Epoch: [3][40/644] Elapsed 0m 39s (remain 9m 41s) Loss: 0.2854(0.3763) Grad: 48506.4375  LR: 0.00000538  \n","Epoch: [3][60/644] Elapsed 0m 58s (remain 9m 20s) Loss: 0.3052(0.3681) Grad: 95690.4219  LR: 0.00000506  \n","Epoch: [3][80/644] Elapsed 1m 17s (remain 9m 1s) Loss: 0.3271(0.3647) Grad: 122937.5234  LR: 0.00000475  \n","Epoch: [3][100/644] Elapsed 1m 36s (remain 8m 40s) Loss: 0.3406(0.3621) Grad: 112821.3438  LR: 0.00000445  \n","Epoch: [3][120/644] Elapsed 1m 55s (remain 8m 21s) Loss: 0.2088(0.3628) Grad: 35493.9883  LR: 0.00000415  \n","Epoch: [3][140/644] Elapsed 2m 15s (remain 8m 2s) Loss: 0.3575(0.3620) Grad: 84630.3281  LR: 0.00000386  \n","Epoch: [3][160/644] Elapsed 2m 34s (remain 7m 42s) Loss: 0.3653(0.3594) Grad: 88908.1094  LR: 0.00000358  \n","Epoch: [3][180/644] Elapsed 2m 53s (remain 7m 23s) Loss: 0.2833(0.3575) Grad: 89700.3125  LR: 0.00000331  \n","Epoch: [3][200/644] Elapsed 3m 12s (remain 7m 4s) Loss: 0.2923(0.3581) Grad: 72750.6094  LR: 0.00000304  \n","Epoch: [3][220/644] Elapsed 3m 31s (remain 6m 44s) Loss: 0.2637(0.3595) Grad: 79053.1953  LR: 0.00000279  \n","Epoch: [3][240/644] Elapsed 3m 50s (remain 6m 25s) Loss: 0.5961(0.3575) Grad: 87425.3594  LR: 0.00000254  \n","Epoch: [3][260/644] Elapsed 4m 9s (remain 6m 6s) Loss: 0.3686(0.3571) Grad: 102301.7734  LR: 0.00000231  \n","Epoch: [3][280/644] Elapsed 4m 28s (remain 5m 47s) Loss: 0.3177(0.3528) Grad: 64568.0430  LR: 0.00000208  \n","Epoch: [3][300/644] Elapsed 4m 47s (remain 5m 27s) Loss: 0.2866(0.3527) Grad: 51687.4141  LR: 0.00000187  \n","Epoch: [3][320/644] Elapsed 5m 6s (remain 5m 8s) Loss: 0.4679(0.3541) Grad: 104420.0625  LR: 0.00000166  \n","Epoch: [3][340/644] Elapsed 5m 25s (remain 4m 49s) Loss: 0.5376(0.3527) Grad: 138527.2812  LR: 0.00000147  \n","Epoch: [3][360/644] Elapsed 5m 45s (remain 4m 30s) Loss: 0.3109(0.3544) Grad: 56321.6914  LR: 0.00000129  \n","Epoch: [3][380/644] Elapsed 6m 3s (remain 4m 11s) Loss: 0.2671(0.3534) Grad: 110665.7266  LR: 0.00000111  \n","Epoch: [3][400/644] Elapsed 6m 22s (remain 3m 51s) Loss: 0.2117(0.3526) Grad: 112895.8828  LR: 0.00000095  \n","Epoch: [3][420/644] Elapsed 6m 41s (remain 3m 32s) Loss: 0.4301(0.3523) Grad: 104724.0391  LR: 0.00000081  \n","Epoch: [3][440/644] Elapsed 7m 1s (remain 3m 13s) Loss: 0.2984(0.3507) Grad: 83588.3516  LR: 0.00000067  \n","Epoch: [3][460/644] Elapsed 7m 20s (remain 2m 54s) Loss: 0.3972(0.3507) Grad: 36151.3477  LR: 0.00000055  \n","Epoch: [3][480/644] Elapsed 7m 39s (remain 2m 35s) Loss: 0.2996(0.3487) Grad: 74358.9375  LR: 0.00000044  \n","Epoch: [3][500/644] Elapsed 7m 58s (remain 2m 16s) Loss: 0.2969(0.3484) Grad: 56386.8594  LR: 0.00000034  \n","Epoch: [3][520/644] Elapsed 8m 17s (remain 1m 57s) Loss: 0.3849(0.3479) Grad: 37905.1016  LR: 0.00000025  \n","Epoch: [3][540/644] Elapsed 8m 36s (remain 1m 38s) Loss: 0.4193(0.3477) Grad: 50067.1875  LR: 0.00000018  \n","Epoch: [3][560/644] Elapsed 8m 55s (remain 1m 19s) Loss: 0.3358(0.3478) Grad: 87750.1875  LR: 0.00000011  \n","Epoch: [3][580/644] Elapsed 9m 14s (remain 1m 0s) Loss: 0.2984(0.3481) Grad: 50181.4453  LR: 0.00000007  \n","Epoch: [3][600/644] Elapsed 9m 33s (remain 0m 41s) Loss: 0.3568(0.3467) Grad: 93048.9766  LR: 0.00000003  \n","Epoch: [3][620/644] Elapsed 9m 52s (remain 0m 21s) Loss: 0.2836(0.3465) Grad: 86484.4062  LR: 0.00000001  \n","Epoch: [3][640/644] Elapsed 10m 11s (remain 0m 2s) Loss: 0.4840(0.3461) Grad: 77386.2656  LR: 0.00000000  \n","Epoch: [3][643/644] Elapsed 10m 14s (remain 0m 0s) Loss: 0.3036(0.3460) Grad: 96425.7422  LR: 0.00000000  \n","EVAL: [0/126] Elapsed 0m 1s (remain 2m 56s) Loss: 0.7208(0.7208) \n","EVAL: [20/126] Elapsed 0m 23s (remain 1m 56s) Loss: 0.4456(0.5121) \n","EVAL: [40/126] Elapsed 0m 45s (remain 1m 34s) Loss: 0.5109(0.5131) \n","EVAL: [60/126] Elapsed 1m 7s (remain 1m 11s) Loss: 0.5939(0.5075) \n","EVAL: [80/126] Elapsed 1m 29s (remain 0m 49s) Loss: 0.5575(0.5079) \n","EVAL: [100/126] Elapsed 1m 50s (remain 0m 27s) Loss: 0.5272(0.5059) \n","EVAL: [120/126] Elapsed 2m 12s (remain 0m 5s) Loss: 0.6258(0.5109) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3460  avg_val_loss: 0.5103  time: 752s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3460  avg_val_loss: 0.5103  time: 752s\n","Epoch 3 - Score: 0.5218  Scores: [0.44299797437229677, 0.6005037246493298]\n","INFO:__main__:Epoch 3 - Score: 0.5218  Scores: [0.44299797437229677, 0.6005037246493298]\n","Epoch 3 - Save Best Score: 0.5218 Model\n","INFO:__main__:Epoch 3 - Save Best Score: 0.5218 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [125/126] Elapsed 2m 17s (remain 0m 0s) Loss: 0.3124(0.5103) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 2 result ==========\n","INFO:__main__:========== fold: 2 result ==========\n","Score: 0.5218  Scores: [0.44299797437229677, 0.6005037246493298]\n","INFO:__main__:Score: 0.5218  Scores: [0.44299797437229677, 0.6005037246493298]\n","========== fold: 3 training ==========\n","INFO:__main__:========== fold: 3 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/646] Elapsed 0m 0s (remain 10m 23s) Loss: 1.2057(1.2057) Grad: inf  LR: 0.00000010  \n","Epoch: [1][20/646] Elapsed 0m 16s (remain 8m 18s) Loss: 1.0208(1.0820) Grad: 175473.0781  LR: 0.00000218  \n","Epoch: [1][40/646] Elapsed 0m 32s (remain 7m 59s) Loss: 0.7648(1.0777) Grad: 145723.5938  LR: 0.00000425  \n","Epoch: [1][60/646] Elapsed 0m 48s (remain 7m 48s) Loss: 0.8300(1.0589) Grad: 158430.4219  LR: 0.00000632  \n","Epoch: [1][80/646] Elapsed 1m 4s (remain 7m 30s) Loss: 0.6461(1.0020) Grad: 1467644.5000  LR: 0.00000839  \n","Epoch: [1][100/646] Elapsed 1m 20s (remain 7m 14s) Loss: 0.7760(0.9539) Grad: 347473.9375  LR: 0.00001047  \n","Epoch: [1][120/646] Elapsed 1m 35s (remain 6m 56s) Loss: 0.5553(0.9045) Grad: 435019.0000  LR: 0.00001254  \n","Epoch: [1][140/646] Elapsed 1m 51s (remain 6m 38s) Loss: 0.6411(0.8802) Grad: 139978.8125  LR: 0.00001461  \n","Epoch: [1][160/646] Elapsed 2m 6s (remain 6m 22s) Loss: 0.4983(0.8529) Grad: 89733.1562  LR: 0.00001668  \n","Epoch: [1][180/646] Elapsed 2m 22s (remain 6m 6s) Loss: 0.4183(0.8238) Grad: 86147.0234  LR: 0.00001876  \n","Epoch: [1][200/646] Elapsed 2m 38s (remain 5m 50s) Loss: 0.5067(0.7968) Grad: 38063.4922  LR: 0.00002000  \n","Epoch: [1][220/646] Elapsed 2m 53s (remain 5m 34s) Loss: 0.6255(0.7733) Grad: 86112.1641  LR: 0.00001999  \n","Epoch: [1][240/646] Elapsed 3m 8s (remain 5m 17s) Loss: 0.6989(0.7537) Grad: 98903.7344  LR: 0.00001996  \n","Epoch: [1][260/646] Elapsed 3m 24s (remain 5m 1s) Loss: 0.5610(0.7384) Grad: 34075.2891  LR: 0.00001993  \n","Epoch: [1][280/646] Elapsed 3m 39s (remain 4m 44s) Loss: 0.6701(0.7328) Grad: 54031.8867  LR: 0.00001987  \n","Epoch: [1][300/646] Elapsed 3m 55s (remain 4m 29s) Loss: 0.7046(0.7244) Grad: 79728.0703  LR: 0.00001981  \n","Epoch: [1][320/646] Elapsed 4m 10s (remain 4m 13s) Loss: 0.4904(0.7145) Grad: 67472.4688  LR: 0.00001974  \n","Epoch: [1][340/646] Elapsed 4m 25s (remain 3m 57s) Loss: 0.6829(0.7064) Grad: 61549.2734  LR: 0.00001965  \n","Epoch: [1][360/646] Elapsed 4m 40s (remain 3m 41s) Loss: 0.5440(0.6948) Grad: 48139.1016  LR: 0.00001955  \n","Epoch: [1][380/646] Elapsed 4m 55s (remain 3m 25s) Loss: 0.4302(0.6853) Grad: 75633.5938  LR: 0.00001943  \n","Epoch: [1][400/646] Elapsed 5m 11s (remain 3m 10s) Loss: 0.4487(0.6804) Grad: 31555.4980  LR: 0.00001931  \n","Epoch: [1][420/646] Elapsed 5m 27s (remain 2m 54s) Loss: 1.0894(0.6776) Grad: 1245903.0000  LR: 0.00001917  \n","Epoch: [1][440/646] Elapsed 5m 42s (remain 2m 39s) Loss: 0.3755(0.6745) Grad: 53847.9766  LR: 0.00001902  \n","Epoch: [1][460/646] Elapsed 5m 58s (remain 2m 23s) Loss: 0.4293(0.6699) Grad: 29178.6582  LR: 0.00001886  \n","Epoch: [1][480/646] Elapsed 6m 13s (remain 2m 8s) Loss: 0.4154(0.6622) Grad: 63526.9492  LR: 0.00001869  \n","Epoch: [1][500/646] Elapsed 6m 30s (remain 1m 52s) Loss: 0.4306(0.6585) Grad: 27200.4590  LR: 0.00001850  \n","Epoch: [1][520/646] Elapsed 6m 45s (remain 1m 37s) Loss: 0.4440(0.6528) Grad: 27664.8672  LR: 0.00001831  \n","Epoch: [1][540/646] Elapsed 7m 1s (remain 1m 21s) Loss: 0.4088(0.6458) Grad: 75359.4062  LR: 0.00001810  \n","Epoch: [1][560/646] Elapsed 7m 16s (remain 1m 6s) Loss: 0.5838(0.6406) Grad: 50273.7812  LR: 0.00001788  \n","Epoch: [1][580/646] Elapsed 7m 31s (remain 0m 50s) Loss: 0.6207(0.6360) Grad: 60944.6602  LR: 0.00001766  \n","Epoch: [1][600/646] Elapsed 7m 47s (remain 0m 34s) Loss: 0.4703(0.6304) Grad: 35850.4805  LR: 0.00001742  \n","Epoch: [1][620/646] Elapsed 8m 3s (remain 0m 19s) Loss: 0.6600(0.6275) Grad: 39722.0781  LR: 0.00001718  \n","Epoch: [1][640/646] Elapsed 8m 18s (remain 0m 3s) Loss: 0.4998(0.6241) Grad: 55550.5234  LR: 0.00001692  \n","Epoch: [1][645/646] Elapsed 8m 22s (remain 0m 0s) Loss: 0.6241(0.6233) Grad: 56950.5312  LR: 0.00001685  \n","EVAL: [0/125] Elapsed 0m 1s (remain 3m 14s) Loss: 0.6522(0.6522) \n","EVAL: [20/125] Elapsed 0m 26s (remain 2m 11s) Loss: 0.7774(0.6808) \n","EVAL: [40/125] Elapsed 0m 51s (remain 1m 45s) Loss: 0.6326(0.6962) \n","EVAL: [60/125] Elapsed 1m 16s (remain 1m 20s) Loss: 0.7807(0.6913) \n","EVAL: [80/125] Elapsed 1m 41s (remain 0m 55s) Loss: 0.6186(0.6826) \n","EVAL: [100/125] Elapsed 2m 6s (remain 0m 30s) Loss: 0.7407(0.6750) \n","EVAL: [120/125] Elapsed 2m 31s (remain 0m 5s) Loss: 0.6165(0.6737) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.6233  avg_val_loss: 0.6726  time: 659s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.6233  avg_val_loss: 0.6726  time: 659s\n","Epoch 1 - Score: 0.6793  Scores: [0.7015418643227753, 0.6571364512585932]\n","INFO:__main__:Epoch 1 - Score: 0.6793  Scores: [0.7015418643227753, 0.6571364512585932]\n","Epoch 1 - Save Best Score: 0.6793 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.6793 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [124/125] Elapsed 2m 36s (remain 0m 0s) Loss: 0.5507(0.6726) \n","Epoch: [2][0/646] Elapsed 0m 1s (remain 11m 42s) Loss: 0.7018(0.7018) Grad: inf  LR: 0.00001684  \n","Epoch: [2][20/646] Elapsed 0m 16s (remain 8m 11s) Loss: 0.4827(0.6201) Grad: 161293.3125  LR: 0.00001657  \n","Epoch: [2][40/646] Elapsed 0m 31s (remain 7m 43s) Loss: 0.4667(0.5399) Grad: 135726.4375  LR: 0.00001630  \n","Epoch: [2][60/646] Elapsed 0m 47s (remain 7m 31s) Loss: 0.4690(0.5239) Grad: 87728.0859  LR: 0.00001602  \n","Epoch: [2][80/646] Elapsed 1m 2s (remain 7m 16s) Loss: 0.6649(0.5078) Grad: 158835.2344  LR: 0.00001572  \n","Epoch: [2][100/646] Elapsed 1m 17s (remain 7m 0s) Loss: 0.3932(0.4987) Grad: 103212.7422  LR: 0.00001542  \n","Epoch: [2][120/646] Elapsed 1m 33s (remain 6m 45s) Loss: 0.4877(0.4882) Grad: 88393.0703  LR: 0.00001512  \n","Epoch: [2][140/646] Elapsed 1m 49s (remain 6m 30s) Loss: 0.3858(0.4830) Grad: 47608.2344  LR: 0.00001481  \n","Epoch: [2][160/646] Elapsed 2m 4s (remain 6m 14s) Loss: 0.3353(0.4794) Grad: 126175.3359  LR: 0.00001449  \n","Epoch: [2][180/646] Elapsed 2m 19s (remain 5m 57s) Loss: 0.4273(0.4775) Grad: 99016.0547  LR: 0.00001416  \n","Epoch: [2][200/646] Elapsed 2m 34s (remain 5m 42s) Loss: 0.2486(0.4741) Grad: 72247.0078  LR: 0.00001383  \n","Epoch: [2][220/646] Elapsed 2m 51s (remain 5m 28s) Loss: 0.3624(0.4748) Grad: 97188.3359  LR: 0.00001350  \n","Epoch: [2][240/646] Elapsed 3m 6s (remain 5m 13s) Loss: 0.5138(0.4725) Grad: 113779.1875  LR: 0.00001316  \n","Epoch: [2][260/646] Elapsed 3m 21s (remain 4m 57s) Loss: 0.3000(0.4697) Grad: 34435.3516  LR: 0.00001281  \n","Epoch: [2][280/646] Elapsed 3m 36s (remain 4m 40s) Loss: 0.4044(0.4660) Grad: 79466.5000  LR: 0.00001247  \n","Epoch: [2][300/646] Elapsed 3m 51s (remain 4m 25s) Loss: 0.4352(0.4632) Grad: 116871.1641  LR: 0.00001212  \n","Epoch: [2][320/646] Elapsed 4m 6s (remain 4m 10s) Loss: 0.4233(0.4620) Grad: 116287.7969  LR: 0.00001176  \n","Epoch: [2][340/646] Elapsed 4m 22s (remain 3m 54s) Loss: 0.3418(0.4581) Grad: 101051.8438  LR: 0.00001141  \n","Epoch: [2][360/646] Elapsed 4m 37s (remain 3m 39s) Loss: 0.6884(0.4592) Grad: 136460.4688  LR: 0.00001105  \n","Epoch: [2][380/646] Elapsed 4m 52s (remain 3m 23s) Loss: 0.3625(0.4567) Grad: 87884.7266  LR: 0.00001069  \n","Epoch: [2][400/646] Elapsed 5m 8s (remain 3m 8s) Loss: 0.4192(0.4547) Grad: 52555.9570  LR: 0.00001033  \n","Epoch: [2][420/646] Elapsed 5m 23s (remain 2m 53s) Loss: 0.4090(0.4545) Grad: 142207.4219  LR: 0.00000997  \n","Epoch: [2][440/646] Elapsed 5m 40s (remain 2m 38s) Loss: 0.4039(0.4544) Grad: 88626.5234  LR: 0.00000961  \n","Epoch: [2][460/646] Elapsed 5m 55s (remain 2m 22s) Loss: 0.2754(0.4546) Grad: 83423.1094  LR: 0.00000925  \n","Epoch: [2][480/646] Elapsed 6m 11s (remain 2m 7s) Loss: 0.8016(0.4564) Grad: 95890.6562  LR: 0.00000890  \n","Epoch: [2][500/646] Elapsed 6m 27s (remain 1m 52s) Loss: 0.3695(0.4566) Grad: 116634.5938  LR: 0.00000854  \n","Epoch: [2][520/646] Elapsed 6m 42s (remain 1m 36s) Loss: 0.5796(0.4561) Grad: 144919.5469  LR: 0.00000818  \n","Epoch: [2][540/646] Elapsed 6m 58s (remain 1m 21s) Loss: 0.3323(0.4543) Grad: 46897.2266  LR: 0.00000783  \n","Epoch: [2][560/646] Elapsed 7m 13s (remain 1m 5s) Loss: 0.5032(0.4528) Grad: 60466.5078  LR: 0.00000748  \n","Epoch: [2][580/646] Elapsed 7m 29s (remain 0m 50s) Loss: 0.4955(0.4514) Grad: 101611.2500  LR: 0.00000713  \n","Epoch: [2][600/646] Elapsed 7m 45s (remain 0m 34s) Loss: 0.4114(0.4530) Grad: 103664.5156  LR: 0.00000679  \n","Epoch: [2][620/646] Elapsed 8m 1s (remain 0m 19s) Loss: 0.3140(0.4516) Grad: 55399.0117  LR: 0.00000645  \n","Epoch: [2][640/646] Elapsed 8m 17s (remain 0m 3s) Loss: 0.4573(0.4515) Grad: 144225.0156  LR: 0.00000612  \n","Epoch: [2][645/646] Elapsed 8m 21s (remain 0m 0s) Loss: 0.3847(0.4512) Grad: 59708.6641  LR: 0.00000603  \n","EVAL: [0/125] Elapsed 0m 1s (remain 3m 12s) Loss: 0.4645(0.4645) \n","EVAL: [20/125] Elapsed 0m 26s (remain 2m 11s) Loss: 0.5602(0.5004) \n","EVAL: [40/125] Elapsed 0m 51s (remain 1m 45s) Loss: 0.4627(0.5169) \n","EVAL: [60/125] Elapsed 1m 16s (remain 1m 20s) Loss: 0.5788(0.5106) \n","EVAL: [80/125] Elapsed 1m 41s (remain 0m 55s) Loss: 0.5410(0.5105) \n","EVAL: [100/125] Elapsed 2m 6s (remain 0m 30s) Loss: 0.5051(0.4999) \n","EVAL: [120/125] Elapsed 2m 31s (remain 0m 5s) Loss: 0.4625(0.5029) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4512  avg_val_loss: 0.5017  time: 658s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4512  avg_val_loss: 0.5017  time: 658s\n","Epoch 2 - Score: 0.5103  Scores: [0.44061196686615334, 0.580036931136618]\n","INFO:__main__:Epoch 2 - Score: 0.5103  Scores: [0.44061196686615334, 0.580036931136618]\n","Epoch 2 - Save Best Score: 0.5103 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.5103 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [124/125] Elapsed 2m 36s (remain 0m 0s) Loss: 0.4582(0.5017) \n","Epoch: [3][0/646] Elapsed 0m 1s (remain 11m 26s) Loss: 0.4779(0.4779) Grad: inf  LR: 0.00000602  \n","Epoch: [3][20/646] Elapsed 0m 16s (remain 8m 13s) Loss: 0.3276(0.4005) Grad: 73878.7109  LR: 0.00000569  \n","Epoch: [3][40/646] Elapsed 0m 32s (remain 7m 53s) Loss: 0.3260(0.3851) Grad: 118728.5703  LR: 0.00000537  \n","Epoch: [3][60/646] Elapsed 0m 48s (remain 7m 41s) Loss: 0.3681(0.3803) Grad: 98236.6875  LR: 0.00000505  \n","Epoch: [3][80/646] Elapsed 1m 3s (remain 7m 24s) Loss: 0.2438(0.3748) Grad: 36530.0586  LR: 0.00000474  \n","Epoch: [3][100/646] Elapsed 1m 19s (remain 7m 7s) Loss: 0.4144(0.3744) Grad: 116516.7188  LR: 0.00000444  \n","Epoch: [3][120/646] Elapsed 1m 34s (remain 6m 50s) Loss: 0.2521(0.3731) Grad: 92218.9766  LR: 0.00000414  \n","Epoch: [3][140/646] Elapsed 1m 49s (remain 6m 33s) Loss: 0.4213(0.3727) Grad: 86390.5078  LR: 0.00000386  \n","Epoch: [3][160/646] Elapsed 2m 4s (remain 6m 16s) Loss: 0.4733(0.3726) Grad: 104489.7344  LR: 0.00000358  \n","Epoch: [3][180/646] Elapsed 2m 20s (remain 6m 2s) Loss: 0.2873(0.3718) Grad: 49646.6836  LR: 0.00000330  \n","Epoch: [3][200/646] Elapsed 2m 37s (remain 5m 47s) Loss: 0.5555(0.3712) Grad: 70216.6172  LR: 0.00000304  \n","Epoch: [3][220/646] Elapsed 2m 52s (remain 5m 32s) Loss: 0.4701(0.3713) Grad: 104696.1953  LR: 0.00000279  \n","Epoch: [3][240/646] Elapsed 3m 9s (remain 5m 18s) Loss: 0.3819(0.3716) Grad: 57575.0039  LR: 0.00000254  \n","Epoch: [3][260/646] Elapsed 3m 25s (remain 5m 2s) Loss: 0.4269(0.3708) Grad: 48111.8203  LR: 0.00000231  \n","Epoch: [3][280/646] Elapsed 3m 40s (remain 4m 45s) Loss: 0.3631(0.3685) Grad: 111178.5312  LR: 0.00000208  \n","Epoch: [3][300/646] Elapsed 3m 54s (remain 4m 29s) Loss: 0.3610(0.3662) Grad: 64683.9375  LR: 0.00000187  \n","Epoch: [3][320/646] Elapsed 4m 10s (remain 4m 13s) Loss: 0.3563(0.3650) Grad: 100004.4609  LR: 0.00000166  \n","Epoch: [3][340/646] Elapsed 4m 25s (remain 3m 57s) Loss: 0.2672(0.3655) Grad: 96976.4375  LR: 0.00000147  \n","Epoch: [3][360/646] Elapsed 4m 41s (remain 3m 41s) Loss: 0.4612(0.3656) Grad: 57893.6289  LR: 0.00000129  \n","Epoch: [3][380/646] Elapsed 4m 56s (remain 3m 26s) Loss: 0.3613(0.3648) Grad: 60408.1367  LR: 0.00000112  \n","Epoch: [3][400/646] Elapsed 5m 12s (remain 3m 11s) Loss: 0.2688(0.3651) Grad: 82136.5000  LR: 0.00000096  \n","Epoch: [3][420/646] Elapsed 5m 27s (remain 2m 55s) Loss: 0.2782(0.3643) Grad: 112919.5625  LR: 0.00000081  \n","Epoch: [3][440/646] Elapsed 5m 43s (remain 2m 39s) Loss: 0.3094(0.3637) Grad: 146096.7656  LR: 0.00000067  \n","Epoch: [3][460/646] Elapsed 5m 59s (remain 2m 24s) Loss: 0.2498(0.3637) Grad: 99976.0547  LR: 0.00000055  \n","Epoch: [3][480/646] Elapsed 6m 15s (remain 2m 8s) Loss: 0.2238(0.3653) Grad: 55062.1797  LR: 0.00000044  \n","Epoch: [3][500/646] Elapsed 6m 30s (remain 1m 53s) Loss: 0.3819(0.3646) Grad: 46202.4414  LR: 0.00000034  \n","Epoch: [3][520/646] Elapsed 6m 45s (remain 1m 37s) Loss: 0.3516(0.3639) Grad: 50026.1523  LR: 0.00000025  \n","Epoch: [3][540/646] Elapsed 7m 1s (remain 1m 21s) Loss: 0.2385(0.3648) Grad: 80880.9297  LR: 0.00000018  \n","Epoch: [3][560/646] Elapsed 7m 17s (remain 1m 6s) Loss: 0.5395(0.3651) Grad: 98866.9141  LR: 0.00000012  \n","Epoch: [3][580/646] Elapsed 7m 32s (remain 0m 50s) Loss: 0.3322(0.3667) Grad: 51768.3594  LR: 0.00000007  \n","Epoch: [3][600/646] Elapsed 7m 48s (remain 0m 35s) Loss: 0.4328(0.3661) Grad: 102988.0781  LR: 0.00000003  \n","Epoch: [3][620/646] Elapsed 8m 4s (remain 0m 19s) Loss: 0.4109(0.3658) Grad: 87622.1641  LR: 0.00000001  \n","Epoch: [3][640/646] Elapsed 8m 19s (remain 0m 3s) Loss: 0.3161(0.3658) Grad: 115232.0078  LR: 0.00000000  \n","Epoch: [3][645/646] Elapsed 8m 23s (remain 0m 0s) Loss: 0.3169(0.3657) Grad: 63110.2539  LR: 0.00000000  \n","EVAL: [0/125] Elapsed 0m 1s (remain 3m 14s) Loss: 0.5013(0.5013) \n","EVAL: [20/125] Elapsed 0m 26s (remain 2m 11s) Loss: 0.5271(0.4845) \n","EVAL: [40/125] Elapsed 0m 51s (remain 1m 45s) Loss: 0.4785(0.4951) \n","EVAL: [60/125] Elapsed 1m 16s (remain 1m 20s) Loss: 0.5102(0.4852) \n","EVAL: [80/125] Elapsed 1m 41s (remain 0m 55s) Loss: 0.4909(0.4856) \n","EVAL: [100/125] Elapsed 2m 6s (remain 0m 30s) Loss: 0.5314(0.4774) \n","EVAL: [120/125] Elapsed 2m 31s (remain 0m 5s) Loss: 0.4777(0.4817) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3657  avg_val_loss: 0.4812  time: 660s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3657  avg_val_loss: 0.4812  time: 660s\n","Epoch 3 - Score: 0.4893  Scores: [0.41724079290341143, 0.5613396900847316]\n","INFO:__main__:Epoch 3 - Score: 0.4893  Scores: [0.41724079290341143, 0.5613396900847316]\n","Epoch 3 - Save Best Score: 0.4893 Model\n","INFO:__main__:Epoch 3 - Save Best Score: 0.4893 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [124/125] Elapsed 2m 36s (remain 0m 0s) Loss: 0.4818(0.4812) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 3 result ==========\n","INFO:__main__:========== fold: 3 result ==========\n","Score: 0.4893  Scores: [0.41724079290341143, 0.5613396900847316]\n","INFO:__main__:Score: 0.4893  Scores: [0.41724079290341143, 0.5613396900847316]\n","========== CV ==========\n","INFO:__main__:========== CV ==========\n","Score: 0.5058  Scores: [0.43038951429943006, 0.5812706372140779]\n","INFO:__main__:Score: 0.5058  Scores: [0.43038951429943006, 0.5812706372140779]\n"]}],"source":["if __name__ == '__main__':\n","\n","    def get_result(oof_df):\n","        labels = oof_df[CFG.target_cols].values\n","        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n","        score, scores = get_score(labels, preds)\n","        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n","\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for fold in range(CFG.n_fold):\n","            if fold in CFG.trn_fold:\n","                if CFG.save_strategy=='epoch':\n","                  _oof_df = train_loop(train, fold)\n","                elif CFG.save_strategy=='step':\n","                  _oof_df = train_loop_steps(train,fold)\n","                elif CFG.save_strategy=='prediction':\n","                  _oof_df = prediction(train,fold)\n","                oof_df = pd.concat([oof_df, _oof_df])\n","\n","                LOGGER.info(f\"========== fold: {fold} result ==========\")\n","                get_result(_oof_df)\n","        oof_df = oof_df.reset_index(drop=True)\n","        LOGGER.info(f\"========== CV ==========\")\n","        get_result(oof_df)\n","        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n","\n","    if CFG.wandb:\n","        wandb.finish()\n","    runtime.unassign()"],"id":"fde1c8af"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a08b7051"},"outputs":[],"source":[],"id":"a08b7051"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeb952c6"},"outputs":[],"source":[],"id":"eeb952c6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"04d58baa"},"outputs":[],"source":[],"id":"04d58baa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3ca03db"},"outputs":[],"source":[],"id":"d3ca03db"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-25T10:25:22.661613","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a5b39311f434ea486713a0125879fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_877381a3e38d45c087e9b93e5a9468a4","IPY_MODEL_9e29079cd88542b49b34ae8f2885516b","IPY_MODEL_1512c590b6a842f8a12fc579985a7ae1"],"layout":"IPY_MODEL_714c037a51c642d9b6a01382c9278ecf"}},"877381a3e38d45c087e9b93e5a9468a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f9305f92f7a4d84bfd8706c9ad46f14","placeholder":"​","style":"IPY_MODEL_e6354c876344490da54dc6f451c8a174","value":"Downloading (…)okenizer_config.json: 100%"}},"9e29079cd88542b49b34ae8f2885516b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c03f21654b45d7a6134bed41357cb4","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86bf46146cea4fc7b5b45c17229147ec","value":52}},"1512c590b6a842f8a12fc579985a7ae1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8271313f1d284478ab248a2da8683f4d","placeholder":"​","style":"IPY_MODEL_af45fd5e4e24436ebcce6578f4716bd9","value":" 52.0/52.0 [00:00&lt;00:00, 4.13kB/s]"}},"714c037a51c642d9b6a01382c9278ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f9305f92f7a4d84bfd8706c9ad46f14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6354c876344490da54dc6f451c8a174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1c03f21654b45d7a6134bed41357cb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86bf46146cea4fc7b5b45c17229147ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8271313f1d284478ab248a2da8683f4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af45fd5e4e24436ebcce6578f4716bd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71d42c3db6494d888f9e561addaab2bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c043c2d840d4a208cb3c6e10dc421c6","IPY_MODEL_371bdfe7e857493aba6821de6a10c1db","IPY_MODEL_3c7823b52afa4fb6b2bea46e9e1eddad"],"layout":"IPY_MODEL_69619edb3ed14be9ba13337aeba46a48"}},"1c043c2d840d4a208cb3c6e10dc421c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed5176b28e334451bc77782e9c86333e","placeholder":"​","style":"IPY_MODEL_447a22d2bd3b4b9eba6bdb32ebbe34a1","value":"Downloading (…)lve/main/config.json: 100%"}},"371bdfe7e857493aba6821de6a10c1db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_debed52ef0a6479d961f1ef409d5cb8b","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ae394a0ad954746ab230fddbdd524e2","value":580}},"3c7823b52afa4fb6b2bea46e9e1eddad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56a84ce0c903412faca81e2cf3e22b1d","placeholder":"​","style":"IPY_MODEL_5fc6942fe6234ae8971ac2a1b8fa76ad","value":" 580/580 [00:00&lt;00:00, 48.7kB/s]"}},"69619edb3ed14be9ba13337aeba46a48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed5176b28e334451bc77782e9c86333e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447a22d2bd3b4b9eba6bdb32ebbe34a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"debed52ef0a6479d961f1ef409d5cb8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ae394a0ad954746ab230fddbdd524e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56a84ce0c903412faca81e2cf3e22b1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fc6942fe6234ae8971ac2a1b8fa76ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66591df0ba614001acf7621c6f152fd5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e3fbded355743df92881b82de4e9af5","IPY_MODEL_ca441ab9b682474bb4ae7df589e471d6","IPY_MODEL_4ccfef89c58d4f4496d9749d88c698f9"],"layout":"IPY_MODEL_dfae7f292d5a4ab9a27bd91c4e507df8"}},"2e3fbded355743df92881b82de4e9af5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b8b9a3f77854935be8ff1db50e4c581","placeholder":"​","style":"IPY_MODEL_0838dc20191941a888df4ce01400c4b3","value":"Downloading spm.model: 100%"}},"ca441ab9b682474bb4ae7df589e471d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_54e32399903a403284c70dee18603b10","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92e7056f26d34eb0a578f533de8233a2","value":2464616}},"4ccfef89c58d4f4496d9749d88c698f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f94e87f8bd2047b4b55beb82f86588f2","placeholder":"​","style":"IPY_MODEL_a29def8575704381a5bdd92ca704ccce","value":" 2.46M/2.46M [00:00&lt;00:00, 89.2MB/s]"}},"dfae7f292d5a4ab9a27bd91c4e507df8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b8b9a3f77854935be8ff1db50e4c581":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0838dc20191941a888df4ce01400c4b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54e32399903a403284c70dee18603b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92e7056f26d34eb0a578f533de8233a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f94e87f8bd2047b4b55beb82f86588f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a29def8575704381a5bdd92ca704ccce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77c711c1675d4b968c516b6f25e71d53":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65087da746474689b75dab6ab58a967f","IPY_MODEL_f7faaa32f3784e2ca65438aa82a4e421","IPY_MODEL_9bcf9da4d98041f2bc278718a7ced7f8"],"layout":"IPY_MODEL_d7d0b6b337b348c5944f72f1c38ed316"}},"65087da746474689b75dab6ab58a967f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec7c52f1d38b4977b605143407edc4fa","placeholder":"​","style":"IPY_MODEL_8b5ea1d9280b4a85b3221d1083283cb7","value":"100%"}},"f7faaa32f3784e2ca65438aa82a4e421":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e07b8238de69476da49c018164b9b38f","max":7165,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9e119bd0c934f84a986ceddf7e8d8d1","value":7165}},"9bcf9da4d98041f2bc278718a7ced7f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cb944dfbf29460d9d73c0f4b32e0453","placeholder":"​","style":"IPY_MODEL_8f610050b6174716b46278339ba61860","value":" 7165/7165 [00:18&lt;00:00, 390.92it/s]"}},"d7d0b6b337b348c5944f72f1c38ed316":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec7c52f1d38b4977b605143407edc4fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b5ea1d9280b4a85b3221d1083283cb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e07b8238de69476da49c018164b9b38f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9e119bd0c934f84a986ceddf7e8d8d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cb944dfbf29460d9d73c0f4b32e0453":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f610050b6174716b46278339ba61860":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb42bd5eb2ab4cb8a5bdcbf31b632602":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6e1cfe3e37244a7b190c5ed863a55ba","IPY_MODEL_ab936b3f9fbf43cd867d7d3aa7947c9c","IPY_MODEL_818b81d13d0142be902def03f557d8c1"],"layout":"IPY_MODEL_8a242c84367f4ad89285e14f94255724"}},"c6e1cfe3e37244a7b190c5ed863a55ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59ae62b2cccb4b59984540fd8b55152d","placeholder":"​","style":"IPY_MODEL_5a9f27babd1541648ce0f4598e993a5f","value":"Downloading pytorch_model.bin: 100%"}},"ab936b3f9fbf43cd867d7d3aa7947c9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_286797c24d774d6f851e61dcc000ce43","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c82a5aac200447609c97694f289cd24d","value":873673253}},"818b81d13d0142be902def03f557d8c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12bb2eebf65444679af262c234f756ef","placeholder":"​","style":"IPY_MODEL_ee9fb1dfcc8b4cabb9a89885ae59d83d","value":" 874M/874M [00:03&lt;00:00, 246MB/s]"}},"8a242c84367f4ad89285e14f94255724":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59ae62b2cccb4b59984540fd8b55152d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a9f27babd1541648ce0f4598e993a5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"286797c24d774d6f851e61dcc000ce43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82a5aac200447609c97694f289cd24d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12bb2eebf65444679af262c234f756ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee9fb1dfcc8b4cabb9a89885ae59d83d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}