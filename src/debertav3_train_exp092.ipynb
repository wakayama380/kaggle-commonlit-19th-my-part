{"cells":[{"cell_type":"markdown","metadata":{"id":"7aa1693d"},"source":["# About this notebook\n","- This notebook is a modified version of the PyTorch pipeline from Y.Nakama's starter NLP notebook from Feedback Prize 3 competition [here](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train). Don't forget to upvote his work!\n","- Inference notebook is [here](https://www.kaggle.com/mohammad2012191/debertav3-pytorch-baseline-inference-cv-0-467)"],"id":"7aa1693d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1695777988534,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"VAH4GU3udQLE","outputId":"248c26ef-1859-4e94-889f-c47bdc6d7ea4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Sep 27 01:26:27 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    47W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"VAH4GU3udQLE"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21023,"status":"ok","timestamp":1695778009553,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"y7YWu2B1dPO_","outputId":"c78a153f-5658-4c15-d74f-5752aea0915b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"y7YWu2B1dPO_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QniqU23EOe1O"},"outputs":[],"source":["from google.colab import runtime\n","\n"],"id":"QniqU23EOe1O"},{"cell_type":"code","execution_count":null,"metadata":{"id":"16c51a23"},"outputs":[],"source":[],"id":"16c51a23"},{"cell_type":"markdown","metadata":{"id":"d2e3dae0"},"source":["# CFG"],"id":"d2e3dae0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d22dfc09"},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    exp='exp092'\n","    is_exp=False\n","    wandb=False\n","    competition='FB3'\n","    _wandb_kernel='nakama'\n","    debug=False\n","    apex=True\n","    print_freq=20\n","    num_workers=4\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=True\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps_rate=0.1\n","    epochs=3\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.98)\n","    batch_size=8\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0, 1, 2, 3]\n","    train=True\n","    awp=False\n","    nth_awp_start_epoch= 3\n","    adv_lr = 1e-4\n","    adv_eps = 1e-2\n","    eval_steps =70\n","    save_strategy='epoch'\n","    pooling='ConcatPooling'\n","    n_layers=10\n","    freeze=True\n","    freeze_top_num_layer=14\n","    lr_weight_decay=0.95\n","    reinit=False\n","\n","\n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]"],"id":"d22dfc09"},{"cell_type":"markdown","metadata":{"id":"NhFDBpBR3vGg"},"source":["# Directory settings"],"id":"NhFDBpBR3vGg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJYVrhufgVPh"},"outputs":[],"source":["# ====================================================\n","# Directory settings\n","# ====================================================\n","import os\n","\n","OUTPUT_DIR = f'/content/drive/MyDrive/Kaggle/outputs/{CFG.exp}/'\n","\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n"],"id":"NJYVrhufgVPh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eedc45a5"},"outputs":[],"source":["# ====================================================\n","# wandb\n","# ====================================================\n","if CFG.wandb:\n","\n","    import wandb\n","\n","    try:\n","        from kaggle_secrets import UserSecretsClient\n","        user_secrets = UserSecretsClient()\n","        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        wandb.login(key=secret_value_0)\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n","\n","\n","    def class2dict(f):\n","        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","    run = wandb.init(project='FB3-Public',\n","                     name=CFG.model,\n","                     config=class2dict(CFG),\n","                     group=CFG.model,\n","                     job_type=\"train\",\n","                     anonymous=anony)"],"id":"eedc45a5"},{"cell_type":"markdown","metadata":{"id":"bec24bf5"},"source":["# Library"],"id":"bec24bf5"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37680,"status":"ok","timestamp":1695778049617,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"f72fdd29","outputId":"492b2cc7-f142-4521-8db8-f62ee7a9be27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.31.0\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.31.0)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.31.0)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.31.0\n","Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.10/dist-packages (0.13.3)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","tokenizers.__version__: 0.13.3\n","transformers.__version__: 4.31.0\n","env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","os.system('pip install iterative-stratification==0.1.7')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","#os.system('pip install -q transformers')\n","!pip install transformers==4.31.0\n","os.system('pip install -q tokenizers')\n","!pip install tokenizers==0.13.3\n","!pip install sentencepiece\n","\n","\n","import tokenizers\n","import transformers\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"f72fdd29"},{"cell_type":"markdown","metadata":{"id":"179b542c"},"source":["# Utils"],"id":"179b542c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9795dd2"},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores\n","\n","\n","def get_logger(filename=OUTPUT_DIR+'train'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything(seed=42)"],"id":"f9795dd2"},{"cell_type":"markdown","metadata":{"id":"fac79d7b"},"source":["# Data Loading"],"id":"fac79d7b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":977},"executionInfo":{"elapsed":2686,"status":"ok","timestamp":1695778052297,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"add36693","outputId":"8074b1a9-6d38-43ad-ce01-1525c29580f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["train.shape: (7165, 8)\n"]},{"output_type":"display_data","data":{"text/plain":["     student_id prompt_id                                               text   content   wording                                    prompt_question               prompt_title                                        prompt_text\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...  0.205683  0.380538  Summarize how the Third Wave developed over su...             The Third Wave  Background \\r\\nThe Third Wave experiment took ...\n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme... -0.548304  0.506755  Summarize the various ways the factory would u...    Excerpt from The Jungle  With one member trimming beef in a cannery, an...\n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...  3.128928  4.231226  In complete sentences, summarize the structure...  Egyptian Social Structure  Egyptian society was structured like a pyramid...\n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we... -0.210614 -0.471415  In complete sentences, summarize the structure...  Egyptian Social Structure  Egyptian society was structured like a pyramid...\n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...  3.272894  3.219757  Summarize how the Third Wave developed over su...             The Third Wave  Background \\r\\nThe Third Wave experiment took ..."],"text/html":["\n","  <div id=\"df-b0241aba-375f-40c8-85ec-1c85edd76c9d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0241aba-375f-40c8-85ec-1c85edd76c9d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b0241aba-375f-40c8-85ec-1c85edd76c9d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b0241aba-375f-40c8-85ec-1c85edd76c9d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4368684f-0184-432f-82be-579f4ce2754c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4368684f-0184-432f-82be-579f4ce2754c')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4368684f-0184-432f-82be-579f4ce2754c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["test.shape: (4, 6)\n"]},{"output_type":"display_data","data":{"text/plain":["     student_id prompt_id            text prompt_question     prompt_title       prompt_text\n","0  000000ffffff    abc123  Example text 1    Summarize...  Example Title 1  Heading\\nText...\n","1  111111eeeeee    def789  Example text 2    Summarize...  Example Title 2  Heading\\nText...\n","2  222222cccccc    abc123  Example text 3    Summarize...  Example Title 1  Heading\\nText...\n","3  333333dddddd    def789  Example text 4    Summarize...  Example Title 2  Heading\\nText..."],"text/html":["\n","  <div id=\"df-004e5af2-f00a-42bd-aa30-f888668b3470\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-004e5af2-f00a-42bd-aa30-f888668b3470')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-004e5af2-f00a-42bd-aa30-f888668b3470 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-004e5af2-f00a-42bd-aa30-f888668b3470');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-032d3cad-7361-4fd8-a603-9686ce07983c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-032d3cad-7361-4fd8-a603-9686ce07983c')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-032d3cad-7361-4fd8-a603-9686ce07983c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["submission.shape: (4, 3)\n"]},{"output_type":"display_data","data":{"text/plain":["     student_id  content  wording\n","0  000000ffffff      0.0      0.0\n","1  111111eeeeee      0.0      0.0\n","2  222222cccccc      0.0      0.0\n","3  333333dddddd      0.0      0.0"],"text/html":["\n","  <div id=\"df-fe5626a9-345c-4146-8dfb-ae1ffd7b4047\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe5626a9-345c-4146-8dfb-ae1ffd7b4047')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fe5626a9-345c-4146-8dfb-ae1ffd7b4047 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fe5626a9-345c-4146-8dfb-ae1ffd7b4047');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-58364fb4-9ed1-4c9d-acfe-a40da56327d6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-58364fb4-9ed1-4c9d-acfe-a40da56327d6')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-58364fb4-9ed1-4c9d-acfe-a40da56327d6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}],"source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","input_path = '/content/drive/MyDrive/Kaggle/inputs/'\n","train = pd.read_csv(input_path+'summaries_train.csv')\n","test = pd.read_csv(input_path+'summaries_test.csv')\n","submission = pd.read_csv(input_path+'sample_submission.csv')\n","prompt_train = pd.read_csv(input_path+'prompts_train.csv')\n","prompt_test = pd.read_csv(input_path+'prompts_test.csv')\n","train = pd.merge(train,prompt_train,how='left',on='prompt_id')\n","test = pd.merge(test,prompt_test,how='left',on='prompt_id')\n","print(f\"train.shape: {train.shape}\")\n","display(train.head())\n","print(f\"test.shape: {test.shape}\")\n","display(test.head())\n","print(f\"submission.shape: {submission.shape}\")\n","display(submission.head())"],"id":"add36693"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KzAhY4R3zRK"},"outputs":[],"source":["# oof_df=pd.read_pickle(input_path+'oof_df.pkl')"],"id":"2KzAhY4R3zRK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a37a56e4"},"outputs":[],"source":["train['text'] = ' question: '+train['prompt_question'] + ' [SEP] summary: '+ train['text']+' [SEP] source: '+train['prompt_text'].str[:6000]\n","test['text'] =  ' question: '+test['prompt_question'] + ' [SEP] summary:  '+ test['text']+' [SEP] source: '+test['prompt_text'].str[:6000]\n","\n","#################################################\n","# prompt_textも\n","#################################################\n","\n","# # \"text\"列の長さを計算して新しい列\"length\"に追加\n","# train['length'] = train['text'].apply(len)\n","# # \"text\"列の先頭に\"length\"列の値を結合\n","# train['text'] = train['length'].astype(str) + '[SEP]' + train['prompt_question'] + '[SEP]' +train['prompt_title'] + 'summary(' + train['text'] +') [SEP] source of summary('+train['prompt_text']+')'\n","\n","# # \"text\"列の長さを計算して新しい列\"length\"に追加\n","# test['length'] = test['text'].apply(len)\n","# # \"text\"列の先頭に\"length\"列の値を結合\n","# test['text'] = test['length'].astype(str) + '[SEP]' + test['prompt_question'] + '[SEP]' +test['prompt_title'] + 'summary(' + test['text'] +') [SEP] source of summary('+test['prompt_text']+')'\n"],"id":"a37a56e4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTjtOHdZERdZ"},"outputs":[],"source":[],"id":"OTjtOHdZERdZ"},{"cell_type":"markdown","metadata":{"id":"bfcf2f21"},"source":["# CV split"],"id":"bfcf2f21"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1695778052298,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"fe773f94","outputId":"40f9b78b-ddbe-4d11-d4aa-d9615ad8d4df"},"outputs":[{"output_type":"display_data","data":{"text/plain":["fold\n","0    1103\n","1    2057\n","2    2009\n","3    1996\n","dtype: int64"]},"metadata":{}}],"source":["# ====================================================\n","# CV split\n","# ====================================================\n","# Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","# for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n","#     train.loc[val_index, 'fold'] = int(n)\n","id2fold = {\n","    \"814d6b\": 0,\n","    \"39c16e\": 1,\n","    \"3b9047\": 2,\n","    \"ebad26\": 3,\n","}\n","\n","train[\"fold\"] = train[\"prompt_id\"].map(id2fold)\n","train['fold'] = train['fold'].astype(int)\n","display(train.groupby('fold').size())"],"id":"fe773f94"},{"cell_type":"code","execution_count":null,"metadata":{"id":"26ece1cd"},"outputs":[],"source":["if CFG.debug:\n","    display(train.groupby('fold').size())\n","    train = train.sample(n=3000, random_state=0).reset_index(drop=True)\n","    display(train.groupby('fold').size())"],"id":"26ece1cd"},{"cell_type":"markdown","metadata":{"id":"478fb80d"},"source":["# tokenizer"],"id":"478fb80d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1695778052298,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"GLyHNCkSh0CG","outputId":"beadf192-9603-427b-a9dc-61fdfbcc014f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'microsoft/deberta-v3-large'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["CFG.model"],"id":"GLyHNCkSh0CG"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253,"referenced_widgets":["0b9f914607a5409a8b8ae48880dd9f9d","e1c372e3c17a402d8ee5ceae66bb95ec","b4a03d2780c449bfb8c64e2db55eae0e","f00e69c7ea394a2a984ced089545ea94","7155fcc406e44cb2a06383bcd1ab50b5","a6680936f240477282968cc441571547","7a502a90c09345d890ce243e99e30006","78cc60897b794f3a9e60e123b89440b8","6b2c2d8dd943447fa4ee7a61ac98cc99","3841ea0112124872803c7cc236c0d024","e2283dd48cdd43fb9d2c7a5362b53e37","38e28e1a606940f4831b41050d52e255","58017f3d88ba4f448d5edfeb41162c43","7481fa147ff54b0e9d8ee7ec1a59d1f7","30bf6036daa945618ce63a32b3f79a44","d0e47712d58b4f7fb24c1a3c1f4cd649","55e49252285e477abf0de43614e09710","810a62c74337458da500f5928b6d86a6","2aa0ce88234e453a9b05aa94b6f0bad7","de890bff24fd4f4e959f253bd113b2bd","a385cfe834b043c9accbd553dabdb2ad","7e6e8543da234107bbf61359785ea473","9897759eb58d4eb5b77bf5b8ef4229e6","eef395af45dc413f88a0c2a9e6476a6b","ff971c96c1b24623bf90854040decdb7","8b9e2b2f89fd48868fd36750837e6b4e","243c7fcddf034e7fa4039a7b351b8790","ec7968b3417947358b42cfa0ee6a3f77","67a5583841e7407aa2558c231c7b5008","00645056793a4a1d9774032b69b0d039","a71950c9cdd7402cb25142d0491f7a0a","da67eec786934ae3addaba8fc038bdea","e336808f0d2340e9b21d5974ab553f34"]},"executionInfo":{"elapsed":3678,"status":"ok","timestamp":1695778055962,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"1a9ff4aa","outputId":"5d7c732a-1f22-4fba-c246-5dd5ce39432e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9f914607a5409a8b8ae48880dd9f9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e28e1a606940f4831b41050d52e255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9897759eb58d4eb5b77bf5b8ef4229e6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# ====================================================\n","# tokenizer\n","# ====================================================\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","CFG.tokenizer = tokenizer"],"id":"1a9ff4aa"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1695778055963,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"cQZWV8VaE0xO","outputId":"ab3458c7-ac31-492f-9492-b527afc6bff5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["' question: In complete sentences, summarize the structure of the ancient Egyptian system of government. How were different social classes involved in this government? Cite evidence from the text. [SEP] summary: In Egypt, there were many occupations and social classes involved in day-to-day living. In many instances if you were at the bottom of the social ladder you could climb up, you didn\\'t have to stay a peasant you could work to bring your status up. Everyone worshipped the gods Ra, Osiris, and Isis, but also they would worship their pharaohs like gods as well. Under the pharaohs were the priests, they had the responsibility to entertain or please the said god. The Chain of Command was placed to keep everyone in check, not one person could handle all the civilians and treasures without any aid. Like the tax collector, called a vizier like stated they were in charge of collecting the peoples\\' tax. They were also one of the rare instances who were able to read and write, that\\'s how they were granted \"vizier\" Also the soldiers did many things as they would fight in wars or \"quelled domestic uprisings\". They were in charge of getting the slaves, farmers, and peasants to build palaces or the famous ancient pyramids. More skilled hardworking workers had occupations of craftsmen or women and physicians. This would mostly make up the middle-class people. The creative craftsmen would often make jewelry, papyrus products, pottery, tools, and many useful things people may need . Of course, you would need merchants to sell the goods to people who would pay for it. [SEP] source: Egyptian society was structured like a pyramid. At the top were the gods, such as Ra, Osiris, and Isis. Egyptians believed that the gods controlled the universe. Therefore, it was important to keep them happy. They could make the Nile overflow, cause famine, or even bring death. \\r\\nThe Egyptians also elevated some human beings to gods. Their leaders, called pharaohs, were believed to be gods in human form. They had absolute power over their subjects. After pharaohs died, huge stone pyramids were built as their tombs. Pharaohs were buried in chambers within the pyramids. \\r\\nBecause the people of Egypt believed that their pharaohs were gods, they entrusted their rulers with many responsibilities. Protection was at the top of the list. The pharaoh directed the army in case of a foreign threat or an internal conflict. All laws were enacted at the discretion of the pharaoh. Each farmer paid taxes in the form of grains, which were stored in the pharaoh’s warehouses. This grain was used to feed the people in the event of a famine. \\r\\nThe Chain of Command \\r\\nNo single person could manage all these duties without assistance. The pharaoh appointed a chief minister called a vizier as a supervisor. The vizier ensured that taxes were collected. \\r\\nWorking with the vizier were scribes who kept government records. These high-level employees had mastered a rare skill in ancient Egypt — they could read and write. \\r\\nNoble Aims \\r\\nRight below the pharaoh in status were powerful nobles and priests. Only nobles could hold government posts; in these positions they profited from tributes paid to the pharaoh. Priests were responsible for pleasing the gods. \\r\\nNobles enjoyed great status and also grew wealthy from donations to the gods. All Egyptians—from pharaohs to farmers—gave gifts to the gods. \\r\\nSoldier On \\r\\nSoldiers fought in wars or quelled domestic uprisings. During long periods of peace, soldiers also supervised the peasants, farmers, and slaves who were involved in building such structures as pyramids and palaces. \\r\\nSkilled workers such as physicians and craftsmen/women made up the middle class. Craftsmen made and sold jewelry, pottery, papyrus products, tools, and other useful things. \\r\\nNaturally, there were people needed to buy goods from artisans and traders. These were the merchants and storekeepers who sold these goods to the public. \\r\\nThe Bottom of the Heap \\r\\nAt the bottom of the social structure were slaves and farmers. Slavery became the fate of those captured as prisoners of war. In addition to being forced to work on building projects, slaves toiled at the discretion of the pharaoh or nobles. \\r\\nFarmers tended the fields, raised animals, kept canals and reservoirs in good order, worked in the stone quarries, and built the royal monuments. Farmers paid taxes that could amount to as much as 60% of their yearly harvest—that’s a lot of hay! \\r\\nSocial mobility was not impossible. A small number of peasants and farmers moved up the economic ladder. Families saved money to send their sons to village schools to learn trades. These schools were run by priests or by artisans. Boys who learned to read and write could become scribes, then go on to gain employment in the government. It was possible for a boy born on a farm to work his way up into the higher ranks of the government. Bureaucracy proved lucrative.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["train['text'].iloc[2]"],"id":"cQZWV8VaE0xO"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1695778055963,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"36zvorT0EVVA","outputId":"4875b9f3-7502-4045-cf3f-5c5e316e8077"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: unnko\n","Encoded: {'input_ids': tensor([[   1, 1655,  673, 4712,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n","Decoded text: [CLS] ▁un n ko [SEP]\n"]}],"source":["# テキストをエンコード\n","text = 'unnko'\n","encoded = tokenizer(text, return_tensors='pt')\n","\n","# デコードして元のテキストを取得\n","\n","decoded_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n","decoded_text = \" \".join(decoded_tokens)\n","\n","print(f\"Original text: {text}\")\n","print(f\"Encoded: {encoded}\")\n","print(f\"Decoded text: {decoded_text}\")"],"id":"36zvorT0EVVA"},{"cell_type":"markdown","metadata":{"id":"0e4568b4"},"source":["# Dataset"],"id":"0e4568b4"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["75dd584ed3a54301aafa35e2ef9fd46c","b92ce781f16643ac9a42723f6c35b178","66d74e6f6f2b46f5aed8e9d9fd3e6f08","a4a20218c3bc48e8b2d2a41ea11c7d3e","b10fa6b5fc5e4744a35bf17495a12e86","b6c5781184224cedbb553597041a95b3","a91cd45e09444e40b721911db6d3e140","0ff5d792b5b943be9f51bbc973b96c43","20aaaa2762804effa86b830df1e00447","64e284679cde4a7ebe744d9cd20d0fc6","332aeacd7b674c9ba898de1e26aa45cb"]},"executionInfo":{"elapsed":19309,"status":"ok","timestamp":1695778075247,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"5d1060ed","outputId":"bb0da6df-10d5-4bd7-d6d5-6c83eab65809"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/7165 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75dd584ed3a54301aafa35e2ef9fd46c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["max_len: 1024\n","INFO:__main__:max_len: 1024\n"]}],"source":["# ====================================================\n","# Define max_len\n","# ====================================================\n","lengths = []\n","tk0 = tqdm(train['text'].fillna(\"\").values, total=len(train))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","    lengths.append(length)\n","CFG.max_len = max(lengths) + 2 # cls & sep\n","CFG.max_len=1024\n","LOGGER.info(f\"max_len: {CFG.max_len}\")"],"id":"5d1060ed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bd231fe"},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text):\n","    inputs = cfg.tokenizer.encode_plus(\n","        text,\n","        return_tensors=None,\n","        add_special_tokens=True,\n","        max_length=CFG.max_len,\n","        pad_to_max_length=True,\n","        truncation=True\n","    )\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['text'].values\n","        self.labels = df[cfg.target_cols].values\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item])\n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        return inputs, label\n","\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:,:mask_len]\n","    return inputs"],"id":"2bd231fe"},{"cell_type":"markdown","metadata":{"id":"694f45ea"},"source":["# Model"],"id":"694f45ea"},{"cell_type":"code","execution_count":null,"metadata":{"id":"be0cf2ca"},"outputs":[],"source":["#ref:https://github.com/shu421/kagglib/blob/main/nlp/model.py\n","# ====================================================\n","# Model\n","# ====================================================\n","\n","def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","\n","    for parameter in module.parameters():\n","        parameter.requires_grad = False\n","# =====================================================\n","# Pooling\n","# =====================================================\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","\n","\n","class AttentionPooling(nn.Module):\n","    \"\"\"\n","    Usage:\n","        self.pool = AttentionPooling(self.config.hidden_size)\n","    \"\"\"\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_dim, in_dim),\n","            nn.LayerNorm(in_dim),\n","            nn.GELU(),\n","            nn.Linear(in_dim, 1),\n","        )\n","\n","        self._init_weights(self.attention)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        w = self.attention(last_hidden_state).float()\n","        w[attention_mask == 0] = float(\"-inf\")\n","        w = torch.softmax(w, 1)\n","        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n","        return attention_embeddings\n","\n","\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n","        )\n","\n","    def forward(self, all_hidden_states):\n","        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","        return weighted_average[:, 0]\n","\n","class LSTMPooling(nn.Module):\n","    def __init__(self, num_layers, hidden_size, hiddendim_lstm, dropout_rate, is_lstm=True):\n","        super(LSTMPooling, self).__init__()\n","        self.num_hidden_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.hiddendim_lstm = hiddendim_lstm\n","\n","        if is_lstm:\n","            self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n","        else:\n","            self.lstm = nn.GRU(self.hidden_size, self.hiddendim_lstm, batch_first=True, bidirectional=True)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, all_hidden_states):\n","        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n","                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n","        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n","        out, _ = self.lstm(hidden_states, None)\n","        out = self.dropout(out[:, -1, :])\n","        return out\n","\n","class ConcatPooling(nn.Module):\n","    def __init__(self, n_layers=4):\n","        super(ConcatPooling, self, ).__init__()\n","\n","        self.n_layers = n_layers\n","\n","    def forward(self, all_hidden_states):\n","        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n","        concatenate_pooling = concatenate_pooling[:, 0]\n","        return concatenate_pooling\n","\n","# GeM\n","class GeMText(nn.Module):\n","    def __init__(self, dim=1, cfg=None, p=3, eps=1e-6):\n","        super(GeMText, self).__init__()\n","        self.dim = dim\n","        self.p = Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","        self.feat_mult = 1\n","        # x seeems last hidden state\n","\n","    def forward(self, x, attention_mask):\n","        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n","        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n","        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n","        ret = ret.pow(1 / self.p)\n","        return ret\n","\n","# ===========================================\n","# custom Model\n","# ===========================================\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","            self.config.hidden_dropout = 0.\n","            self.config.hidden_dropout_prob = 0.\n","            self.config.attention_dropout = 0.\n","            self.config.attention_probs_dropout_prob = 0.\n","            LOGGER.info(self.config)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel(self.config)\n","        if self.cfg.gradient_checkpointing:\n","            self.model.gradient_checkpointing_enable()\n","        if cfg.pooling =='LSTMPooling':\n","            self.pool =  LSTMPooling(self.config.num_hidden_layers,\n","                                       self.config.hidden_size,\n","                                       self.cfg.hidden_size,\n","                                       0.1,\n","                                       is_lstm=True\n","                           )\n","            self.fc = nn.Linear(self.cfg.hidden_size, 2)\n","        elif cfg.pooling == \"GeM\":\n","            self.pool = GeMText()\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","        elif cfg.pooling=='ConcatPooling':\n","            self.pool = ConcatPooling(n_layers=cfg.n_layers)\n","            self.fc = nn.Linear(cfg.n_layers*self.config.hidden_size, 2)\n","        elif cfg.pooling=='WeightedLayerPooling':\n","            self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","\n","\n","        self._init_weights(self.fc)\n","\n","\n","        # Freeze\n","        if self.cfg.freeze:\n","            freeze(self.model.encoder.layer[:self.cfg.freeze_top_num_layer])\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        if self.cfg.pooling=='MeanPooling':\n","          feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling in ['GRUPooling', 'LSTMPooling']:\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling=='GeM':\n","          feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling == 'ConcatPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling == 'WeightedLayerPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(feature)\n","        return output\n","\n","\n","# initialize layer\n","def reinit_bert(model):\n","    \"\"\"_summary_\n","\n","    Args:\n","        model (AutoModel): _description_\n","\n","    Returns:\n","        model (AutoModel): _description_\n","\n","    Usage:\n","        model = reinit_bert(model)\n","    \"\"\"\n","    for layer in model.model.encoder.layer[-1:]:\n","        for module in layer.modules():\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","    return model"],"id":"be0cf2ca"},{"cell_type":"markdown","metadata":{"id":"31b35d08"},"source":["# Loss"],"id":"31b35d08"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ac3909be"},"outputs":[],"source":["# ====================================================\n","# Loss\n","# ====================================================\n","class RMSELoss(nn.Module):\n","    def __init__(self, reduction='mean', eps=1e-9):\n","        super().__init__()\n","        self.mse = nn.MSELoss(reduction='none')\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def forward(self, y_pred, y_true):\n","        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","\n","class WeightedSmoothL1Loss(nn.Module):\n","    def __init__(self,weights = torch.tensor([0.5, 1.2], device = device )):\n","        super(WeightedSmoothL1Loss, self).__init__()\n","        self.weights=weights\n","\n","    def forward(self, inputs, targets):\n","        \"\"\"\n","        inputs: ネットワークの出力 (予測値)\n","        targets: 正解ラベル\n","        weights: 各サンプルに対する重み\n","        \"\"\"\n","        # Smooth L1 損失を計算\n","        loss = nn.SmoothL1Loss(reduction='none')(inputs, targets)\n","\n","        # 重みを適用して損失を計算\n","        weighted_loss = torch.mean(loss * self.weights)\n","\n","        return weighted_loss\n","\n","\n","class MCRMSELoss(nn.Module):\n","    def __init__(self):\n","        super(MCRMSELoss, self).__init__()\n","\n","    def forward(self, y_true, y_pred):\n","        colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=0)\n","        return torch.mean(torch.sqrt(colwise_mse), dim=0)"],"id":"ac3909be"},{"cell_type":"markdown","metadata":{"id":"lKO48EXzL8mV"},"source":["# AWP"],"id":"lKO48EXzL8mV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zMdI9u5L-HP"},"outputs":[],"source":["from torch import Tensor\n","from torch.nn import Module\n","from torch.optim import Optimizer\n","from torch.nn.modules.loss import _Loss\n","\n","class AWP:\n","    def __init__(\n","        self,\n","        model: Module,\n","        criterion: _Loss,\n","        optimizer: Optimizer,\n","        apex: bool,\n","        adv_param: str=\"weight\",\n","        adv_lr: float=1.0,\n","        adv_eps: float=0.01\n","    ) -> None:\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.apex = apex\n","        self.backup = {}\n","        self.backup_eps = {}\n","\n","    def attack_backward(self, inputs: dict, label: Tensor) -> Tensor:\n","        with torch.cuda.amp.autocast(enabled=self.apex):\n","            self._save()\n","            self._attack_step() # モデルを近傍の悪い方へ改変\n","            y_preds = self.model(inputs)\n","            adv_loss = self.criterion(\n","                y_preds.view(-1, 1), label.view(-1, 1))\n","            mask = (label.view(-1, 1) != -1)\n","            adv_loss = torch.masked_select(adv_loss, mask).mean()\n","            self.optimizer.zero_grad()\n","        return adv_loss\n","\n","    def _attack_step(self) -> None:\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    # 直前に損失関数に通してパラメータの勾配を取得できるようにしておく必要あり\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(\n","                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","\n","    def _save(self) -> None:\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self) -> None:\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}"],"id":"1zMdI9u5L-HP"},{"cell_type":"markdown","metadata":{"id":"d6344760"},"source":["# Helpler functions"],"id":"d6344760"},{"cell_type":"code","execution_count":null,"metadata":{"id":"978896ec"},"outputs":[],"source":["# ====================================================\n","# Helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","\n","    if CFG.awp and epoch+1 >= CFG.nth_awp_start_epoch:\n","        LOGGER.info(f'AWP training with epoch {epoch+1}')\n","    model.train()\n","    awp = AWP(\n","            model,\n","            criterion,\n","            optimizer,\n","            CFG.apex,\n","            adv_lr=CFG.adv_lr,\n","            adv_eps=CFG.adv_eps\n","        )\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","        if CFG.awp and CFG.nth_awp_start_epoch <= epoch+1:\n","            loss = awp.attack_backward(inputs, labels)\n","            scaler.scale(loss).backward()\n","            awp._restore()\n","\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader),\n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    return losses.avg, predictions\n","\n","\n","\n","\n","\n","# def train_fn_by_step(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, now_step):\n","\n","#     # if CFG.awp and epoch+1 >= CFG.nth_awp_start_epoch:\n","#     #     LOGGER.info(f'AWP training with epoch {epoch+1}')\n","#     model.train()\n","#     # awp = AWP(\n","#     #         model,\n","#     #         criterion,\n","#     #         optimizer,\n","#     #         CFG.apex,\n","#     #         adv_lr=CFG.adv_lr,\n","#     #         adv_eps=CFG.adv_eps\n","#     #     )\n","#     if now_step==0:\n","#       scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","#       losses = AverageMeter()\n","#       start = end = time.time()\n","#       global_step = 0\n","#     for step, (inputs, labels) in enumerate(train_loader):\n","#         if now_step>step:\n","#           continue\n","#         inputs = collate(inputs)\n","#         for k, v in inputs.items():\n","#             inputs[k] = v.to(device)\n","#         labels = labels.to(device)\n","#         batch_size = labels.size(0)\n","#         with torch.cuda.amp.autocast(enabled=CFG.apex):\n","#             y_preds = model(inputs)\n","#             loss = criterion(y_preds, labels)\n","#         if CFG.gradient_accumulation_steps > 1:\n","#             loss = loss / CFG.gradient_accumulation_steps\n","#         losses.update(loss.item(), batch_size)\n","#         scaler.scale(loss).backward()\n","#         grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","\n","#         # if CFG.awp and CFG.nth_awp_start_epoch <= epoch+1:\n","#         #     loss = awp.attack_backward(inputs, labels)\n","#         #     scaler.scale(loss).backward()\n","#         #     awp._restore()\n","\n","#         if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","#             scaler.step(optimizer)\n","#             scaler.update()\n","#             optimizer.zero_grad()\n","#             global_step += 1\n","#             if CFG.batch_scheduler:\n","#                 scheduler.step()\n","#         end = time.time()\n","#         if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","#             print('Epoch: [{0}][{1}/{2}] '\n","#                   'Elapsed {remain:s} '\n","#                   'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","#                   'Grad: {grad_norm:.4f}  '\n","#                   'LR: {lr:.8f}  '\n","#                   .format(epoch+1, step, len(train_loader),\n","#                           remain=timeSince(start, float(step+1)/len(train_loader)),\n","#                           loss=losses,\n","#                           grad_norm=grad_norm,\n","#                           lr=scheduler.get_lr()[0]))\n","\n","#         if CFG.wandb:\n","#             wandb.log({f\"[fold{fold}] loss\": losses.val,\n","#                        f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","#         if step%CFG.eval_steps==0:\n","#           return losses.avg, step+1 ,epoch\n","\n","#     return losses.avg, step+1 ,epoch+1\n","\n","\n"],"id":"978896ec"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUz1bDR9nNsa"},"outputs":[],"source":[],"id":"rUz1bDR9nNsa"},{"cell_type":"markdown","metadata":{"id":"fdb6b5d6"},"source":["# train loop"],"id":"fdb6b5d6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9088253a"},"outputs":[],"source":["# ====================================================\n","# train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","\n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    def get_optimizer_grouped_parameters(cfg, model):\n","        \"\"\"Layerwise Learning Rate Decay\"\"\"\n","        model_type = \"model\"\n","        no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if model_type not in n],\n","                \"lr\": cfg.decoder_lr,\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        num_layers = model.config.num_hidden_layers\n","        layers = [getattr(model, model_type).embeddings] + list(\n","            getattr(model, model_type).encoder.layer\n","        )\n","        layers.reverse()\n","        lr = cfg.encoder_lr\n","        for layer in layers:\n","            optimizer_grouped_parameters += [\n","                {\n","                    \"params\": [\n","                        p\n","                        for n, p in layer.named_parameters()\n","                        if not any(nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\": cfg.weight_decay,\n","                    \"lr\": lr,\n","                },\n","                {\n","                    \"params\": [\n","                        p\n","                        for n, p in layer.named_parameters()\n","                        if any(nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\": 0.0,\n","                    \"lr\": lr,\n","                },\n","            ]\n","\n","            lr *= cfg.lr_weight_decay\n","        return optimizer_grouped_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr,\n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    # optimizer_parameters = get_optimizer_grouped_parameters(CFG,model)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","\n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","\n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = MCRMSELoss()\n","    #nn.SmoothL1Loss(reduction='mean')\n","    # WeightedSmoothL1Loss(reduction='mean') #\n","\n","    best_score = np.inf\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","\n","        # scoring\n","        score, scores = get_score(valid_labels, predictions)\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] epoch\": epoch+1,\n","                       f\"[fold{fold}] avg_train_loss\": avg_loss,\n","                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                       f\"[fold{fold}] score\": score})\n","\n","        if best_score > score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# ====================================================\n","# train loop by steps\n","# ====================================================\n","def train_loop_steps(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    if CFG.reinit:\n","      model=reinit_bert(model)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","\n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr,\n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","\n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=int(cfg.num_warmup_steps_rate*num_train_steps), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","\n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = MCRMSELoss()\n","    #nn.SmoothL1Loss(reduction='mean')\n","    # WeightedSmoothL1Loss(reduction='mean') #\n","\n","    best_score = np.inf\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        model.train()\n","        # if CFG.awp and epoch+1 >= CFG.nth_awp_start_epoch:\n","        #   LOGGER.info(f'AWP training with epoch {epoch+1}')\n","\n","        # awp = AWP(\n","        #     model,\n","        #     criterion,\n","        #     optimizer,\n","        #     CFG.apex,\n","        #     adv_lr=CFG.adv_lr,\n","        #     adv_eps=CFG.adv_eps\n","        #     )\n","        scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","        losses = AverageMeter()\n","        start = end = time.time()\n","        global_step = 0\n","        for step, (inputs, labels) in enumerate(tqdm(train_loader)):\n","\n","            inputs = collate(inputs)\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","            labels = labels.to(device)\n","            batch_size = labels.size(0)\n","            with torch.cuda.amp.autocast(enabled=CFG.apex):\n","                y_preds = model(inputs)\n","                loss = criterion(y_preds, labels)\n","            if CFG.gradient_accumulation_steps > 1:\n","                loss = loss / CFG.gradient_accumulation_steps\n","            losses.update(loss.item(), batch_size)\n","            scaler.scale(loss).backward()\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","            if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                global_step += 1\n","                if CFG.batch_scheduler:\n","                    scheduler.step()\n","            end = time.time()\n","            if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","                print('Epoch: [{0}][{1}/{2}] '\n","                      'Elapsed {remain:s} '\n","                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                      'Grad: {grad_norm:.4f}  '\n","                      'LR: {lr:.8f}  '\n","                      .format(epoch+1, step, len(train_loader),\n","                              remain=timeSince(start, float(step+1)/len(train_loader)),\n","                              loss=losses,\n","                              grad_norm=grad_norm,\n","                              lr=scheduler.get_lr()[0]))\n","            if CFG.wandb:\n","                wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                          f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","\n","            if (step % CFG.eval_steps==0 and step!=0) or step == (len(train_loader)-1):\n","\n","                  # valid\n","                  losses_val = AverageMeter()\n","                  model.eval()\n","                  preds = []\n","\n","                  for val_step, (inputs, labels) in enumerate(valid_loader):\n","\n","                      inputs = collate(inputs)\n","                      for k, v in inputs.items():\n","                          inputs[k] = v.to(device)\n","                      labels = labels.to(device)\n","                      batch_size = labels.size(0)\n","                      with torch.no_grad():\n","                          y_preds = model(inputs)\n","                          loss = criterion(y_preds, labels)\n","                      if CFG.gradient_accumulation_steps > 1:\n","                          loss = loss / CFG.gradient_accumulation_steps\n","                      losses_val.update(loss.item(), batch_size)\n","                      preds.append(y_preds.to('cpu').numpy())\n","\n","                      if val_step % CFG.print_freq == 0 or val_step == (len(valid_loader)-1):\n","                          print('EVAL: [{0}/{1}] '\n","                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                                .format(val_step, len(valid_loader),\n","                                        loss=losses_val))\n","                  predictions = np.concatenate(preds)\n","\n","                  # scoring\n","                  score, scores = get_score(valid_labels, predictions)\n","\n","                  elapsed = time.time() - start_time\n","\n","\n","\n","                  if best_score > score:\n","                      best_score = score\n","                      LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","                      torch.save({'model': model.state_dict(),\n","                                  'predictions': predictions},\n","                                  OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","\n","                  LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {losses.avg:.4f}  avg_val_loss: {losses_val.avg:.4f}')\n","                  LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n","                  # if CFG.wandb:\n","                  #     wandb.log({f\"[fold{fold}] epoch\": epoch+1,\n","                  #                f\"[fold{fold}] avg_train_loss\": avg_loss,\n","                  #                f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                  #                f\"[fold{fold}] score\": score})\n","                  model.train()\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds\n","\n","\n","\n","\n","\n","\n","# ====================================================\n","# train loop by steps\n","# ====================================================\n","def prediction(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds[CFG.target_cols].values\n","\n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    if CFG.reinit:\n","      model=reinit_bert(model)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","    model.eval()\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return valid_folds"],"id":"9088253a"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3f6ed8be644d43dca95a468ae0a2ab42","86836e86bf76421782522575dbe34eba","588b8a323d4d437fa4d1fe80be58e1fa","2f481bb64965463ca82149b1c8b85c5e","6df167a89a9e42119aead8dd1e6bda6b","b526c040313b4a1ea67281e79db5684b","02d21b88637046c087c9ebaf82ad10b3","5ce8d3c271544bbf993160b8a5a1ed7e","b3879f074f2542c58a4e0a6f1d2efb51","b557fa82f1584e63b7d8cc0ba18337d4","1764a6d89d144a4c9cebf56a7d1fd1e3"]},"id":"fde1c8af","outputId":"56709a3e-861d-49a8-9ba7-0605ce407338"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["========== fold: 0 training ==========\n","INFO:__main__:========== fold: 0 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f6ed8be644d43dca95a468ae0a2ab42","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [1][0/757] Elapsed 0m 5s (remain 65m 57s) Loss: 0.9861(0.9861) Grad: inf  LR: 0.00000009  \n","Epoch: [1][20/757] Elapsed 0m 24s (remain 14m 9s) Loss: 0.9393(1.2215) Grad: 99944.4531  LR: 0.00000185  \n","Epoch: [1][40/757] Elapsed 0m 43s (remain 12m 37s) Loss: 0.8005(1.0863) Grad: 85489.2031  LR: 0.00000361  \n","Epoch: [1][60/757] Elapsed 1m 2s (remain 11m 49s) Loss: 0.7167(1.0467) Grad: 388117.3438  LR: 0.00000537  \n","Epoch: [1][80/757] Elapsed 1m 21s (remain 11m 16s) Loss: 1.0258(0.9948) Grad: 359737.8750  LR: 0.00000714  \n","Epoch: [1][100/757] Elapsed 1m 40s (remain 10m 50s) Loss: 0.6835(0.9402) Grad: 571556.2500  LR: 0.00000890  \n","Epoch: [1][120/757] Elapsed 1m 59s (remain 10m 27s) Loss: 0.6780(0.9006) Grad: 236600.3281  LR: 0.00001066  \n","Epoch: [1][140/757] Elapsed 2m 18s (remain 10m 4s) Loss: 0.8513(0.8619) Grad: 249007.3281  LR: 0.00001242  \n","Epoch: [1][160/757] Elapsed 2m 37s (remain 9m 43s) Loss: 0.6394(0.8433) Grad: 150115.9688  LR: 0.00001419  \n","Epoch: [1][180/757] Elapsed 2m 56s (remain 9m 22s) Loss: 0.8577(0.8130) Grad: 226301.5938  LR: 0.00001595  \n","Epoch: [1][200/757] Elapsed 3m 15s (remain 9m 1s) Loss: 0.5947(0.7899) Grad: 174016.2656  LR: 0.00001771  \n","Epoch: [1][220/757] Elapsed 3m 34s (remain 8m 40s) Loss: 0.6373(0.7670) Grad: 166576.5469  LR: 0.00001947  \n","Epoch: [1][240/757] Elapsed 3m 53s (remain 8m 20s) Loss: 0.4143(0.7465) Grad: 58149.5547  LR: 0.00002000  \n","Epoch: [1][260/757] Elapsed 4m 12s (remain 8m 0s) Loss: 0.5426(0.7303) Grad: 104415.8750  LR: 0.00001999  \n","Epoch: [1][280/757] Elapsed 4m 31s (remain 7m 40s) Loss: 0.8274(0.7181) Grad: 169890.1875  LR: 0.00001997  \n","Epoch: [1][300/757] Elapsed 4m 50s (remain 7m 19s) Loss: 0.3077(0.7054) Grad: 61215.4258  LR: 0.00001994  \n","Epoch: [1][320/757] Elapsed 5m 9s (remain 6m 59s) Loss: 0.4176(0.6945) Grad: 140911.7656  LR: 0.00001990  \n","Epoch: [1][340/757] Elapsed 5m 28s (remain 6m 40s) Loss: 0.6050(0.6868) Grad: 77002.5547  LR: 0.00001985  \n","Epoch: [1][360/757] Elapsed 5m 47s (remain 6m 21s) Loss: 0.3986(0.6754) Grad: 91428.1641  LR: 0.00001979  \n","Epoch: [1][380/757] Elapsed 6m 6s (remain 6m 1s) Loss: 0.4764(0.6698) Grad: 122365.2656  LR: 0.00001972  \n","Epoch: [1][400/757] Elapsed 6m 25s (remain 5m 42s) Loss: 0.5391(0.6621) Grad: 87154.6797  LR: 0.00001965  \n","Epoch: [1][420/757] Elapsed 6m 44s (remain 5m 22s) Loss: 0.4989(0.6553) Grad: 103898.1250  LR: 0.00001956  \n","Epoch: [1][440/757] Elapsed 7m 3s (remain 5m 3s) Loss: 0.4619(0.6491) Grad: 190362.8438  LR: 0.00001946  \n","Epoch: [1][460/757] Elapsed 7m 22s (remain 4m 44s) Loss: 0.2798(0.6412) Grad: 103829.4062  LR: 0.00001936  \n","Epoch: [1][480/757] Elapsed 7m 42s (remain 4m 25s) Loss: 0.8612(0.6385) Grad: 146614.8750  LR: 0.00001925  \n","Epoch: [1][500/757] Elapsed 8m 0s (remain 4m 5s) Loss: 0.4765(0.6349) Grad: 104134.1641  LR: 0.00001913  \n","Epoch: [1][520/757] Elapsed 8m 20s (remain 3m 46s) Loss: 0.4668(0.6291) Grad: 50764.3008  LR: 0.00001900  \n","Epoch: [1][540/757] Elapsed 8m 38s (remain 3m 27s) Loss: 0.3591(0.6238) Grad: 44670.4336  LR: 0.00001886  \n","Epoch: [1][560/757] Elapsed 8m 57s (remain 3m 7s) Loss: 0.5122(0.6178) Grad: 159259.9219  LR: 0.00001871  \n","Epoch: [1][580/757] Elapsed 9m 16s (remain 2m 48s) Loss: 0.5803(0.6139) Grad: 51940.2852  LR: 0.00001856  \n","Epoch: [1][600/757] Elapsed 9m 36s (remain 2m 29s) Loss: 0.3983(0.6089) Grad: 36825.1719  LR: 0.00001840  \n","Epoch: [1][620/757] Elapsed 9m 55s (remain 2m 10s) Loss: 0.5460(0.6051) Grad: 67819.2344  LR: 0.00001823  \n","Epoch: [1][640/757] Elapsed 10m 14s (remain 1m 51s) Loss: 0.5213(0.6023) Grad: 57195.4766  LR: 0.00001805  \n","Epoch: [1][660/757] Elapsed 10m 33s (remain 1m 31s) Loss: 0.4798(0.5984) Grad: 83780.4688  LR: 0.00001786  \n","Epoch: [1][680/757] Elapsed 10m 52s (remain 1m 12s) Loss: 0.5595(0.5940) Grad: 68499.9453  LR: 0.00001767  \n","Epoch: [1][700/757] Elapsed 11m 11s (remain 0m 53s) Loss: 0.3081(0.5901) Grad: 124272.5391  LR: 0.00001747  \n","Epoch: [1][720/757] Elapsed 11m 30s (remain 0m 34s) Loss: 0.5007(0.5873) Grad: 120388.8828  LR: 0.00001726  \n","Epoch: [1][740/757] Elapsed 11m 49s (remain 0m 15s) Loss: 0.5412(0.5857) Grad: 173833.4375  LR: 0.00001704  \n","Epoch: [1][756/757] Elapsed 12m 4s (remain 0m 0s) Loss: 0.4707(0.5833) Grad: 108504.8828  LR: 0.00001687  \n","EVAL: [0/69] Elapsed 0m 1s (remain 1m 34s) Loss: 0.6773(0.6773) \n","EVAL: [20/69] Elapsed 0m 22s (remain 0m 50s) Loss: 0.5791(0.6835) \n","EVAL: [40/69] Elapsed 0m 43s (remain 0m 29s) Loss: 0.6498(0.6582) \n","EVAL: [60/69] Elapsed 1m 5s (remain 0m 8s) Loss: 0.5629(0.6519) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5833  avg_val_loss: 0.6624  time: 799s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.5833  avg_val_loss: 0.6624  time: 799s\n","Epoch 1 - Score: 0.6785  Scores: [0.5540214195433778, 0.8028901100415703]\n","INFO:__main__:Epoch 1 - Score: 0.6785  Scores: [0.5540214195433778, 0.8028901100415703]\n","Epoch 1 - Save Best Score: 0.6785 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.6785 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [68/69] Elapsed 1m 13s (remain 0m 0s) Loss: 0.9612(0.6624) \n","Epoch: [2][0/757] Elapsed 0m 1s (remain 14m 51s) Loss: 0.3503(0.3503) Grad: inf  LR: 0.00001686  \n","Epoch: [2][20/757] Elapsed 0m 19s (remain 11m 33s) Loss: 0.2778(0.4786) Grad: 102246.5234  LR: 0.00001663  \n","Epoch: [2][40/757] Elapsed 0m 38s (remain 11m 14s) Loss: 0.5601(0.4544) Grad: 113027.0469  LR: 0.00001640  \n","Epoch: [2][60/757] Elapsed 0m 57s (remain 10m 59s) Loss: 0.4905(0.4400) Grad: 133465.8438  LR: 0.00001616  \n","Epoch: [2][80/757] Elapsed 1m 16s (remain 10m 42s) Loss: 0.3294(0.4493) Grad: 67458.6719  LR: 0.00001591  \n","Epoch: [2][100/757] Elapsed 1m 35s (remain 10m 22s) Loss: 0.5408(0.4472) Grad: 55273.1641  LR: 0.00001566  \n","Epoch: [2][120/757] Elapsed 1m 55s (remain 10m 4s) Loss: 0.2114(0.4563) Grad: 71772.2578  LR: 0.00001541  \n","Epoch: [2][140/757] Elapsed 2m 14s (remain 9m 46s) Loss: 0.3068(0.4578) Grad: 30938.0020  LR: 0.00001515  \n","Epoch: [2][160/757] Elapsed 2m 33s (remain 9m 27s) Loss: 0.3654(0.4561) Grad: 117313.6719  LR: 0.00001488  \n","Epoch: [2][180/757] Elapsed 2m 52s (remain 9m 8s) Loss: 0.3064(0.4547) Grad: 58807.3672  LR: 0.00001461  \n","Epoch: [2][200/757] Elapsed 3m 11s (remain 8m 48s) Loss: 0.4115(0.4530) Grad: 89851.2031  LR: 0.00001433  \n","Epoch: [2][220/757] Elapsed 3m 30s (remain 8m 30s) Loss: 0.3654(0.4518) Grad: 63672.3008  LR: 0.00001406  \n","Epoch: [2][240/757] Elapsed 3m 49s (remain 8m 10s) Loss: 0.4224(0.4482) Grad: 83639.0625  LR: 0.00001377  \n","Epoch: [2][260/757] Elapsed 4m 8s (remain 7m 52s) Loss: 0.5375(0.4487) Grad: 99644.0625  LR: 0.00001349  \n","Epoch: [2][280/757] Elapsed 4m 27s (remain 7m 33s) Loss: 0.4009(0.4496) Grad: 75977.5156  LR: 0.00001320  \n","Epoch: [2][300/757] Elapsed 4m 46s (remain 7m 14s) Loss: 0.3953(0.4470) Grad: 83402.5781  LR: 0.00001291  \n","Epoch: [2][320/757] Elapsed 5m 5s (remain 6m 55s) Loss: 0.4910(0.4457) Grad: 26792.5117  LR: 0.00001261  \n","Epoch: [2][340/757] Elapsed 5m 24s (remain 6m 36s) Loss: 0.4359(0.4470) Grad: 107049.7031  LR: 0.00001231  \n","Epoch: [2][360/757] Elapsed 5m 43s (remain 6m 17s) Loss: 0.4864(0.4464) Grad: 120768.7891  LR: 0.00001201  \n","Epoch: [2][380/757] Elapsed 6m 2s (remain 5m 58s) Loss: 0.4442(0.4450) Grad: 99338.0312  LR: 0.00001171  \n","Epoch: [2][400/757] Elapsed 6m 21s (remain 5m 38s) Loss: 0.4720(0.4460) Grad: 107089.0469  LR: 0.00001141  \n","Epoch: [2][420/757] Elapsed 6m 40s (remain 5m 19s) Loss: 0.4538(0.4449) Grad: 82829.1172  LR: 0.00001110  \n","Epoch: [2][440/757] Elapsed 7m 0s (remain 5m 0s) Loss: 0.4647(0.4434) Grad: 104426.1094  LR: 0.00001080  \n","Epoch: [2][460/757] Elapsed 7m 19s (remain 4m 41s) Loss: 0.2893(0.4444) Grad: 128895.6953  LR: 0.00001049  \n","Epoch: [2][480/757] Elapsed 7m 38s (remain 4m 22s) Loss: 0.3625(0.4442) Grad: 65953.2969  LR: 0.00001018  \n","Epoch: [2][500/757] Elapsed 7m 57s (remain 4m 3s) Loss: 0.4019(0.4427) Grad: 142306.4375  LR: 0.00000988  \n","Epoch: [2][520/757] Elapsed 8m 16s (remain 3m 44s) Loss: 0.4194(0.4429) Grad: 68555.5312  LR: 0.00000957  \n","Epoch: [2][540/757] Elapsed 8m 35s (remain 3m 25s) Loss: 0.4518(0.4427) Grad: 112255.0312  LR: 0.00000926  \n","Epoch: [2][560/757] Elapsed 8m 54s (remain 3m 6s) Loss: 0.4895(0.4423) Grad: 92267.6406  LR: 0.00000896  \n","Epoch: [2][580/757] Elapsed 9m 13s (remain 2m 47s) Loss: 0.3800(0.4424) Grad: 62513.3242  LR: 0.00000865  \n","Epoch: [2][600/757] Elapsed 9m 32s (remain 2m 28s) Loss: 0.4242(0.4422) Grad: 72304.3594  LR: 0.00000835  \n","Epoch: [2][620/757] Elapsed 9m 51s (remain 2m 9s) Loss: 0.3838(0.4423) Grad: 82217.0156  LR: 0.00000805  \n","Epoch: [2][640/757] Elapsed 10m 10s (remain 1m 50s) Loss: 0.5009(0.4416) Grad: 52457.0938  LR: 0.00000775  \n","Epoch: [2][660/757] Elapsed 10m 29s (remain 1m 31s) Loss: 0.3631(0.4406) Grad: 59832.6484  LR: 0.00000745  \n","Epoch: [2][680/757] Elapsed 10m 49s (remain 1m 12s) Loss: 0.4597(0.4401) Grad: 40980.5391  LR: 0.00000715  \n","Epoch: [2][700/757] Elapsed 11m 8s (remain 0m 53s) Loss: 0.3948(0.4391) Grad: 113772.9531  LR: 0.00000686  \n","Epoch: [2][720/757] Elapsed 11m 27s (remain 0m 34s) Loss: 0.5359(0.4391) Grad: 44682.9219  LR: 0.00000657  \n","Epoch: [2][740/757] Elapsed 11m 46s (remain 0m 15s) Loss: 0.5327(0.4388) Grad: 156283.0781  LR: 0.00000628  \n","Epoch: [2][756/757] Elapsed 12m 1s (remain 0m 0s) Loss: 0.3689(0.4387) Grad: 66198.6562  LR: 0.00000606  \n","EVAL: [0/69] Elapsed 0m 1s (remain 1m 34s) Loss: 0.5766(0.5766) \n","EVAL: [20/69] Elapsed 0m 22s (remain 0m 50s) Loss: 0.4502(0.5633) \n","EVAL: [40/69] Elapsed 0m 43s (remain 0m 29s) Loss: 0.5573(0.5454) \n","EVAL: [60/69] Elapsed 1m 5s (remain 0m 8s) Loss: 0.4852(0.5437) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.4387  avg_val_loss: 0.5519  time: 796s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4387  avg_val_loss: 0.5519  time: 796s\n","Epoch 2 - Score: 0.5650  Scores: [0.5103575770084716, 0.6196013618203523]\n","INFO:__main__:Epoch 2 - Score: 0.5650  Scores: [0.5103575770084716, 0.6196013618203523]\n","Epoch 2 - Save Best Score: 0.5650 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.5650 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [68/69] Elapsed 1m 13s (remain 0m 0s) Loss: 0.7755(0.5519) \n","Epoch: [3][0/757] Elapsed 0m 1s (remain 14m 59s) Loss: 0.5771(0.5771) Grad: inf  LR: 0.00000604  \n","Epoch: [3][20/757] Elapsed 0m 20s (remain 11m 44s) Loss: 0.2492(0.4114) Grad: 124716.1172  LR: 0.00000576  \n","Epoch: [3][40/757] Elapsed 0m 39s (remain 11m 25s) Loss: 0.3673(0.4124) Grad: 74738.1562  LR: 0.00000549  \n","Epoch: [3][60/757] Elapsed 0m 58s (remain 11m 4s) Loss: 0.3310(0.4011) Grad: 27653.2090  LR: 0.00000521  \n","Epoch: [3][80/757] Elapsed 1m 17s (remain 10m 45s) Loss: 0.3234(0.4011) Grad: 83773.8672  LR: 0.00000495  \n","Epoch: [3][100/757] Elapsed 1m 36s (remain 10m 26s) Loss: 0.4681(0.4001) Grad: 113445.2188  LR: 0.00000468  \n","Epoch: [3][120/757] Elapsed 1m 55s (remain 10m 7s) Loss: 0.4618(0.4016) Grad: 101065.9922  LR: 0.00000443  \n","Epoch: [3][140/757] Elapsed 2m 14s (remain 9m 47s) Loss: 0.2858(0.3962) Grad: 87506.5156  LR: 0.00000417  \n","Epoch: [3][160/757] Elapsed 2m 33s (remain 9m 26s) Loss: 0.4805(0.3932) Grad: 58111.7109  LR: 0.00000393  \n","Epoch: [3][180/757] Elapsed 2m 52s (remain 9m 7s) Loss: 0.4699(0.3880) Grad: 47545.8750  LR: 0.00000369  \n","Epoch: [3][200/757] Elapsed 3m 11s (remain 8m 49s) Loss: 0.3031(0.3889) Grad: 105252.8594  LR: 0.00000345  \n","Epoch: [3][220/757] Elapsed 3m 30s (remain 8m 30s) Loss: 0.3401(0.3877) Grad: 119579.5859  LR: 0.00000322  \n","Epoch: [3][240/757] Elapsed 3m 49s (remain 8m 11s) Loss: 0.2802(0.3873) Grad: 38293.7734  LR: 0.00000300  \n","Epoch: [3][260/757] Elapsed 4m 8s (remain 7m 52s) Loss: 0.2558(0.3841) Grad: 28024.6172  LR: 0.00000278  \n","Epoch: [3][280/757] Elapsed 4m 27s (remain 7m 33s) Loss: 0.2914(0.3810) Grad: 102793.1797  LR: 0.00000257  \n","Epoch: [3][300/757] Elapsed 4m 46s (remain 7m 14s) Loss: 0.6069(0.3803) Grad: 113878.0781  LR: 0.00000237  \n","Epoch: [3][320/757] Elapsed 5m 5s (remain 6m 55s) Loss: 0.4011(0.3812) Grad: 30903.4531  LR: 0.00000218  \n","Epoch: [3][340/757] Elapsed 5m 25s (remain 6m 36s) Loss: 0.3566(0.3806) Grad: 117489.3516  LR: 0.00000199  \n","Epoch: [3][360/757] Elapsed 5m 44s (remain 6m 17s) Loss: 0.5734(0.3791) Grad: 103383.8672  LR: 0.00000181  \n","Epoch: [3][380/757] Elapsed 6m 2s (remain 5m 58s) Loss: 0.4267(0.3768) Grad: 55621.9414  LR: 0.00000164  \n","Epoch: [3][400/757] Elapsed 6m 21s (remain 5m 38s) Loss: 0.2943(0.3760) Grad: 93759.1953  LR: 0.00000147  \n","Epoch: [3][420/757] Elapsed 6m 40s (remain 5m 19s) Loss: 0.4608(0.3757) Grad: 70033.0625  LR: 0.00000132  \n","Epoch: [3][440/757] Elapsed 6m 59s (remain 5m 0s) Loss: 0.3831(0.3754) Grad: 88732.4766  LR: 0.00000117  \n","Epoch: [3][460/757] Elapsed 7m 18s (remain 4m 41s) Loss: 0.3284(0.3732) Grad: 53412.8320  LR: 0.00000103  \n","Epoch: [3][480/757] Elapsed 7m 37s (remain 4m 22s) Loss: 0.3974(0.3734) Grad: 103310.3359  LR: 0.00000090  \n","Epoch: [3][500/757] Elapsed 7m 56s (remain 4m 3s) Loss: 0.3580(0.3741) Grad: 40444.9648  LR: 0.00000077  \n","Epoch: [3][520/757] Elapsed 8m 16s (remain 3m 44s) Loss: 0.3096(0.3730) Grad: 87409.2109  LR: 0.00000066  \n","Epoch: [3][540/757] Elapsed 8m 35s (remain 3m 25s) Loss: 0.3953(0.3717) Grad: 87074.6406  LR: 0.00000056  \n","Epoch: [3][560/757] Elapsed 8m 54s (remain 3m 6s) Loss: 0.3834(0.3703) Grad: 72332.1328  LR: 0.00000046  \n","Epoch: [3][580/757] Elapsed 9m 13s (remain 2m 47s) Loss: 0.3206(0.3702) Grad: 50882.0000  LR: 0.00000037  \n","Epoch: [3][600/757] Elapsed 9m 32s (remain 2m 28s) Loss: 0.3691(0.3713) Grad: 77290.1797  LR: 0.00000029  \n","Epoch: [3][620/757] Elapsed 9m 51s (remain 2m 9s) Loss: 0.3276(0.3713) Grad: 63341.9844  LR: 0.00000022  \n","Epoch: [3][640/757] Elapsed 10m 10s (remain 1m 50s) Loss: 0.3462(0.3709) Grad: 59647.3750  LR: 0.00000016  \n","Epoch: [3][660/757] Elapsed 10m 29s (remain 1m 31s) Loss: 0.3380(0.3693) Grad: 56906.4570  LR: 0.00000011  \n","Epoch: [3][680/757] Elapsed 10m 48s (remain 1m 12s) Loss: 0.3580(0.3690) Grad: 94042.7656  LR: 0.00000007  \n","Epoch: [3][700/757] Elapsed 11m 6s (remain 0m 53s) Loss: 0.4401(0.3693) Grad: 68231.5938  LR: 0.00000004  \n","Epoch: [3][720/757] Elapsed 11m 26s (remain 0m 34s) Loss: 0.3660(0.3687) Grad: 87427.0000  LR: 0.00000002  \n","Epoch: [3][740/757] Elapsed 11m 45s (remain 0m 15s) Loss: 0.3354(0.3679) Grad: 59360.9180  LR: 0.00000000  \n","Epoch: [3][756/757] Elapsed 12m 0s (remain 0m 0s) Loss: 0.4622(0.3675) Grad: 74905.4297  LR: 0.00000000  \n","EVAL: [0/69] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6451(0.6451) \n","EVAL: [20/69] Elapsed 0m 22s (remain 0m 50s) Loss: 0.5185(0.6304) \n","EVAL: [40/69] Elapsed 0m 43s (remain 0m 29s) Loss: 0.6343(0.6100) \n","EVAL: [60/69] Elapsed 1m 5s (remain 0m 8s) Loss: 0.5118(0.6063) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3675  avg_val_loss: 0.6139  time: 794s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3675  avg_val_loss: 0.6139  time: 794s\n","Epoch 3 - Score: 0.6286  Scores: [0.5723919164536118, 0.6848512246914695]\n","INFO:__main__:Epoch 3 - Score: 0.6286  Scores: [0.5723919164536118, 0.6848512246914695]\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [68/69] Elapsed 1m 13s (remain 0m 0s) Loss: 0.8597(0.6139) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 0 result ==========\n","INFO:__main__:========== fold: 0 result ==========\n","Score: 0.5650  Scores: [0.5103575770084716, 0.6196013618203523]\n","INFO:__main__:Score: 0.5650  Scores: [0.5103575770084716, 0.6196013618203523]\n","========== fold: 1 training ==========\n","INFO:__main__:========== fold: 1 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/638] Elapsed 0m 1s (remain 13m 7s) Loss: 1.7260(1.7260) Grad: inf  LR: 0.00000010  \n","Epoch: [1][20/638] Elapsed 0m 20s (remain 9m 56s) Loss: 1.2687(1.6267) Grad: 1251775.7500  LR: 0.00000220  \n","Epoch: [1][40/638] Elapsed 0m 39s (remain 9m 34s) Loss: 1.5753(1.3919) Grad: 919441.8125  LR: 0.00000429  \n","Epoch: [1][60/638] Elapsed 0m 58s (remain 9m 14s) Loss: 1.1463(1.2483) Grad: 666866.8750  LR: 0.00000639  \n","Epoch: [1][80/638] Elapsed 1m 17s (remain 8m 54s) Loss: 0.6129(1.1664) Grad: 2771121.5000  LR: 0.00000848  \n","Epoch: [1][100/638] Elapsed 1m 36s (remain 8m 34s) Loss: 0.6529(1.0927) Grad: 2677764.5000  LR: 0.00001058  \n","Epoch: [1][120/638] Elapsed 1m 55s (remain 8m 15s) Loss: 0.7784(1.0472) Grad: 782994.6250  LR: 0.00001267  \n","Epoch: [1][140/638] Elapsed 2m 15s (remain 7m 56s) Loss: 0.7543(1.0145) Grad: 525673.6875  LR: 0.00001476  \n","Epoch: [1][160/638] Elapsed 2m 34s (remain 7m 36s) Loss: 0.7329(0.9814) Grad: 1131583.6250  LR: 0.00001686  \n","Epoch: [1][180/638] Elapsed 2m 53s (remain 7m 17s) Loss: 1.0492(0.9618) Grad: 640159.6250  LR: 0.00001895  \n","Epoch: [1][200/638] Elapsed 3m 12s (remain 6m 57s) Loss: 0.6293(0.9493) Grad: 721169.0625  LR: 0.00002000  \n","Epoch: [1][220/638] Elapsed 3m 31s (remain 6m 38s) Loss: 0.5614(0.9386) Grad: 398242.5938  LR: 0.00001999  \n","Epoch: [1][240/638] Elapsed 3m 50s (remain 6m 19s) Loss: 0.6818(0.9176) Grad: 421571.6562  LR: 0.00001996  \n","Epoch: [1][260/638] Elapsed 4m 9s (remain 6m 0s) Loss: 0.7719(0.9015) Grad: 763100.0000  LR: 0.00001992  \n","Epoch: [1][280/638] Elapsed 4m 28s (remain 5m 41s) Loss: 0.6204(0.8821) Grad: 156468.1562  LR: 0.00001987  \n","Epoch: [1][300/638] Elapsed 4m 47s (remain 5m 22s) Loss: 0.6056(0.8658) Grad: 237100.5625  LR: 0.00001980  \n","Epoch: [1][320/638] Elapsed 5m 6s (remain 5m 3s) Loss: 0.5850(0.8582) Grad: 355056.1562  LR: 0.00001972  \n","Epoch: [1][340/638] Elapsed 5m 25s (remain 4m 43s) Loss: 0.6091(0.8446) Grad: 210893.7031  LR: 0.00001963  \n","Epoch: [1][360/638] Elapsed 5m 44s (remain 4m 24s) Loss: 0.8338(0.8312) Grad: 249136.8125  LR: 0.00001952  \n","Epoch: [1][380/638] Elapsed 6m 3s (remain 4m 5s) Loss: 0.5737(0.8175) Grad: 237251.5625  LR: 0.00001941  \n","Epoch: [1][400/638] Elapsed 6m 23s (remain 3m 46s) Loss: 0.5493(0.8152) Grad: 228436.2656  LR: 0.00001928  \n","Epoch: [1][420/638] Elapsed 6m 42s (remain 3m 27s) Loss: 0.3842(0.8070) Grad: 83308.2031  LR: 0.00001913  \n","Epoch: [1][440/638] Elapsed 7m 1s (remain 3m 8s) Loss: 0.8000(0.8029) Grad: 219596.4844  LR: 0.00001898  \n","Epoch: [1][460/638] Elapsed 7m 20s (remain 2m 49s) Loss: 0.3189(0.7908) Grad: 166850.2031  LR: 0.00001881  \n","Epoch: [1][480/638] Elapsed 7m 39s (remain 2m 30s) Loss: 0.5809(0.7791) Grad: 156555.1875  LR: 0.00001864  \n","Epoch: [1][500/638] Elapsed 7m 58s (remain 2m 10s) Loss: 0.5346(0.7683) Grad: 143001.8281  LR: 0.00001845  \n","Epoch: [1][520/638] Elapsed 8m 17s (remain 1m 51s) Loss: 0.5833(0.7576) Grad: 87383.4844  LR: 0.00001825  \n","Epoch: [1][540/638] Elapsed 8m 37s (remain 1m 32s) Loss: 0.3924(0.7499) Grad: 145361.0000  LR: 0.00001803  \n","Epoch: [1][560/638] Elapsed 8m 56s (remain 1m 13s) Loss: 0.4342(0.7408) Grad: 182473.1875  LR: 0.00001781  \n","Epoch: [1][580/638] Elapsed 9m 15s (remain 0m 54s) Loss: 0.7433(0.7353) Grad: 140087.0312  LR: 0.00001758  \n","Epoch: [1][600/638] Elapsed 9m 34s (remain 0m 35s) Loss: 0.5955(0.7283) Grad: 61801.0000  LR: 0.00001734  \n","Epoch: [1][620/638] Elapsed 9m 53s (remain 0m 16s) Loss: 0.5571(0.7235) Grad: 181301.6406  LR: 0.00001708  \n","Epoch: [1][637/638] Elapsed 10m 9s (remain 0m 0s) Loss: 0.4074(0.7173) Grad: 110124.7109  LR: 0.00001686  \n","EVAL: [0/129] Elapsed 0m 1s (remain 2m 46s) Loss: 0.5904(0.5904) \n","EVAL: [20/129] Elapsed 0m 24s (remain 2m 3s) Loss: 0.5885(0.5940) \n","EVAL: [40/129] Elapsed 0m 46s (remain 1m 38s) Loss: 0.6290(0.5790) \n","EVAL: [60/129] Elapsed 1m 8s (remain 1m 16s) Loss: 0.7030(0.5861) \n","EVAL: [80/129] Elapsed 1m 30s (remain 0m 53s) Loss: 0.5110(0.5799) \n","EVAL: [100/129] Elapsed 1m 52s (remain 0m 31s) Loss: 0.7053(0.5822) \n","EVAL: [120/129] Elapsed 2m 14s (remain 0m 8s) Loss: 0.5398(0.5816) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.7173  avg_val_loss: 0.5817  time: 753s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.7173  avg_val_loss: 0.5817  time: 753s\n","Epoch 1 - Score: 0.5913  Scores: [0.6102958961758471, 0.5723110165897597]\n","INFO:__main__:Epoch 1 - Score: 0.5913  Scores: [0.6102958961758471, 0.5723110165897597]\n","Epoch 1 - Save Best Score: 0.5913 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.5913 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [128/129] Elapsed 2m 23s (remain 0m 0s) Loss: 0.5025(0.5817) \n","Epoch: [2][0/638] Elapsed 0m 1s (remain 12m 50s) Loss: 0.4959(0.4959) Grad: inf  LR: 0.00001685  \n","Epoch: [2][20/638] Elapsed 0m 20s (remain 9m 52s) Loss: 0.5726(0.4936) Grad: 74735.0000  LR: 0.00001658  \n","Epoch: [2][40/638] Elapsed 0m 39s (remain 9m 32s) Loss: 0.5596(0.4977) Grad: 57000.2852  LR: 0.00001630  \n","Epoch: [2][60/638] Elapsed 0m 58s (remain 9m 12s) Loss: 0.5799(0.4946) Grad: 123864.4453  LR: 0.00001601  \n","Epoch: [2][80/638] Elapsed 1m 17s (remain 8m 53s) Loss: 0.4717(0.4849) Grad: 149995.6562  LR: 0.00001572  \n","Epoch: [2][100/638] Elapsed 1m 36s (remain 8m 34s) Loss: 0.4505(0.5038) Grad: 52000.4375  LR: 0.00001541  \n","Epoch: [2][120/638] Elapsed 1m 55s (remain 8m 14s) Loss: 0.3638(0.4951) Grad: 101025.1797  LR: 0.00001510  \n","Epoch: [2][140/638] Elapsed 2m 14s (remain 7m 55s) Loss: 0.5099(0.4978) Grad: 55850.9414  LR: 0.00001479  \n","Epoch: [2][160/638] Elapsed 2m 33s (remain 7m 35s) Loss: 0.3869(0.5009) Grad: 122367.3281  LR: 0.00001447  \n","Epoch: [2][180/638] Elapsed 2m 52s (remain 7m 16s) Loss: 0.4738(0.5007) Grad: 124099.1172  LR: 0.00001414  \n","Epoch: [2][200/638] Elapsed 3m 11s (remain 6m 57s) Loss: 0.4025(0.5032) Grad: 126282.3828  LR: 0.00001380  \n","Epoch: [2][220/638] Elapsed 3m 31s (remain 6m 38s) Loss: 0.3747(0.4965) Grad: 116371.7891  LR: 0.00001346  \n","Epoch: [2][240/638] Elapsed 3m 50s (remain 6m 19s) Loss: 0.4309(0.4965) Grad: 106470.7578  LR: 0.00001312  \n","Epoch: [2][260/638] Elapsed 4m 9s (remain 6m 0s) Loss: 0.3184(0.4943) Grad: 105381.5469  LR: 0.00001277  \n","Epoch: [2][280/638] Elapsed 4m 28s (remain 5m 41s) Loss: 0.4732(0.4940) Grad: 90613.1953  LR: 0.00001242  \n","Epoch: [2][300/638] Elapsed 4m 47s (remain 5m 22s) Loss: 0.4607(0.4936) Grad: 80826.0859  LR: 0.00001206  \n","Epoch: [2][320/638] Elapsed 5m 6s (remain 5m 3s) Loss: 0.5441(0.4938) Grad: 100393.9219  LR: 0.00001170  \n","Epoch: [2][340/638] Elapsed 5m 25s (remain 4m 43s) Loss: 0.3938(0.4915) Grad: 94789.6953  LR: 0.00001134  \n","Epoch: [2][360/638] Elapsed 5m 45s (remain 4m 24s) Loss: 0.4831(0.4909) Grad: 76203.6016  LR: 0.00001098  \n","Epoch: [2][380/638] Elapsed 6m 4s (remain 4m 5s) Loss: 0.4595(0.4908) Grad: 96623.0703  LR: 0.00001062  \n","Epoch: [2][400/638] Elapsed 6m 23s (remain 3m 46s) Loss: 0.4733(0.4922) Grad: 74039.4922  LR: 0.00001026  \n","Epoch: [2][420/638] Elapsed 6m 42s (remain 3m 27s) Loss: 0.7214(0.4921) Grad: 164310.5000  LR: 0.00000989  \n","Epoch: [2][440/638] Elapsed 7m 1s (remain 3m 8s) Loss: 0.3206(0.4905) Grad: 75432.1641  LR: 0.00000953  \n","Epoch: [2][460/638] Elapsed 7m 20s (remain 2m 49s) Loss: 0.3477(0.4879) Grad: 131683.5938  LR: 0.00000916  \n","Epoch: [2][480/638] Elapsed 7m 39s (remain 2m 30s) Loss: 0.4067(0.4872) Grad: 84534.5547  LR: 0.00000880  \n","Epoch: [2][500/638] Elapsed 7m 59s (remain 2m 10s) Loss: 0.4642(0.4858) Grad: 102398.7969  LR: 0.00000844  \n","Epoch: [2][520/638] Elapsed 8m 18s (remain 1m 51s) Loss: 0.4710(0.4853) Grad: 58309.5664  LR: 0.00000808  \n","Epoch: [2][540/638] Elapsed 8m 37s (remain 1m 32s) Loss: 0.2854(0.4849) Grad: 54886.5391  LR: 0.00000772  \n","Epoch: [2][560/638] Elapsed 8m 56s (remain 1m 13s) Loss: 0.3493(0.4838) Grad: 114742.2422  LR: 0.00000737  \n","Epoch: [2][580/638] Elapsed 9m 15s (remain 0m 54s) Loss: 0.5373(0.4809) Grad: 48490.4922  LR: 0.00000702  \n","Epoch: [2][600/638] Elapsed 9m 34s (remain 0m 35s) Loss: 0.4757(0.4810) Grad: 59569.6992  LR: 0.00000668  \n","Epoch: [2][620/638] Elapsed 9m 53s (remain 0m 16s) Loss: 0.4999(0.4798) Grad: 89853.5625  LR: 0.00000633  \n","Epoch: [2][637/638] Elapsed 10m 10s (remain 0m 0s) Loss: 0.3499(0.4776) Grad: 92075.6484  LR: 0.00000605  \n","EVAL: [0/129] Elapsed 0m 1s (remain 2m 45s) Loss: 0.5357(0.5357) \n","EVAL: [20/129] Elapsed 0m 24s (remain 2m 3s) Loss: 0.5380(0.5123) \n","EVAL: [40/129] Elapsed 0m 46s (remain 1m 38s) Loss: 0.5150(0.4994) \n","EVAL: [60/129] Elapsed 1m 8s (remain 1m 16s) Loss: 0.5674(0.5057) \n","EVAL: [80/129] Elapsed 1m 30s (remain 0m 53s) Loss: 0.5208(0.4982) \n","EVAL: [100/129] Elapsed 1m 52s (remain 0m 31s) Loss: 0.5504(0.4944) \n","EVAL: [120/129] Elapsed 2m 14s (remain 0m 8s) Loss: 0.4782(0.4931) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4776  avg_val_loss: 0.4920  time: 753s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4776  avg_val_loss: 0.4920  time: 753s\n","Epoch 2 - Score: 0.5001  Scores: [0.4563724140893895, 0.5437784964515058]\n","INFO:__main__:Epoch 2 - Score: 0.5001  Scores: [0.4563724140893895, 0.5437784964515058]\n","Epoch 2 - Save Best Score: 0.5001 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.5001 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [128/129] Elapsed 2m 23s (remain 0m 0s) Loss: 0.4570(0.4920) \n","Epoch: [3][0/638] Elapsed 0m 1s (remain 12m 53s) Loss: 0.3140(0.3140) Grad: inf  LR: 0.00000603  \n","Epoch: [3][20/638] Elapsed 0m 20s (remain 9m 55s) Loss: 0.6198(0.4035) Grad: 74967.1172  LR: 0.00000570  \n","Epoch: [3][40/638] Elapsed 0m 39s (remain 9m 33s) Loss: 0.3309(0.4129) Grad: 86514.4453  LR: 0.00000537  \n","Epoch: [3][60/638] Elapsed 0m 58s (remain 9m 13s) Loss: 0.3009(0.4068) Grad: 80261.1797  LR: 0.00000505  \n","Epoch: [3][80/638] Elapsed 1m 17s (remain 8m 53s) Loss: 0.4690(0.4109) Grad: 235836.3594  LR: 0.00000474  \n","Epoch: [3][100/638] Elapsed 1m 36s (remain 8m 32s) Loss: 0.5462(0.4178) Grad: 67710.5859  LR: 0.00000443  \n","Epoch: [3][120/638] Elapsed 1m 55s (remain 8m 13s) Loss: 0.3145(0.4207) Grad: 71801.2109  LR: 0.00000413  \n","Epoch: [3][140/638] Elapsed 2m 14s (remain 7m 54s) Loss: 0.5648(0.4189) Grad: 63995.2461  LR: 0.00000384  \n","Epoch: [3][160/638] Elapsed 2m 33s (remain 7m 35s) Loss: 0.5275(0.4172) Grad: 88775.7422  LR: 0.00000356  \n","Epoch: [3][180/638] Elapsed 2m 52s (remain 7m 16s) Loss: 0.2957(0.4190) Grad: 63676.5078  LR: 0.00000329  \n","Epoch: [3][200/638] Elapsed 3m 12s (remain 6m 57s) Loss: 0.3509(0.4167) Grad: 66969.8750  LR: 0.00000302  \n","Epoch: [3][220/638] Elapsed 3m 31s (remain 6m 38s) Loss: 0.6115(0.4174) Grad: 78935.2031  LR: 0.00000276  \n","Epoch: [3][240/638] Elapsed 3m 50s (remain 6m 19s) Loss: 0.4351(0.4165) Grad: 100814.0234  LR: 0.00000252  \n","Epoch: [3][260/638] Elapsed 4m 9s (remain 6m 0s) Loss: 0.4591(0.4173) Grad: 38138.2266  LR: 0.00000228  \n","Epoch: [3][280/638] Elapsed 4m 28s (remain 5m 40s) Loss: 0.3409(0.4153) Grad: 72007.3203  LR: 0.00000205  \n","Epoch: [3][300/638] Elapsed 4m 47s (remain 5m 21s) Loss: 0.3729(0.4129) Grad: 112304.6172  LR: 0.00000184  \n","Epoch: [3][320/638] Elapsed 5m 6s (remain 5m 2s) Loss: 0.5513(0.4135) Grad: 54577.8672  LR: 0.00000163  \n","Epoch: [3][340/638] Elapsed 5m 25s (remain 4m 43s) Loss: 0.5516(0.4145) Grad: 55844.8125  LR: 0.00000144  \n","Epoch: [3][360/638] Elapsed 5m 44s (remain 4m 24s) Loss: 0.4779(0.4134) Grad: 83184.5391  LR: 0.00000126  \n","Epoch: [3][380/638] Elapsed 6m 3s (remain 4m 5s) Loss: 0.3177(0.4130) Grad: 69256.0000  LR: 0.00000108  \n","Epoch: [3][400/638] Elapsed 6m 23s (remain 3m 46s) Loss: 0.4403(0.4124) Grad: 95895.0078  LR: 0.00000093  \n","Epoch: [3][420/638] Elapsed 6m 42s (remain 3m 27s) Loss: 0.3662(0.4115) Grad: 72452.1328  LR: 0.00000078  \n","Epoch: [3][440/638] Elapsed 7m 1s (remain 3m 8s) Loss: 0.4347(0.4123) Grad: 92877.5234  LR: 0.00000064  \n","Epoch: [3][460/638] Elapsed 7m 20s (remain 2m 49s) Loss: 0.4080(0.4113) Grad: 65621.0156  LR: 0.00000052  \n","Epoch: [3][480/638] Elapsed 7m 39s (remain 2m 29s) Loss: 0.3932(0.4092) Grad: 83545.1953  LR: 0.00000041  \n","Epoch: [3][500/638] Elapsed 7m 58s (remain 2m 10s) Loss: 0.3675(0.4075) Grad: 44908.4727  LR: 0.00000031  \n","Epoch: [3][520/638] Elapsed 8m 17s (remain 1m 51s) Loss: 0.4605(0.4065) Grad: 62823.1289  LR: 0.00000023  \n","Epoch: [3][540/638] Elapsed 8m 36s (remain 1m 32s) Loss: 0.3771(0.4066) Grad: 39271.6211  LR: 0.00000016  \n","Epoch: [3][560/638] Elapsed 8m 55s (remain 1m 13s) Loss: 0.3739(0.4063) Grad: 78072.8359  LR: 0.00000010  \n","Epoch: [3][580/638] Elapsed 9m 14s (remain 0m 54s) Loss: 0.2725(0.4059) Grad: 82231.2031  LR: 0.00000006  \n","Epoch: [3][600/638] Elapsed 9m 33s (remain 0m 35s) Loss: 0.4197(0.4054) Grad: 77593.3594  LR: 0.00000002  \n","Epoch: [3][620/638] Elapsed 9m 52s (remain 0m 16s) Loss: 0.5061(0.4048) Grad: 39590.3633  LR: 0.00000001  \n","Epoch: [3][637/638] Elapsed 10m 9s (remain 0m 0s) Loss: 0.3939(0.4044) Grad: 64544.6992  LR: 0.00000000  \n","EVAL: [0/129] Elapsed 0m 1s (remain 2m 46s) Loss: 0.4396(0.4396) \n","EVAL: [20/129] Elapsed 0m 24s (remain 2m 3s) Loss: 0.4544(0.4702) \n","EVAL: [40/129] Elapsed 0m 46s (remain 1m 38s) Loss: 0.4873(0.4635) \n","EVAL: [60/129] Elapsed 1m 8s (remain 1m 16s) Loss: 0.5236(0.4663) \n","EVAL: [80/129] Elapsed 1m 30s (remain 0m 53s) Loss: 0.4589(0.4606) \n","EVAL: [100/129] Elapsed 1m 52s (remain 0m 31s) Loss: 0.5592(0.4571) \n","EVAL: [120/129] Elapsed 2m 14s (remain 0m 8s) Loss: 0.4151(0.4560) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.4044  avg_val_loss: 0.4546  time: 753s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.4044  avg_val_loss: 0.4546  time: 753s\n","Epoch 3 - Score: 0.4636  Scores: [0.4142017583793012, 0.5130377705589038]\n","INFO:__main__:Epoch 3 - Score: 0.4636  Scores: [0.4142017583793012, 0.5130377705589038]\n","Epoch 3 - Save Best Score: 0.4636 Model\n","INFO:__main__:Epoch 3 - Save Best Score: 0.4636 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [128/129] Elapsed 2m 23s (remain 0m 0s) Loss: 0.3908(0.4546) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 1 result ==========\n","INFO:__main__:========== fold: 1 result ==========\n","Score: 0.4636  Scores: [0.4142017583793012, 0.5130377705589038]\n","INFO:__main__:Score: 0.4636  Scores: [0.4142017583793012, 0.5130377705589038]\n","========== fold: 2 training ==========\n","INFO:__main__:========== fold: 2 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/644] Elapsed 0m 1s (remain 13m 8s) Loss: 1.1447(1.1447) Grad: inf  LR: 0.00000010  \n","Epoch: [1][20/644] Elapsed 0m 20s (remain 10m 2s) Loss: 0.5419(1.0952) Grad: 128770.7266  LR: 0.00000218  \n","Epoch: [1][40/644] Elapsed 0m 39s (remain 9m 38s) Loss: 0.6212(1.0517) Grad: 102766.7734  LR: 0.00000425  \n","Epoch: [1][60/644] Elapsed 0m 58s (remain 9m 18s) Loss: 1.0342(1.0164) Grad: 146498.9531  LR: 0.00000632  \n","Epoch: [1][80/644] Elapsed 1m 17s (remain 8m 59s) Loss: 1.1921(0.9803) Grad: 1295099.1250  LR: 0.00000839  \n","Epoch: [1][100/644] Elapsed 1m 36s (remain 8m 40s) Loss: 0.5622(0.9566) Grad: 1848592.8750  LR: 0.00001047  \n","Epoch: [1][120/644] Elapsed 1m 55s (remain 8m 20s) Loss: 0.8650(0.9088) Grad: 1390417.2500  LR: 0.00001254  \n","Epoch: [1][140/644] Elapsed 2m 14s (remain 8m 0s) Loss: 0.8542(0.8802) Grad: 1002350.6250  LR: 0.00001461  \n","Epoch: [1][160/644] Elapsed 2m 33s (remain 7m 41s) Loss: 0.4301(0.8537) Grad: 611889.9375  LR: 0.00001668  \n","Epoch: [1][180/644] Elapsed 2m 53s (remain 7m 22s) Loss: 0.6068(0.8342) Grad: 1173587.6250  LR: 0.00001876  \n","Epoch: [1][200/644] Elapsed 3m 12s (remain 7m 3s) Loss: 0.5607(0.8138) Grad: 594280.1250  LR: 0.00002000  \n","Epoch: [1][220/644] Elapsed 3m 31s (remain 6m 44s) Loss: 0.8421(0.8004) Grad: 551184.0625  LR: 0.00001999  \n","Epoch: [1][240/644] Elapsed 3m 50s (remain 6m 25s) Loss: 0.5442(0.7885) Grad: 235360.7969  LR: 0.00001996  \n","Epoch: [1][260/644] Elapsed 4m 9s (remain 6m 5s) Loss: 0.5873(0.7745) Grad: 410463.9375  LR: 0.00001992  \n","Epoch: [1][280/644] Elapsed 4m 28s (remain 5m 46s) Loss: 0.5208(0.7626) Grad: 118702.8672  LR: 0.00001987  \n","Epoch: [1][300/644] Elapsed 4m 47s (remain 5m 27s) Loss: 0.4623(0.7482) Grad: 155773.0625  LR: 0.00001981  \n","Epoch: [1][320/644] Elapsed 5m 6s (remain 5m 8s) Loss: 0.4438(0.7383) Grad: 64734.7852  LR: 0.00001973  \n","Epoch: [1][340/644] Elapsed 5m 25s (remain 4m 49s) Loss: 0.5820(0.7261) Grad: 75793.9531  LR: 0.00001965  \n","Epoch: [1][360/644] Elapsed 5m 44s (remain 4m 30s) Loss: 0.6768(0.7184) Grad: 226476.3906  LR: 0.00001954  \n","Epoch: [1][380/644] Elapsed 6m 4s (remain 4m 11s) Loss: 0.6639(0.7088) Grad: 182135.9062  LR: 0.00001943  \n","Epoch: [1][400/644] Elapsed 6m 23s (remain 3m 52s) Loss: 0.5513(0.7026) Grad: 225167.8906  LR: 0.00001930  \n","Epoch: [1][420/644] Elapsed 6m 42s (remain 3m 33s) Loss: 0.7132(0.6940) Grad: 134708.7656  LR: 0.00001916  \n","Epoch: [1][440/644] Elapsed 7m 1s (remain 3m 13s) Loss: 0.4389(0.6851) Grad: 78575.7109  LR: 0.00001901  \n","Epoch: [1][460/644] Elapsed 7m 20s (remain 2m 54s) Loss: 0.2991(0.6756) Grad: 65493.7070  LR: 0.00001885  \n","Epoch: [1][480/644] Elapsed 7m 39s (remain 2m 35s) Loss: 0.4300(0.6694) Grad: 87501.6484  LR: 0.00001868  \n","Epoch: [1][500/644] Elapsed 7m 58s (remain 2m 16s) Loss: 0.4003(0.6609) Grad: 118498.5234  LR: 0.00001849  \n","Epoch: [1][520/644] Elapsed 8m 17s (remain 1m 57s) Loss: 0.5138(0.6544) Grad: 82891.7500  LR: 0.00001830  \n","Epoch: [1][540/644] Elapsed 8m 37s (remain 1m 38s) Loss: 0.5796(0.6483) Grad: 122143.3359  LR: 0.00001809  \n","Epoch: [1][560/644] Elapsed 8m 56s (remain 1m 19s) Loss: 0.5652(0.6469) Grad: 99262.4141  LR: 0.00001787  \n","Epoch: [1][580/644] Elapsed 9m 15s (remain 1m 0s) Loss: 0.6230(0.6419) Grad: 108205.7344  LR: 0.00001764  \n","Epoch: [1][600/644] Elapsed 9m 34s (remain 0m 41s) Loss: 0.5345(0.6366) Grad: 128101.3672  LR: 0.00001741  \n","Epoch: [1][620/644] Elapsed 9m 53s (remain 0m 21s) Loss: 0.8991(0.6325) Grad: 108774.0078  LR: 0.00001716  \n","Epoch: [1][640/644] Elapsed 10m 12s (remain 0m 2s) Loss: 0.5397(0.6282) Grad: 198955.7500  LR: 0.00001690  \n","Epoch: [1][643/644] Elapsed 10m 15s (remain 0m 0s) Loss: 0.6879(0.6276) Grad: 88817.1172  LR: 0.00001686  \n","EVAL: [0/126] Elapsed 0m 1s (remain 3m 2s) Loss: 0.9566(0.9566) \n","EVAL: [20/126] Elapsed 0m 24s (remain 2m 1s) Loss: 0.7724(0.7755) \n","EVAL: [40/126] Elapsed 0m 47s (remain 1m 38s) Loss: 0.7895(0.7547) \n","EVAL: [60/126] Elapsed 1m 10s (remain 1m 15s) Loss: 0.8391(0.7578) \n","EVAL: [80/126] Elapsed 1m 33s (remain 0m 52s) Loss: 0.8478(0.7679) \n","EVAL: [100/126] Elapsed 1m 55s (remain 0m 28s) Loss: 0.7678(0.7719) \n","EVAL: [120/126] Elapsed 2m 18s (remain 0m 5s) Loss: 0.9430(0.7678) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.6276  avg_val_loss: 0.7696  time: 759s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.6276  avg_val_loss: 0.7696  time: 759s\n","Epoch 1 - Score: 0.7819  Scores: [0.620771975481113, 0.9429453064209558]\n","INFO:__main__:Epoch 1 - Score: 0.7819  Scores: [0.620771975481113, 0.9429453064209558]\n","Epoch 1 - Save Best Score: 0.7819 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.7819 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [125/126] Elapsed 2m 23s (remain 0m 0s) Loss: 0.6346(0.7696) \n","Epoch: [2][0/644] Elapsed 0m 1s (remain 13m 11s) Loss: 0.6878(0.6878) Grad: inf  LR: 0.00001685  \n","Epoch: [2][20/644] Elapsed 0m 20s (remain 10m 1s) Loss: 0.5908(0.5286) Grad: 95538.5078  LR: 0.00001658  \n","Epoch: [2][40/644] Elapsed 0m 39s (remain 9m 39s) Loss: 0.5683(0.5007) Grad: 86240.6719  LR: 0.00001631  \n","Epoch: [2][60/644] Elapsed 0m 58s (remain 9m 19s) Loss: 0.3387(0.4909) Grad: 129774.2656  LR: 0.00001602  \n","Epoch: [2][80/644] Elapsed 1m 17s (remain 8m 59s) Loss: 0.3878(0.4747) Grad: 26776.8301  LR: 0.00001573  \n","Epoch: [2][100/644] Elapsed 1m 36s (remain 8m 40s) Loss: 0.4897(0.4778) Grad: 130081.5391  LR: 0.00001543  \n","Epoch: [2][120/644] Elapsed 1m 55s (remain 8m 21s) Loss: 0.6060(0.4848) Grad: 155081.1250  LR: 0.00001512  \n","Epoch: [2][140/644] Elapsed 2m 15s (remain 8m 1s) Loss: 0.4785(0.4759) Grad: 67172.3281  LR: 0.00001481  \n","Epoch: [2][160/644] Elapsed 2m 33s (remain 7m 41s) Loss: 0.5274(0.4712) Grad: 139200.8750  LR: 0.00001449  \n","Epoch: [2][180/644] Elapsed 2m 53s (remain 7m 22s) Loss: 0.4411(0.4733) Grad: 71828.0781  LR: 0.00001417  \n","Epoch: [2][200/644] Elapsed 3m 12s (remain 7m 3s) Loss: 0.4362(0.4734) Grad: 107255.2500  LR: 0.00001384  \n","Epoch: [2][220/644] Elapsed 3m 31s (remain 6m 44s) Loss: 0.5439(0.4753) Grad: 127472.0312  LR: 0.00001350  \n","Epoch: [2][240/644] Elapsed 3m 50s (remain 6m 25s) Loss: 0.5630(0.4746) Grad: 110166.5781  LR: 0.00001316  \n","Epoch: [2][260/644] Elapsed 4m 9s (remain 6m 6s) Loss: 0.4839(0.4750) Grad: 128598.0547  LR: 0.00001281  \n","Epoch: [2][280/644] Elapsed 4m 28s (remain 5m 47s) Loss: 0.4768(0.4737) Grad: 63518.6250  LR: 0.00001247  \n","Epoch: [2][300/644] Elapsed 4m 47s (remain 5m 27s) Loss: 0.4058(0.4742) Grad: 45004.9844  LR: 0.00001211  \n","Epoch: [2][320/644] Elapsed 5m 6s (remain 5m 8s) Loss: 0.4166(0.4714) Grad: 113584.4844  LR: 0.00001176  \n","Epoch: [2][340/644] Elapsed 5m 25s (remain 4m 49s) Loss: 0.3224(0.4684) Grad: 83757.7500  LR: 0.00001140  \n","Epoch: [2][360/644] Elapsed 5m 44s (remain 4m 30s) Loss: 0.6515(0.4690) Grad: 169684.6719  LR: 0.00001105  \n","Epoch: [2][380/644] Elapsed 6m 4s (remain 4m 11s) Loss: 0.5422(0.4671) Grad: 119736.9219  LR: 0.00001069  \n","Epoch: [2][400/644] Elapsed 6m 22s (remain 3m 52s) Loss: 0.4669(0.4646) Grad: 88674.3984  LR: 0.00001032  \n","Epoch: [2][420/644] Elapsed 6m 41s (remain 3m 32s) Loss: 0.4715(0.4628) Grad: 47616.7188  LR: 0.00000996  \n","Epoch: [2][440/644] Elapsed 7m 0s (remain 3m 13s) Loss: 0.5159(0.4605) Grad: 90136.4766  LR: 0.00000960  \n","Epoch: [2][460/644] Elapsed 7m 20s (remain 2m 54s) Loss: 0.3993(0.4589) Grad: 119378.3828  LR: 0.00000924  \n","Epoch: [2][480/644] Elapsed 7m 39s (remain 2m 35s) Loss: 0.3566(0.4580) Grad: 100018.3750  LR: 0.00000888  \n","Epoch: [2][500/644] Elapsed 7m 58s (remain 2m 16s) Loss: 0.4595(0.4563) Grad: 99190.3438  LR: 0.00000852  \n","Epoch: [2][520/644] Elapsed 8m 17s (remain 1m 57s) Loss: 0.5789(0.4565) Grad: 135924.0312  LR: 0.00000817  \n","Epoch: [2][540/644] Elapsed 8m 36s (remain 1m 38s) Loss: 0.3416(0.4565) Grad: 100265.3281  LR: 0.00000782  \n","Epoch: [2][560/644] Elapsed 8m 55s (remain 1m 19s) Loss: 0.4315(0.4557) Grad: 55489.9727  LR: 0.00000746  \n","Epoch: [2][580/644] Elapsed 9m 14s (remain 1m 0s) Loss: 0.4514(0.4554) Grad: 68430.7734  LR: 0.00000712  \n","Epoch: [2][600/644] Elapsed 9m 33s (remain 0m 41s) Loss: 0.2807(0.4537) Grad: 65639.4297  LR: 0.00000677  \n","Epoch: [2][620/644] Elapsed 9m 52s (remain 0m 21s) Loss: 0.3740(0.4523) Grad: 110053.6641  LR: 0.00000643  \n","Epoch: [2][640/644] Elapsed 10m 11s (remain 0m 2s) Loss: 0.3601(0.4518) Grad: 87324.8750  LR: 0.00000610  \n","Epoch: [2][643/644] Elapsed 10m 14s (remain 0m 0s) Loss: 0.4927(0.4522) Grad: 138986.9844  LR: 0.00000605  \n","EVAL: [0/126] Elapsed 0m 1s (remain 3m 2s) Loss: 0.7682(0.7682) \n","EVAL: [20/126] Elapsed 0m 24s (remain 2m 1s) Loss: 0.5383(0.5604) \n","EVAL: [40/126] Elapsed 0m 47s (remain 1m 38s) Loss: 0.6442(0.5556) \n","EVAL: [60/126] Elapsed 1m 10s (remain 1m 15s) Loss: 0.6304(0.5488) \n","EVAL: [80/126] Elapsed 1m 33s (remain 0m 51s) Loss: 0.6428(0.5533) \n","EVAL: [100/126] Elapsed 1m 55s (remain 0m 28s) Loss: 0.5652(0.5567) \n","EVAL: [120/126] Elapsed 2m 18s (remain 0m 5s) Loss: 0.7329(0.5559) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4522  avg_val_loss: 0.5562  time: 758s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4522  avg_val_loss: 0.5562  time: 758s\n","Epoch 2 - Score: 0.5668  Scores: [0.5120275815362262, 0.6216015226680706]\n","INFO:__main__:Epoch 2 - Score: 0.5668  Scores: [0.5120275815362262, 0.6216015226680706]\n","Epoch 2 - Save Best Score: 0.5668 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.5668 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [125/126] Elapsed 2m 23s (remain 0m 0s) Loss: 0.4063(0.5562) \n","Epoch: [3][0/644] Elapsed 0m 1s (remain 13m 3s) Loss: 0.3388(0.3388) Grad: inf  LR: 0.00000603  \n","Epoch: [3][20/644] Elapsed 0m 20s (remain 10m 2s) Loss: 0.2857(0.3904) Grad: 93235.1172  LR: 0.00000570  \n","Epoch: [3][40/644] Elapsed 0m 39s (remain 9m 40s) Loss: 0.3440(0.3921) Grad: 71455.8281  LR: 0.00000538  \n","Epoch: [3][60/644] Elapsed 0m 58s (remain 9m 20s) Loss: 0.3419(0.3874) Grad: 77528.2812  LR: 0.00000506  \n","Epoch: [3][80/644] Elapsed 1m 17s (remain 9m 0s) Loss: 0.3266(0.3816) Grad: 110419.2734  LR: 0.00000475  \n","Epoch: [3][100/644] Elapsed 1m 36s (remain 8m 40s) Loss: 0.3652(0.3812) Grad: 97408.5078  LR: 0.00000445  \n","Epoch: [3][120/644] Elapsed 1m 55s (remain 8m 20s) Loss: 0.2333(0.3815) Grad: 38735.8594  LR: 0.00000415  \n","Epoch: [3][140/644] Elapsed 2m 15s (remain 8m 1s) Loss: 0.3388(0.3810) Grad: 102466.4922  LR: 0.00000386  \n","Epoch: [3][160/644] Elapsed 2m 34s (remain 7m 42s) Loss: 0.3803(0.3807) Grad: 100417.9375  LR: 0.00000358  \n","Epoch: [3][180/644] Elapsed 2m 53s (remain 7m 23s) Loss: 0.2486(0.3782) Grad: 93678.3125  LR: 0.00000331  \n","Epoch: [3][200/644] Elapsed 3m 12s (remain 7m 3s) Loss: 0.3760(0.3797) Grad: 85798.3594  LR: 0.00000304  \n","Epoch: [3][220/644] Elapsed 3m 31s (remain 6m 44s) Loss: 0.2561(0.3804) Grad: 42793.6836  LR: 0.00000279  \n","Epoch: [3][240/644] Elapsed 3m 50s (remain 6m 25s) Loss: 0.3963(0.3784) Grad: 56019.7969  LR: 0.00000254  \n","Epoch: [3][260/644] Elapsed 4m 9s (remain 6m 6s) Loss: 0.4535(0.3787) Grad: 102084.9297  LR: 0.00000231  \n","Epoch: [3][280/644] Elapsed 4m 28s (remain 5m 47s) Loss: 0.3340(0.3752) Grad: 78863.1797  LR: 0.00000208  \n","Epoch: [3][300/644] Elapsed 4m 47s (remain 5m 27s) Loss: 0.3094(0.3751) Grad: 68992.0000  LR: 0.00000187  \n","Epoch: [3][320/644] Elapsed 5m 6s (remain 5m 8s) Loss: 0.4637(0.3757) Grad: 105884.8438  LR: 0.00000166  \n","Epoch: [3][340/644] Elapsed 5m 25s (remain 4m 49s) Loss: 0.5926(0.3752) Grad: 143638.4688  LR: 0.00000147  \n","Epoch: [3][360/644] Elapsed 5m 44s (remain 4m 30s) Loss: 0.3479(0.3777) Grad: 38783.4453  LR: 0.00000129  \n","Epoch: [3][380/644] Elapsed 6m 3s (remain 4m 11s) Loss: 0.4173(0.3768) Grad: 132773.0469  LR: 0.00000111  \n","Epoch: [3][400/644] Elapsed 6m 22s (remain 3m 51s) Loss: 0.2974(0.3767) Grad: 96953.8516  LR: 0.00000095  \n","Epoch: [3][420/644] Elapsed 6m 41s (remain 3m 32s) Loss: 0.3672(0.3765) Grad: 107393.2266  LR: 0.00000081  \n","Epoch: [3][440/644] Elapsed 7m 0s (remain 3m 13s) Loss: 0.3291(0.3752) Grad: 74680.6953  LR: 0.00000067  \n","Epoch: [3][460/644] Elapsed 7m 20s (remain 2m 54s) Loss: 0.4170(0.3750) Grad: 43157.0234  LR: 0.00000055  \n","Epoch: [3][480/644] Elapsed 7m 38s (remain 2m 35s) Loss: 0.3715(0.3735) Grad: 69666.3281  LR: 0.00000044  \n","Epoch: [3][500/644] Elapsed 7m 58s (remain 2m 16s) Loss: 0.3126(0.3729) Grad: 48104.8008  LR: 0.00000034  \n","Epoch: [3][520/644] Elapsed 8m 17s (remain 1m 57s) Loss: 0.4370(0.3725) Grad: 46257.6797  LR: 0.00000025  \n","Epoch: [3][540/644] Elapsed 8m 36s (remain 1m 38s) Loss: 0.5286(0.3723) Grad: 103759.5078  LR: 0.00000018  \n","Epoch: [3][560/644] Elapsed 8m 55s (remain 1m 19s) Loss: 0.3325(0.3725) Grad: 78509.0469  LR: 0.00000011  \n","Epoch: [3][580/644] Elapsed 9m 14s (remain 1m 0s) Loss: 0.3380(0.3727) Grad: 52282.4219  LR: 0.00000007  \n","Epoch: [3][600/644] Elapsed 9m 33s (remain 0m 41s) Loss: 0.4555(0.3719) Grad: 96558.8984  LR: 0.00000003  \n","Epoch: [3][620/644] Elapsed 9m 52s (remain 0m 21s) Loss: 0.3187(0.3720) Grad: 97345.4219  LR: 0.00000001  \n","Epoch: [3][640/644] Elapsed 10m 11s (remain 0m 2s) Loss: 0.5070(0.3716) Grad: 68995.5469  LR: 0.00000000  \n","Epoch: [3][643/644] Elapsed 10m 14s (remain 0m 0s) Loss: 0.3709(0.3714) Grad: 87795.1875  LR: 0.00000000  \n","EVAL: [0/126] Elapsed 0m 1s (remain 3m 1s) Loss: 0.7466(0.7466) \n","EVAL: [20/126] Elapsed 0m 24s (remain 2m 0s) Loss: 0.4158(0.4935) \n","EVAL: [40/126] Elapsed 0m 47s (remain 1m 38s) Loss: 0.5119(0.5026) \n","EVAL: [60/126] Elapsed 1m 10s (remain 1m 15s) Loss: 0.5420(0.4955) \n","EVAL: [80/126] Elapsed 1m 33s (remain 0m 51s) Loss: 0.5670(0.4963) \n","EVAL: [100/126] Elapsed 1m 55s (remain 0m 28s) Loss: 0.4866(0.4962) \n","EVAL: [120/126] Elapsed 2m 18s (remain 0m 5s) Loss: 0.6247(0.5000) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3714  avg_val_loss: 0.4992  time: 758s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3714  avg_val_loss: 0.4992  time: 758s\n","Epoch 3 - Score: 0.5098  Scores: [0.4368407311072702, 0.5827776890856115]\n","INFO:__main__:Epoch 3 - Score: 0.5098  Scores: [0.4368407311072702, 0.5827776890856115]\n","Epoch 3 - Save Best Score: 0.5098 Model\n","INFO:__main__:Epoch 3 - Save Best Score: 0.5098 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [125/126] Elapsed 2m 23s (remain 0m 0s) Loss: 0.2821(0.4992) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 2 result ==========\n","INFO:__main__:========== fold: 2 result ==========\n","Score: 0.5098  Scores: [0.4368407311072702, 0.5827776890856115]\n","INFO:__main__:Score: 0.5098  Scores: [0.4368407311072702, 0.5827776890856115]\n","========== fold: 3 training ==========\n","INFO:__main__:========== fold: 3 training ==========\n","DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 1024,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.31.0\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/646] Elapsed 0m 0s (remain 10m 31s) Loss: 1.2212(1.2212) Grad: inf  LR: 0.00000010  \n","Epoch: [1][20/646] Elapsed 0m 17s (remain 8m 39s) Loss: 1.0362(1.1059) Grad: 134395.8281  LR: 0.00000218  \n","Epoch: [1][40/646] Elapsed 0m 33s (remain 8m 20s) Loss: 0.8986(1.0938) Grad: 145497.6250  LR: 0.00000425  \n","Epoch: [1][60/646] Elapsed 0m 50s (remain 8m 6s) Loss: 1.0158(1.0847) Grad: 148990.1562  LR: 0.00000632  \n","Epoch: [1][80/646] Elapsed 1m 7s (remain 7m 48s) Loss: 0.8733(1.0438) Grad: 195638.2656  LR: 0.00000839  \n","Epoch: [1][100/646] Elapsed 1m 23s (remain 7m 32s) Loss: 0.5360(0.9877) Grad: 499724.0625  LR: 0.00001047  \n","Epoch: [1][120/646] Elapsed 1m 39s (remain 7m 13s) Loss: 0.6082(0.9261) Grad: 256313.6562  LR: 0.00001254  \n","Epoch: [1][140/646] Elapsed 1m 56s (remain 6m 56s) Loss: 0.7217(0.8870) Grad: 650705.0625  LR: 0.00001461  \n","Epoch: [1][160/646] Elapsed 2m 12s (remain 6m 39s) Loss: 1.0437(0.8688) Grad: 415284.2188  LR: 0.00001668  \n","Epoch: [1][180/646] Elapsed 2m 29s (remain 6m 22s) Loss: 0.6654(0.8501) Grad: 357537.0625  LR: 0.00001876  \n","Epoch: [1][200/646] Elapsed 2m 45s (remain 6m 6s) Loss: 0.4126(0.8209) Grad: 91991.5625  LR: 0.00002000  \n","Epoch: [1][220/646] Elapsed 3m 1s (remain 5m 48s) Loss: 0.6774(0.7988) Grad: 231397.7344  LR: 0.00001999  \n","Epoch: [1][240/646] Elapsed 3m 17s (remain 5m 31s) Loss: 0.5262(0.7792) Grad: 37386.5195  LR: 0.00001996  \n","Epoch: [1][260/646] Elapsed 3m 33s (remain 5m 14s) Loss: 0.5402(0.7624) Grad: 70551.0234  LR: 0.00001993  \n","Epoch: [1][280/646] Elapsed 3m 49s (remain 4m 58s) Loss: 0.7342(0.7527) Grad: 194964.8906  LR: 0.00001987  \n","Epoch: [1][300/646] Elapsed 4m 5s (remain 4m 41s) Loss: 0.8179(0.7416) Grad: 234921.0312  LR: 0.00001981  \n","Epoch: [1][320/646] Elapsed 4m 22s (remain 4m 25s) Loss: 0.4192(0.7313) Grad: 210261.4062  LR: 0.00001974  \n","Epoch: [1][340/646] Elapsed 4m 37s (remain 4m 8s) Loss: 0.5313(0.7222) Grad: 108398.2969  LR: 0.00001965  \n","Epoch: [1][360/646] Elapsed 4m 53s (remain 3m 51s) Loss: 0.4152(0.7142) Grad: 73903.5391  LR: 0.00001955  \n","Epoch: [1][380/646] Elapsed 5m 8s (remain 3m 34s) Loss: 0.3318(0.7045) Grad: 218621.1719  LR: 0.00001943  \n","Epoch: [1][400/646] Elapsed 5m 25s (remain 3m 19s) Loss: 0.4717(0.6980) Grad: 139374.5938  LR: 0.00001931  \n","Epoch: [1][420/646] Elapsed 5m 42s (remain 3m 2s) Loss: 0.7588(0.6913) Grad: 175098.4219  LR: 0.00001917  \n","Epoch: [1][440/646] Elapsed 5m 57s (remain 2m 46s) Loss: 0.4060(0.6855) Grad: 100358.3828  LR: 0.00001902  \n","Epoch: [1][460/646] Elapsed 6m 14s (remain 2m 30s) Loss: 0.5108(0.6798) Grad: 94336.0078  LR: 0.00001886  \n","Epoch: [1][480/646] Elapsed 6m 31s (remain 2m 14s) Loss: 0.4938(0.6720) Grad: 172496.5156  LR: 0.00001869  \n","Epoch: [1][500/646] Elapsed 6m 48s (remain 1m 58s) Loss: 0.4237(0.6664) Grad: 67796.5078  LR: 0.00001850  \n","Epoch: [1][520/646] Elapsed 7m 4s (remain 1m 41s) Loss: 0.3575(0.6604) Grad: 114709.2344  LR: 0.00001831  \n","Epoch: [1][540/646] Elapsed 7m 20s (remain 1m 25s) Loss: 0.3565(0.6531) Grad: 105220.0312  LR: 0.00001810  \n","Epoch: [1][560/646] Elapsed 7m 36s (remain 1m 9s) Loss: 0.5059(0.6479) Grad: 77080.5234  LR: 0.00001788  \n","Epoch: [1][580/646] Elapsed 7m 52s (remain 0m 52s) Loss: 0.5420(0.6428) Grad: 130019.0703  LR: 0.00001766  \n","Epoch: [1][600/646] Elapsed 8m 9s (remain 0m 36s) Loss: 0.3908(0.6379) Grad: 77957.7969  LR: 0.00001742  \n","Epoch: [1][620/646] Elapsed 8m 25s (remain 0m 20s) Loss: 0.8523(0.6337) Grad: 126602.5859  LR: 0.00001718  \n","Epoch: [1][640/646] Elapsed 8m 41s (remain 0m 4s) Loss: 0.4788(0.6300) Grad: 105557.0469  LR: 0.00001692  \n","Epoch: [1][645/646] Elapsed 8m 45s (remain 0m 0s) Loss: 0.5220(0.6291) Grad: 74211.2734  LR: 0.00001685  \n","EVAL: [0/125] Elapsed 0m 1s (remain 3m 10s) Loss: 0.5865(0.5865) \n","EVAL: [20/125] Elapsed 0m 26s (remain 2m 11s) Loss: 0.6430(0.5830) \n","EVAL: [40/125] Elapsed 0m 51s (remain 1m 45s) Loss: 0.5136(0.6015) \n","EVAL: [60/125] Elapsed 1m 16s (remain 1m 20s) Loss: 0.6834(0.5935) \n","EVAL: [80/125] Elapsed 1m 41s (remain 0m 55s) Loss: 0.4739(0.5853) \n","EVAL: [100/125] Elapsed 2m 6s (remain 0m 30s) Loss: 0.5926(0.5751) \n","EVAL: [120/125] Elapsed 2m 31s (remain 0m 5s) Loss: 0.6382(0.5771) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.6291  avg_val_loss: 0.5762  time: 682s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.6291  avg_val_loss: 0.5762  time: 682s\n","Epoch 1 - Score: 0.5840  Scores: [0.6345501725076598, 0.533399471895208]\n","INFO:__main__:Epoch 1 - Score: 0.5840  Scores: [0.6345501725076598, 0.533399471895208]\n","Epoch 1 - Save Best Score: 0.5840 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.5840 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [124/125] Elapsed 2m 36s (remain 0m 0s) Loss: 0.4932(0.5762) \n","Epoch: [2][0/646] Elapsed 0m 1s (remain 11m 46s) Loss: 0.6355(0.6355) Grad: inf  LR: 0.00001684  \n","Epoch: [2][20/646] Elapsed 0m 17s (remain 8m 28s) Loss: 0.4103(0.5345) Grad: 110244.0000  LR: 0.00001657  \n","Epoch: [2][40/646] Elapsed 0m 32s (remain 8m 4s) Loss: 0.5430(0.5176) Grad: 187120.1250  LR: 0.00001630  \n","Epoch: [2][60/646] Elapsed 0m 49s (remain 7m 51s) Loss: 0.3712(0.5138) Grad: 75158.3281  LR: 0.00001602  \n","Epoch: [2][80/646] Elapsed 1m 5s (remain 7m 36s) Loss: 0.7367(0.5136) Grad: 182314.3906  LR: 0.00001572  \n","Epoch: [2][100/646] Elapsed 1m 21s (remain 7m 20s) Loss: 0.3655(0.5089) Grad: 70281.9609  LR: 0.00001542  \n","Epoch: [2][120/646] Elapsed 1m 38s (remain 7m 5s) Loss: 0.3049(0.4981) Grad: 84260.1562  LR: 0.00001512  \n","Epoch: [2][140/646] Elapsed 1m 54s (remain 6m 49s) Loss: 0.4008(0.4918) Grad: 51424.3125  LR: 0.00001481  \n","Epoch: [2][160/646] Elapsed 2m 10s (remain 6m 32s) Loss: 0.3671(0.4913) Grad: 121193.2812  LR: 0.00001449  \n","Epoch: [2][180/646] Elapsed 2m 25s (remain 6m 14s) Loss: 0.3995(0.4889) Grad: 82953.0156  LR: 0.00001416  \n","Epoch: [2][200/646] Elapsed 2m 42s (remain 5m 58s) Loss: 0.2604(0.4830) Grad: 48530.9453  LR: 0.00001383  \n","Epoch: [2][220/646] Elapsed 2m 59s (remain 5m 44s) Loss: 0.4304(0.4854) Grad: 107196.9375  LR: 0.00001350  \n","Epoch: [2][240/646] Elapsed 3m 15s (remain 5m 28s) Loss: 0.4911(0.4832) Grad: 131169.5000  LR: 0.00001316  \n","Epoch: [2][260/646] Elapsed 3m 31s (remain 5m 11s) Loss: 0.3330(0.4806) Grad: 77578.2188  LR: 0.00001281  \n","Epoch: [2][280/646] Elapsed 3m 46s (remain 4m 54s) Loss: 0.4135(0.4784) Grad: 73297.6250  LR: 0.00001247  \n","Epoch: [2][300/646] Elapsed 4m 2s (remain 4m 38s) Loss: 0.4984(0.4769) Grad: 115144.9297  LR: 0.00001212  \n","Epoch: [2][320/646] Elapsed 4m 18s (remain 4m 22s) Loss: 0.4676(0.4766) Grad: 126246.1094  LR: 0.00001176  \n","Epoch: [2][340/646] Elapsed 4m 34s (remain 4m 5s) Loss: 0.3326(0.4724) Grad: 85833.7031  LR: 0.00001141  \n","Epoch: [2][360/646] Elapsed 4m 50s (remain 3m 49s) Loss: 0.6377(0.4722) Grad: 101426.6719  LR: 0.00001105  \n","Epoch: [2][380/646] Elapsed 5m 7s (remain 3m 33s) Loss: 0.4141(0.4704) Grad: 115627.0078  LR: 0.00001069  \n","Epoch: [2][400/646] Elapsed 5m 23s (remain 3m 17s) Loss: 0.5659(0.4695) Grad: 87214.0391  LR: 0.00001033  \n","Epoch: [2][420/646] Elapsed 5m 39s (remain 3m 1s) Loss: 0.4022(0.4690) Grad: 127541.5859  LR: 0.00000997  \n","Epoch: [2][440/646] Elapsed 5m 56s (remain 2m 45s) Loss: 0.4226(0.4677) Grad: 102130.1250  LR: 0.00000961  \n","Epoch: [2][460/646] Elapsed 6m 12s (remain 2m 29s) Loss: 0.2939(0.4676) Grad: 98156.0938  LR: 0.00000925  \n","Epoch: [2][480/646] Elapsed 6m 29s (remain 2m 13s) Loss: 0.7406(0.4700) Grad: 106387.8906  LR: 0.00000890  \n","Epoch: [2][500/646] Elapsed 6m 46s (remain 1m 57s) Loss: 0.4170(0.4701) Grad: 143942.7500  LR: 0.00000854  \n","Epoch: [2][520/646] Elapsed 7m 2s (remain 1m 41s) Loss: 0.5828(0.4684) Grad: 128742.3281  LR: 0.00000818  \n","Epoch: [2][540/646] Elapsed 7m 18s (remain 1m 25s) Loss: 0.3095(0.4665) Grad: 103792.2344  LR: 0.00000783  \n","Epoch: [2][560/646] Elapsed 7m 34s (remain 1m 8s) Loss: 0.5132(0.4651) Grad: 51707.9102  LR: 0.00000748  \n","Epoch: [2][580/646] Elapsed 7m 51s (remain 0m 52s) Loss: 0.4562(0.4633) Grad: 81957.2031  LR: 0.00000713  \n","Epoch: [2][600/646] Elapsed 8m 7s (remain 0m 36s) Loss: 0.4833(0.4645) Grad: 98237.5312  LR: 0.00000679  \n","Epoch: [2][620/646] Elapsed 8m 24s (remain 0m 20s) Loss: 0.3215(0.4629) Grad: 88283.1484  LR: 0.00000645  \n","Epoch: [2][640/646] Elapsed 8m 41s (remain 0m 4s) Loss: 0.4215(0.4630) Grad: 96348.0625  LR: 0.00000612  \n","Epoch: [2][645/646] Elapsed 8m 45s (remain 0m 0s) Loss: 0.3936(0.4626) Grad: 46944.8477  LR: 0.00000603  \n","EVAL: [0/125] Elapsed 0m 1s (remain 3m 10s) Loss: 0.4002(0.4002) \n","EVAL: [20/125] Elapsed 0m 26s (remain 2m 11s) Loss: 0.5309(0.4967) \n","EVAL: [40/125] Elapsed 0m 51s (remain 1m 45s) Loss: 0.4826(0.5146) \n","EVAL: [60/125] Elapsed 1m 16s (remain 1m 20s) Loss: 0.6096(0.5086) \n","EVAL: [80/125] Elapsed 1m 41s (remain 0m 55s) Loss: 0.4653(0.5045) \n","EVAL: [100/125] Elapsed 2m 6s (remain 0m 30s) Loss: 0.4698(0.4942) \n","EVAL: [120/125] Elapsed 2m 31s (remain 0m 5s) Loss: 0.4993(0.5003) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.4626  avg_val_loss: 0.4991  time: 682s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.4626  avg_val_loss: 0.4991  time: 682s\n","Epoch 2 - Score: 0.5079  Scores: [0.43044144007941115, 0.585284801772327]\n","INFO:__main__:Epoch 2 - Score: 0.5079  Scores: [0.43044144007941115, 0.585284801772327]\n","Epoch 2 - Save Best Score: 0.5079 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.5079 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [124/125] Elapsed 2m 36s (remain 0m 0s) Loss: 0.4856(0.4991) \n","Epoch: [3][0/646] Elapsed 0m 1s (remain 11m 50s) Loss: 0.5323(0.5323) Grad: inf  LR: 0.00000602  \n","Epoch: [3][20/646] Elapsed 0m 17s (remain 8m 34s) Loss: 0.3489(0.4040) Grad: 87065.6719  LR: 0.00000569  \n","Epoch: [3][40/646] Elapsed 0m 33s (remain 8m 14s) Loss: 0.3604(0.3855) Grad: 107629.1719  LR: 0.00000537  \n","Epoch: [3][60/646] Elapsed 0m 50s (remain 8m 0s) Loss: 0.3021(0.3870) Grad: 104349.3359  LR: 0.00000505  \n","Epoch: [3][80/646] Elapsed 1m 6s (remain 7m 42s) Loss: 0.2740(0.3844) Grad: 44464.6133  LR: 0.00000474  \n","Epoch: [3][100/646] Elapsed 1m 22s (remain 7m 26s) Loss: 0.4380(0.3869) Grad: 117302.6875  LR: 0.00000444  \n","Epoch: [3][120/646] Elapsed 1m 38s (remain 7m 8s) Loss: 0.2677(0.3875) Grad: 116527.5469  LR: 0.00000414  \n","Epoch: [3][140/646] Elapsed 1m 54s (remain 6m 50s) Loss: 0.4630(0.3873) Grad: 73726.7422  LR: 0.00000386  \n","Epoch: [3][160/646] Elapsed 2m 10s (remain 6m 32s) Loss: 0.4583(0.3863) Grad: 104477.6797  LR: 0.00000358  \n","Epoch: [3][180/646] Elapsed 2m 27s (remain 6m 17s) Loss: 0.3084(0.3871) Grad: 39914.0820  LR: 0.00000330  \n","Epoch: [3][200/646] Elapsed 2m 43s (remain 6m 2s) Loss: 0.4581(0.3855) Grad: 46981.1328  LR: 0.00000304  \n","Epoch: [3][220/646] Elapsed 3m 0s (remain 5m 46s) Loss: 0.5900(0.3875) Grad: 120253.8125  LR: 0.00000279  \n","Epoch: [3][240/646] Elapsed 3m 17s (remain 5m 32s) Loss: 0.3732(0.3892) Grad: 67525.0078  LR: 0.00000254  \n","Epoch: [3][260/646] Elapsed 3m 34s (remain 5m 15s) Loss: 0.4745(0.3881) Grad: 57952.6406  LR: 0.00000231  \n","Epoch: [3][280/646] Elapsed 3m 49s (remain 4m 57s) Loss: 0.3378(0.3857) Grad: 99107.5938  LR: 0.00000208  \n","Epoch: [3][300/646] Elapsed 4m 4s (remain 4m 40s) Loss: 0.4073(0.3840) Grad: 56505.1445  LR: 0.00000187  \n","Epoch: [3][320/646] Elapsed 4m 20s (remain 4m 24s) Loss: 0.4419(0.3837) Grad: 93018.0938  LR: 0.00000166  \n","Epoch: [3][340/646] Elapsed 4m 37s (remain 4m 7s) Loss: 0.2411(0.3837) Grad: 70283.5859  LR: 0.00000147  \n","Epoch: [3][360/646] Elapsed 4m 53s (remain 3m 51s) Loss: 0.5133(0.3835) Grad: 59718.4180  LR: 0.00000129  \n","Epoch: [3][380/646] Elapsed 5m 9s (remain 3m 35s) Loss: 0.4203(0.3828) Grad: 74914.9766  LR: 0.00000112  \n","Epoch: [3][400/646] Elapsed 5m 26s (remain 3m 19s) Loss: 0.2540(0.3828) Grad: 67599.4453  LR: 0.00000096  \n","Epoch: [3][420/646] Elapsed 5m 42s (remain 3m 2s) Loss: 0.3804(0.3819) Grad: 116780.7109  LR: 0.00000081  \n","Epoch: [3][440/646] Elapsed 5m 58s (remain 2m 46s) Loss: 0.3757(0.3809) Grad: 128295.0703  LR: 0.00000067  \n","Epoch: [3][460/646] Elapsed 6m 15s (remain 2m 30s) Loss: 0.2920(0.3810) Grad: 101612.0859  LR: 0.00000055  \n","Epoch: [3][480/646] Elapsed 6m 31s (remain 2m 14s) Loss: 0.2440(0.3822) Grad: 54936.5078  LR: 0.00000044  \n","Epoch: [3][500/646] Elapsed 6m 47s (remain 1m 58s) Loss: 0.3789(0.3815) Grad: 44708.6328  LR: 0.00000034  \n","Epoch: [3][520/646] Elapsed 7m 3s (remain 1m 41s) Loss: 0.4048(0.3811) Grad: 65854.3828  LR: 0.00000025  \n","Epoch: [3][540/646] Elapsed 7m 19s (remain 1m 25s) Loss: 0.2883(0.3825) Grad: 71303.9062  LR: 0.00000018  \n","Epoch: [3][560/646] Elapsed 7m 36s (remain 1m 9s) Loss: 0.5932(0.3829) Grad: 93045.4766  LR: 0.00000012  \n","Epoch: [3][580/646] Elapsed 7m 53s (remain 0m 52s) Loss: 0.3699(0.3844) Grad: 48370.6289  LR: 0.00000007  \n","Epoch: [3][600/646] Elapsed 8m 9s (remain 0m 36s) Loss: 0.4039(0.3835) Grad: 97359.1797  LR: 0.00000003  \n","Epoch: [3][620/646] Elapsed 8m 26s (remain 0m 20s) Loss: 0.4918(0.3837) Grad: 66787.8203  LR: 0.00000001  \n","Epoch: [3][640/646] Elapsed 8m 42s (remain 0m 4s) Loss: 0.3186(0.3832) Grad: 102297.5703  LR: 0.00000000  \n","Epoch: [3][645/646] Elapsed 8m 46s (remain 0m 0s) Loss: 0.3254(0.3829) Grad: 53862.5117  LR: 0.00000000  \n","EVAL: [0/125] Elapsed 0m 1s (remain 3m 10s) Loss: 0.4084(0.4084) \n","EVAL: [20/125] Elapsed 0m 26s (remain 2m 11s) Loss: 0.4578(0.4473) \n","EVAL: [40/125] Elapsed 0m 51s (remain 1m 45s) Loss: 0.4235(0.4582) \n","EVAL: [60/125] Elapsed 1m 16s (remain 1m 20s) Loss: 0.5238(0.4519) \n","EVAL: [80/125] Elapsed 1m 41s (remain 0m 55s) Loss: 0.3869(0.4510) \n","EVAL: [100/125] Elapsed 2m 6s (remain 0m 30s) Loss: 0.4506(0.4407) \n","EVAL: [120/125] Elapsed 2m 31s (remain 0m 5s) Loss: 0.5108(0.4465) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.3829  avg_val_loss: 0.4455  time: 683s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.3829  avg_val_loss: 0.4455  time: 683s\n","Epoch 3 - Score: 0.4537  Scores: [0.3951309021594487, 0.5123606191049076]\n","INFO:__main__:Epoch 3 - Score: 0.4537  Scores: [0.3951309021594487, 0.5123606191049076]\n","Epoch 3 - Save Best Score: 0.4537 Model\n","INFO:__main__:Epoch 3 - Save Best Score: 0.4537 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [124/125] Elapsed 2m 36s (remain 0m 0s) Loss: 0.4131(0.4455) \n"]}],"source":["if __name__ == '__main__':\n","\n","    def get_result(oof_df):\n","        labels = oof_df[CFG.target_cols].values\n","        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n","        score, scores = get_score(labels, preds)\n","        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n","\n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for fold in range(CFG.n_fold):\n","            if fold in CFG.trn_fold:\n","                if CFG.save_strategy=='epoch':\n","                  _oof_df = train_loop(train, fold)\n","                elif CFG.save_strategy=='step':\n","                  _oof_df = train_loop_steps(train,fold)\n","                elif CFG.save_strategy=='prediction':\n","                  _oof_df = prediction(train,fold)\n","                oof_df = pd.concat([oof_df, _oof_df])\n","\n","                LOGGER.info(f\"========== fold: {fold} result ==========\")\n","                get_result(_oof_df)\n","        oof_df = oof_df.reset_index(drop=True)\n","        LOGGER.info(f\"========== CV ==========\")\n","        get_result(oof_df)\n","        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n","\n","    if CFG.wandb:\n","        wandb.finish()\n","    runtime.unassign()"],"id":"fde1c8af"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a08b7051"},"outputs":[],"source":[],"id":"a08b7051"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeb952c6"},"outputs":[],"source":[],"id":"eeb952c6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"04d58baa"},"outputs":[],"source":[],"id":"04d58baa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3ca03db"},"outputs":[],"source":[],"id":"d3ca03db"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-25T10:25:22.661613","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b9f914607a5409a8b8ae48880dd9f9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1c372e3c17a402d8ee5ceae66bb95ec","IPY_MODEL_b4a03d2780c449bfb8c64e2db55eae0e","IPY_MODEL_f00e69c7ea394a2a984ced089545ea94"],"layout":"IPY_MODEL_7155fcc406e44cb2a06383bcd1ab50b5"}},"e1c372e3c17a402d8ee5ceae66bb95ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6680936f240477282968cc441571547","placeholder":"​","style":"IPY_MODEL_7a502a90c09345d890ce243e99e30006","value":"Downloading (…)okenizer_config.json: 100%"}},"b4a03d2780c449bfb8c64e2db55eae0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78cc60897b794f3a9e60e123b89440b8","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b2c2d8dd943447fa4ee7a61ac98cc99","value":52}},"f00e69c7ea394a2a984ced089545ea94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3841ea0112124872803c7cc236c0d024","placeholder":"​","style":"IPY_MODEL_e2283dd48cdd43fb9d2c7a5362b53e37","value":" 52.0/52.0 [00:00&lt;00:00, 4.43kB/s]"}},"7155fcc406e44cb2a06383bcd1ab50b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6680936f240477282968cc441571547":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a502a90c09345d890ce243e99e30006":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78cc60897b794f3a9e60e123b89440b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b2c2d8dd943447fa4ee7a61ac98cc99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3841ea0112124872803c7cc236c0d024":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2283dd48cdd43fb9d2c7a5362b53e37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38e28e1a606940f4831b41050d52e255":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58017f3d88ba4f448d5edfeb41162c43","IPY_MODEL_7481fa147ff54b0e9d8ee7ec1a59d1f7","IPY_MODEL_30bf6036daa945618ce63a32b3f79a44"],"layout":"IPY_MODEL_d0e47712d58b4f7fb24c1a3c1f4cd649"}},"58017f3d88ba4f448d5edfeb41162c43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55e49252285e477abf0de43614e09710","placeholder":"​","style":"IPY_MODEL_810a62c74337458da500f5928b6d86a6","value":"Downloading (…)lve/main/config.json: 100%"}},"7481fa147ff54b0e9d8ee7ec1a59d1f7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aa0ce88234e453a9b05aa94b6f0bad7","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de890bff24fd4f4e959f253bd113b2bd","value":580}},"30bf6036daa945618ce63a32b3f79a44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a385cfe834b043c9accbd553dabdb2ad","placeholder":"​","style":"IPY_MODEL_7e6e8543da234107bbf61359785ea473","value":" 580/580 [00:00&lt;00:00, 47.5kB/s]"}},"d0e47712d58b4f7fb24c1a3c1f4cd649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55e49252285e477abf0de43614e09710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"810a62c74337458da500f5928b6d86a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2aa0ce88234e453a9b05aa94b6f0bad7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de890bff24fd4f4e959f253bd113b2bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a385cfe834b043c9accbd553dabdb2ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e6e8543da234107bbf61359785ea473":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9897759eb58d4eb5b77bf5b8ef4229e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eef395af45dc413f88a0c2a9e6476a6b","IPY_MODEL_ff971c96c1b24623bf90854040decdb7","IPY_MODEL_8b9e2b2f89fd48868fd36750837e6b4e"],"layout":"IPY_MODEL_243c7fcddf034e7fa4039a7b351b8790"}},"eef395af45dc413f88a0c2a9e6476a6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec7968b3417947358b42cfa0ee6a3f77","placeholder":"​","style":"IPY_MODEL_67a5583841e7407aa2558c231c7b5008","value":"Downloading spm.model: 100%"}},"ff971c96c1b24623bf90854040decdb7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00645056793a4a1d9774032b69b0d039","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a71950c9cdd7402cb25142d0491f7a0a","value":2464616}},"8b9e2b2f89fd48868fd36750837e6b4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da67eec786934ae3addaba8fc038bdea","placeholder":"​","style":"IPY_MODEL_e336808f0d2340e9b21d5974ab553f34","value":" 2.46M/2.46M [00:00&lt;00:00, 46.3MB/s]"}},"243c7fcddf034e7fa4039a7b351b8790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec7968b3417947358b42cfa0ee6a3f77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67a5583841e7407aa2558c231c7b5008":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00645056793a4a1d9774032b69b0d039":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a71950c9cdd7402cb25142d0491f7a0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da67eec786934ae3addaba8fc038bdea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e336808f0d2340e9b21d5974ab553f34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75dd584ed3a54301aafa35e2ef9fd46c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b92ce781f16643ac9a42723f6c35b178","IPY_MODEL_66d74e6f6f2b46f5aed8e9d9fd3e6f08","IPY_MODEL_a4a20218c3bc48e8b2d2a41ea11c7d3e"],"layout":"IPY_MODEL_b10fa6b5fc5e4744a35bf17495a12e86"}},"b92ce781f16643ac9a42723f6c35b178":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6c5781184224cedbb553597041a95b3","placeholder":"​","style":"IPY_MODEL_a91cd45e09444e40b721911db6d3e140","value":"100%"}},"66d74e6f6f2b46f5aed8e9d9fd3e6f08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ff5d792b5b943be9f51bbc973b96c43","max":7165,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20aaaa2762804effa86b830df1e00447","value":7165}},"a4a20218c3bc48e8b2d2a41ea11c7d3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64e284679cde4a7ebe744d9cd20d0fc6","placeholder":"​","style":"IPY_MODEL_332aeacd7b674c9ba898de1e26aa45cb","value":" 7165/7165 [00:19&lt;00:00, 369.26it/s]"}},"b10fa6b5fc5e4744a35bf17495a12e86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6c5781184224cedbb553597041a95b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a91cd45e09444e40b721911db6d3e140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ff5d792b5b943be9f51bbc973b96c43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20aaaa2762804effa86b830df1e00447":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64e284679cde4a7ebe744d9cd20d0fc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"332aeacd7b674c9ba898de1e26aa45cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f6ed8be644d43dca95a468ae0a2ab42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86836e86bf76421782522575dbe34eba","IPY_MODEL_588b8a323d4d437fa4d1fe80be58e1fa","IPY_MODEL_2f481bb64965463ca82149b1c8b85c5e"],"layout":"IPY_MODEL_6df167a89a9e42119aead8dd1e6bda6b"}},"86836e86bf76421782522575dbe34eba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b526c040313b4a1ea67281e79db5684b","placeholder":"​","style":"IPY_MODEL_02d21b88637046c087c9ebaf82ad10b3","value":"Downloading pytorch_model.bin: 100%"}},"588b8a323d4d437fa4d1fe80be58e1fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce8d3c271544bbf993160b8a5a1ed7e","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3879f074f2542c58a4e0a6f1d2efb51","value":873673253}},"2f481bb64965463ca82149b1c8b85c5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b557fa82f1584e63b7d8cc0ba18337d4","placeholder":"​","style":"IPY_MODEL_1764a6d89d144a4c9cebf56a7d1fd1e3","value":" 874M/874M [00:02&lt;00:00, 376MB/s]"}},"6df167a89a9e42119aead8dd1e6bda6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b526c040313b4a1ea67281e79db5684b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02d21b88637046c087c9ebaf82ad10b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ce8d3c271544bbf993160b8a5a1ed7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3879f074f2542c58a4e0a6f1d2efb51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b557fa82f1584e63b7d8cc0ba18337d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1764a6d89d144a4c9cebf56a7d1fd1e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}