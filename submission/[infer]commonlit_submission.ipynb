{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/infer-exp411base-change-weight-by-total-length-560b4614-4862-46c3-8644-f66a6c681a27.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240427/auto/storage/goog4_request&X-Goog-Date=20240427T091651Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2e7609065be5369ed13f965ff824a89c2a9656bfcaead0e4ebff4d489234034034e62c3d324a50fa814d6968ab665c7185bc04daf715153f3a093ce37ee025e7d3d5f2e6e9946ca7e95c99c6bac8f66881f0122b4fa21ac2ee606fb144a9b4eb4ae57f44e317472541db676d7e9b09101cbd650f927d6d30bc26ed1ed9bf07c217e3fdc39ed06acb9c605f6055b7ec6452fb8492a463633bbd69058c1c213c7383ec5bfbcf91ccdf3a2aae8d3657d3eab3f143fab150a8afb25b52abcdb85ca18da86febe8725a2445e4791c38e9d1fae009dab6196bae828097ea964ba56c451ce3f00db900a9944ae2c8b1304c010cc88fc53f28731be22df83b28efe829c7","timestamp":1714209506298}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import warnings\n","import logging\n","import shutil\n","import json\n","import random\n","import re\n","from collections import Counter\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize, treebank\n","from spellchecker import SpellChecker\n","import spacy\n","import lightgbm as lgb\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.optim import Adam, AdamW, SGD\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import KFold, GroupKFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tokenizers\n","\n","import transformers\n","from transformers import (\n","    AutoModel,\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding,\n","    get_linear_schedule_with_warmup,\n","    get_cosine_schedule_with_warmup\n",")\n","from datasets import Dataset, load_dataset, load_from_disk, load_metric, disable_progress_bar\n","from tqdm import tqdm\n","\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# logging setting\n","warnings.simplefilter(\"ignore\")\n","logging.disable(logging.ERROR)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","disable_progress_bar()\n","tqdm.pandas()"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:29.166974Z","iopub.execute_input":"2023-10-06T16:26:29.167347Z","iopub.status.idle":"2023-10-06T16:26:45.577977Z","shell.execute_reply.started":"2023-10-06T16:26:29.16731Z","shell.execute_reply":"2023-10-06T16:26:45.577155Z"},"trusted":true,"id":"zZKRmwk4197H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append('/kaggle/input/sentence-transformers')\n","from sentence_transformers import SentenceTransformer"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:45.579415Z","iopub.execute_input":"2023-10-06T16:26:45.579861Z","iopub.status.idle":"2023-10-06T16:26:45.835927Z","shell.execute_reply.started":"2023-10-06T16:26:45.579827Z","shell.execute_reply":"2023-10-06T16:26:45.835126Z"},"trusted":true,"id":"huh1nHHo197H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","import difflib\n","os.system('python -m pip install --no-index --find-links=/kaggle/input/reability py_readability_metrics')\n","from readability import Readability"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:45.838077Z","iopub.execute_input":"2023-10-06T16:26:45.838528Z","iopub.status.idle":"2023-10-06T16:26:53.442466Z","shell.execute_reply.started":"2023-10-06T16:26:45.838494Z","shell.execute_reply":"2023-10-06T16:26:53.441612Z"},"trusted":true,"id":"ajAGSlPP197I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set random seed\n","def seed_everything(seed: int):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","\n","seed_everything(seed=42)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.44385Z","iopub.execute_input":"2023-10-06T16:26:53.444288Z","iopub.status.idle":"2023-10-06T16:26:53.454775Z","shell.execute_reply.started":"2023-10-06T16:26:53.444256Z","shell.execute_reply":"2023-10-06T16:26:53.453867Z"},"trusted":true,"id":"MXBbNvYq197I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_DIR = './'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.456319Z","iopub.execute_input":"2023-10-06T16:26:53.456826Z","iopub.status.idle":"2023-10-06T16:26:53.462985Z","shell.execute_reply.started":"2023-10-06T16:26:53.45677Z","shell.execute_reply":"2023-10-06T16:26:53.462259Z"},"trusted":true,"id":"h2-AVXIY197I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataload"],"metadata":{"id":"Ax_MDd2n197J"}},{"cell_type":"code","source":["DATA_DIR = \"/kaggle/input/commonlit-evaluate-student-summaries/\"\n","\n","prompts_train = pd.read_csv(DATA_DIR + \"prompts_train.csv\")\n","prompts_test = pd.read_csv(DATA_DIR + \"prompts_test.csv\")\n","summaries_train = pd.read_csv(DATA_DIR + \"summaries_train.csv\")\n","summaries_test = pd.read_csv(DATA_DIR + \"summaries_test.csv\")\n","sample_submission = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n","\n","#summaries_train = summaries_train.head(10) # for dev mode"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.464267Z","iopub.execute_input":"2023-10-06T16:26:53.464702Z","iopub.status.idle":"2023-10-06T16:26:53.585077Z","shell.execute_reply.started":"2023-10-06T16:26:53.464672Z","shell.execute_reply":"2023-10-06T16:26:53.584259Z"},"trusted":true,"id":"u1F9yOEr197J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = summaries_test.merge(prompts_test, on=\"prompt_id\", how=\"left\")"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.586457Z","iopub.execute_input":"2023-10-06T16:26:53.586674Z","iopub.status.idle":"2023-10-06T16:26:53.605315Z","shell.execute_reply.started":"2023-10-06T16:26:53.586646Z","shell.execute_reply":"2023-10-06T16:26:53.604567Z"},"trusted":true,"id":"MBGYKtJB197J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics"],"metadata":{"id":"ikQxhywT197K"}},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","def compute_mcrmse(eval_pred):\n","    \"\"\"\n","    Calculates mean columnwise root mean squared error\n","    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n","    \"\"\"\n","    preds, labels = eval_pred\n","\n","    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n","    mcrmse = np.mean(col_rmse)\n","\n","    return {\n","        \"content_rmse\": col_rmse[0],\n","        \"wording_rmse\": col_rmse[1],\n","        \"mcrmse\": mcrmse,\n","    }\n","\n","def compt_score(content_true, content_pred, wording_true, wording_pred):\n","    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n","    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n","\n","    return (content_score + wording_score)/2"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.606656Z","iopub.execute_input":"2023-10-06T16:26:53.606919Z","iopub.status.idle":"2023-10-06T16:26:53.615315Z","shell.execute_reply.started":"2023-10-06T16:26:53.606889Z","shell.execute_reply":"2023-10-06T16:26:53.614543Z"},"trusted":true,"id":"RPLbr5es197K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Inference"],"metadata":{"id":"u916TkiR197L"}},{"cell_type":"code","source":["transformer_preds_dict = {}"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.619295Z","iopub.execute_input":"2023-10-06T16:26:53.619477Z","iopub.status.idle":"2023-10-06T16:26:53.625713Z","shell.execute_reply.started":"2023-10-06T16:26:53.619446Z","shell.execute_reply":"2023-10-06T16:26:53.624946Z"},"trusted":true,"id":"tPTPChYx197L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"jDxfrLVu197L"}},{"cell_type":"code","source":["class CFG:\n","    model_name=\"debertav3base\"\n","    learning_rate=1.5e-5\n","    weight_decay=0.02\n","    hidden_dropout_prob=0.005\n","    attention_probs_dropout_prob=0.005\n","    num_train_epochs=5\n","    n_splits=4\n","    batch_size=8\n","    random_seed=42\n","    save_steps=100\n","    max_length=512\n","\n","class CFG2:\n","    num_workers=4\n","    path=[\"/kaggle/input/exp023-content/exp023_content/\",\"/kaggle/input/exp023-wording/exp023_wording/\"]\n","    config_path=path[0]+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    make_feat=False\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","\n","class CFG3:\n","    num_workers=4\n","    path='/kaggle/input/exp008/exp008/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","class CFG4:\n","    num_workers=4\n","    path='/kaggle/input/exp012/exp012/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","\n","class CFG5:\n","    num_workers=4\n","    path='/kaggle/input/exp015/exp015/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-xlarge\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","class CFG6:\n","    num_workers=4\n","    path='/kaggle/input/exp009/exp009/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-base\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","class CFG7:\n","    num_workers=4\n","    path='/kaggle/input/exp011/exp011/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v2-xlarge\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","class CFG8:\n","    num_workers=4\n","    path='/kaggle/input/exp038/exp038/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-base\"\n","    gradient_checkpointing=False\n","    batch_size=64\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.007     #0.005\n","    attention_probs_dropout_prob=0.007     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","class CFG10:\n","    num_workers=4\n","    path=[\"/kaggle/input/exp041-content/exp042_content/\",\"/kaggle/input/exp042-wording/exp042_wording/\"]\n","    config_path=path[0]+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    make_feat=False\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='MeanPooling'\n","    is_max_len=False\n","\n","class CFG11:\n","    num_workers=4\n","    path='/kaggle/input/exp046/exp046/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='LSTMPooling'\n","    hidden_size=512\n","    is_max_len=False\n","\n","class CFG12:\n","    num_workers=4\n","    path='/kaggle/input/exp047/exp047/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='LSTMPooling'\n","    hidden_size=512\n","    is_max_len=False\n","\n","class CFG13:\n","    num_workers=4\n","    path='/kaggle/input/exp048/exp048/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=4\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=4\n","    is_max_len=False\n","\n","class CFG14:\n","    num_workers=4\n","    path='/kaggle/input/exp049-2/exp049/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=64\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='WeightedLayerPooling'\n","    is_max_len=False\n","\n","class CFG15:\n","    num_workers=4\n","    path='/kaggle/input/exp050-2/exp050/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=64\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=14\n","    is_max_len=False\n","\n","\n","class CFG16:\n","    num_workers=4\n","    path=[\"/kaggle/input/exp052-wording/exp052_wording/\"]\n","    config_path=path[0]+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=8\n","    target_cols=['wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    make_feat=False\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=14\n","    is_max_len=False\n","\n","class CFG19:\n","    num_workers=4\n","    path='/kaggle/input/exp064/exp064/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=1\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=14\n","    is_max_len=True\n","    max_len=896\n","\n","class CFG20:\n","    num_workers=4\n","    path='/kaggle/input/exp068/exp068/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=1\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=10\n","    is_max_len=True\n","    max_len=1392\n","\n","class CFG21:\n","    num_workers=4\n","    path='/kaggle/input/exp085-full-train/exp085_full_train/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=1\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=10\n","    is_max_len=True\n","    max_len=1024\n","\n","class CFG22:\n","    num_workers=4\n","    path='/kaggle/input/exp108-full-train/exp108_full_train/'\n","    config_path=path+'config.pth'\n","    model=\"microsoft/deberta-v3-large\"\n","    gradient_checkpointing=False\n","    batch_size=1\n","    target_cols=['content', 'wording']\n","    seed=42\n","    n_fold=4\n","    trn_fold=[0,1,2,3]\n","    weight=1\n","    hidden_dropout_prob=0.    #0.005\n","    attention_probs_dropout_prob=0.     #0.005\n","    pooling='ConcatPooling'\n","    n_layers=10\n","    is_max_len=True\n","    max_len=1800\n","\n","\n","CFG_list=[CFG14, CFG15, CFG22] # ,CFG16] # ,CFG21]"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.627181Z","iopub.execute_input":"2023-10-06T16:26:53.627777Z","iopub.status.idle":"2023-10-06T16:26:53.654183Z","shell.execute_reply.started":"2023-10-06T16:26:53.627731Z","shell.execute_reply":"2023-10-06T16:26:53.653341Z"},"trusted":true,"id":"72b7vhDg197L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Deberta Regressor"],"metadata":{"id":"4gGt4zIA197M"}},{"cell_type":"code","source":["class ContentScoreRegressor:\n","    def __init__(self,\n","                model_name: str,\n","                model_dir: str,\n","                target: str,\n","                hidden_dropout_prob: float,\n","                attention_probs_dropout_prob: float,\n","                max_length: int,\n","                ):\n","        self.inputs = [\"prompt_text\", \"prompt_title\", \"prompt_question\", \"text\"]\n","        self.input_col = \"input\"\n","\n","        self.text_cols = [self.input_col]\n","        self.target = target\n","        self.target_cols = [target]\n","\n","        self.model_name = model_name\n","        self.model_dir = model_dir\n","        self.max_length = max_length\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(f\"/kaggle/input/{model_name}\")\n","        self.model_config = AutoConfig.from_pretrained(f\"/kaggle/input/{model_name}\")\n","\n","        self.model_config.update({\n","            \"hidden_dropout_prob\": hidden_dropout_prob,\n","            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n","            \"num_labels\": 1,\n","            \"problem_type\": \"regression\",\n","        })\n","\n","        seed_everything(seed=42)\n","\n","        self.data_collator = DataCollatorWithPadding(\n","            tokenizer=self.tokenizer\n","        )\n","\n","\n","    def tokenize_function(self, examples: pd.DataFrame):\n","        labels = [examples[self.target]]\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return {\n","            **tokenized,\n","            \"labels\": labels,\n","        }\n","\n","    def tokenize_function_test(self, examples: pd.DataFrame):\n","        tokenized = self.tokenizer(examples[self.input_col],\n","                         padding=False,\n","                         truncation=True,\n","                         max_length=self.max_length)\n","        return tokenized\n","\n","    def train(self,\n","            fold: int,\n","            train_df: pd.DataFrame,\n","            valid_df: pd.DataFrame,\n","            batch_size: int,\n","            learning_rate: float,\n","            weight_decay: float,\n","            num_train_epochs: float,\n","            save_steps: int,\n","        ) -> None:\n","        \"\"\"fine-tuning\"\"\"\n","\n","        sep = self.tokenizer.sep_token\n","        train_df[self.input_col] = (\n","                    train_df[\"prompt_title\"] + sep\n","                    + train_df[\"prompt_question\"] + sep\n","                    + train_df[\"text\"]\n","                  )\n","\n","        valid_df[self.input_col] = (\n","                    valid_df[\"prompt_title\"] + sep\n","                    + valid_df[\"prompt_question\"] + sep\n","                    + valid_df[\"text\"]\n","                  )\n","\n","        train_df = train_df[[self.input_col] + self.target_cols]\n","        valid_df = valid_df[[self.input_col] + self.target_cols]\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(\n","            f\"/kaggle/input/{self.model_name}\",\n","            config=self.model_config\n","        )\n","\n","        train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n","        val_dataset = Dataset.from_pandas(valid_df, preserve_index=False)\n","\n","        train_tokenized_datasets = train_dataset.map(self.tokenize_function, batched=False)\n","        val_tokenized_datasets = val_dataset.map(self.tokenize_function, batched=False)\n","\n","        # eg. \"bert/fold_0/\"\n","        model_fold_dir = os.path.join(self.model_dir, str(fold))\n","\n","        training_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            load_best_model_at_end=True, # select best model\n","            learning_rate=learning_rate,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=8,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=weight_decay,\n","            report_to='none',\n","            greater_is_better=False,\n","            save_strategy=\"steps\",\n","            evaluation_strategy=\"steps\",\n","            eval_steps=save_steps,\n","            save_steps=save_steps,\n","            metric_for_best_model=\"rmse\",\n","            save_total_limit=1\n","        )\n","\n","        trainer = Trainer(\n","            model=model_content,\n","            args=training_args,\n","            train_dataset=train_tokenized_datasets,\n","            eval_dataset=val_tokenized_datasets,\n","            tokenizer=self.tokenizer,\n","            compute_metrics=compute_metrics,\n","            data_collator=self.data_collator\n","        )\n","\n","        trainer.train()\n","\n","        model_content.save_pretrained(self.model_dir)\n","        self.tokenizer.save_pretrained(self.model_dir)\n","\n","\n","    def predict(self,\n","                test_df: pd.DataFrame,\n","                fold: int,\n","               ):\n","        \"\"\"predict content score\"\"\"\n","\n","        sep = self.tokenizer.sep_token\n","        in_text = (\n","                    test_df[\"prompt_title\"] + sep\n","                    + test_df[\"prompt_question\"] + sep\n","                    + test_df[\"text\"]\n","                  )\n","        test_df[self.input_col] = in_text\n","\n","        test_ = test_df[[self.input_col]]\n","\n","        test_dataset = Dataset.from_pandas(test_, preserve_index=False)\n","        test_tokenized_dataset = test_dataset.map(self.tokenize_function_test, batched=False)\n","\n","        model_content = AutoModelForSequenceClassification.from_pretrained(f\"{self.model_dir}\")\n","        model_content.eval()\n","\n","        # e.g. \"bert/fold_0/\"\n","        model_fold_dir = os.path.join(self.model_dir, str(fold))\n","\n","        test_args = TrainingArguments(\n","            output_dir=model_fold_dir,\n","            do_train = False,\n","            do_predict = True,\n","            per_device_eval_batch_size = 4,\n","            dataloader_drop_last = False,\n","        )\n","\n","        # init trainer\n","        infer_content = Trainer(\n","                      model = model_content,\n","                      tokenizer=self.tokenizer,\n","                      data_collator=self.data_collator,\n","                      args = test_args)\n","\n","        preds = infer_content.predict(test_tokenized_dataset)[0]\n","\n","        return preds"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.655657Z","iopub.execute_input":"2023-10-06T16:26:53.655885Z","iopub.status.idle":"2023-10-06T16:26:53.673384Z","shell.execute_reply.started":"2023-10-06T16:26:53.655857Z","shell.execute_reply":"2023-10-06T16:26:53.67251Z"},"trusted":true,"id":"Adc1iXgG197M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text):\n","    if cfg.is_max_len:\n","        inputs = cfg.tokenizer.encode_plus(\n","        text,\n","        return_tensors=None,\n","        add_special_tokens=True,\n","        max_length=cfg.max_len,\n","        pad_to_max_length=True,\n","        truncation=True\n","        )\n","    else:\n","        inputs = cfg.tokenizer.encode_plus(\n","            text,\n","            return_tensors=None,\n","            add_special_tokens=True,\n","    #         max_length=cfg.max_len,\n","    #         pad_to_max_length=True,\n","    #         truncation=True\n","        )\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TestDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['full_text'].values\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item])\n","        return inputs\n","\n","\n","#ref:https://github.com/shu421/kagglib/blob/main/nlp/model.py\n","# ====================================================\n","# Model\n","# ====================================================\n","\n","def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","\n","    for parameter in module.parameters():\n","        parameter.requires_grad = False\n","# =====================================================\n","# Pooling\n","# =====================================================\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","\n","\n","class AttentionPooling(nn.Module):\n","    \"\"\"\n","    Usage:\n","        self.pool = AttentionPooling(self.config.hidden_size)\n","    \"\"\"\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_dim, in_dim),\n","            nn.LayerNorm(in_dim),\n","            nn.GELU(),\n","            nn.Linear(in_dim, 1),\n","        )\n","\n","        self._init_weights(self.attention)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        w = self.attention(last_hidden_state).float()\n","        w[attention_mask == 0] = float(\"-inf\")\n","        w = torch.softmax(w, 1)\n","        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n","        return attention_embeddings\n","\n","\n","\n","class WeightedLayerPooling(nn.Module):\n","    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n","        super(WeightedLayerPooling, self).__init__()\n","        self.layer_start = layer_start\n","        self.num_hidden_layers = num_hidden_layers\n","        self.layer_weights = layer_weights if layer_weights is not None \\\n","            else nn.Parameter(\n","            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n","        )\n","\n","    def forward(self, all_hidden_states):\n","        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n","        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n","        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n","        return weighted_average[:, 0]\n","\n","class LSTMPooling(nn.Module):\n","    def __init__(self, num_layers, hidden_size, hiddendim_lstm, dropout_rate, is_lstm=True):\n","        super(LSTMPooling, self).__init__()\n","        self.num_hidden_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.hiddendim_lstm = hiddendim_lstm\n","\n","        if is_lstm:\n","            self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n","        else:\n","            self.lstm = nn.GRU(self.hidden_size, self.hiddendim_lstm, batch_first=True, bidirectional=True)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, all_hidden_states):\n","        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n","                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n","        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n","        out, _ = self.lstm(hidden_states, None)\n","        out = self.dropout(out[:, -1, :])\n","        return out\n","\n","class ConcatPooling(nn.Module):\n","    def __init__(self, n_layers=4):\n","        super(ConcatPooling, self, ).__init__()\n","\n","        self.n_layers = n_layers\n","\n","    def forward(self, all_hidden_states):\n","        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n","        concatenate_pooling = concatenate_pooling[:, 0]\n","        return concatenate_pooling\n","\n","# GeM\n","class GeMText(nn.Module):\n","    def __init__(self, dim=1, cfg=None, p=3, eps=1e-6):\n","        super(GeMText, self).__init__()\n","        self.dim = dim\n","        self.p = Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","        self.feat_mult = 1\n","        # x seeems last hidden state\n","\n","    def forward(self, x, attention_mask):\n","        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n","        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n","        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n","        ret = ret.pow(1 / self.p)\n","        return ret\n","\n","# ===========================================\n","# custom Model\n","# ===========================================\n","\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","            self.config.hidden_dropout = 0.\n","            self.config.hidden_dropout_prob = 0.\n","            self.config.attention_dropout = 0.\n","            self.config.attention_probs_dropout_prob = 0.\n","            LOGGER.info(self.config)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        if self.cfg.gradient_checkpointing:\n","            self.model.gradient_checkpointing_enable()\n","        if cfg.pooling =='MeanPooling':\n","            self.pool=MeanPooling()\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","        elif cfg.pooling =='LSTMPooling':\n","            self.pool =  LSTMPooling(self.config.num_hidden_layers,\n","                                       self.config.hidden_size,\n","                                       self.cfg.hidden_size,\n","                                       0.1,\n","                                       is_lstm=True\n","                           )\n","            self.fc = nn.Linear(self.cfg.hidden_size, 2)\n","        elif cfg.pooling == \"GeM\":\n","            self.pool = GeMText()\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","        elif cfg.pooling=='ConcatPooling':\n","            self.pool = ConcatPooling(n_layers=cfg.n_layers)\n","            self.fc = nn.Linear(cfg.n_layers*self.config.hidden_size, 2)\n","        elif cfg.pooling=='WeightedLayerPooling':\n","            self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","\n","\n","        self._init_weights(self.fc)\n","\n","\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        if self.cfg.pooling=='MeanPooling':\n","            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling in ['GRUPooling', 'LSTMPooling']:\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling=='GeM':\n","            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling == 'ConcatPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling == 'WeightedLayerPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(feature)\n","        return output\n","\n","class CustomModel_1(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","            self.config.hidden_dropout = 0.\n","            self.config.hidden_dropout_prob = 0.\n","            self.config.attention_dropout = 0.\n","            self.config.attention_probs_dropout_prob = 0.\n","            LOGGER.info(self.config)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        if self.cfg.gradient_checkpointing:\n","            self.model.gradient_checkpointing_enable()\n","        if cfg.pooling =='MeanPooling':\n","            self.pool=MeanPooling()\n","            self.fc = nn.Linear(self.config.hidden_size, 1)\n","        elif cfg.pooling =='LSTMPooling':\n","            self.pool =  LSTMPooling(self.config.num_hidden_layers,\n","                                       self.config.hidden_size,\n","                                       self.cfg.hidden_size,\n","                                       0.1,\n","                                       is_lstm=True\n","                           )\n","            self.fc = nn.Linear(self.cfg.hidden_size, 1)\n","        elif cfg.pooling == \"GeM\":\n","            self.pool = GeMText()\n","            self.fc = nn.Linear(self.config.hidden_size, 1)\n","        elif cfg.pooling=='ConcatPooling':\n","            self.pool = ConcatPooling(n_layers=cfg.n_layers)\n","            self.fc = nn.Linear(cfg.n_layers*self.config.hidden_size, 1)\n","        elif cfg.pooling=='WeightedLayerPooling':\n","            self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n","            self.fc = nn.Linear(self.config.hidden_size, 1)\n","\n","\n","        self._init_weights(self.fc)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        if self.cfg.pooling=='MeanPooling':\n","            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling in ['GRUPooling', 'LSTMPooling']:\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling=='GeM':\n","            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","        elif self.cfg.pooling == 'ConcatPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        elif self.cfg.pooling == 'WeightedLayerPooling':\n","            all_hidden_states = torch.stack(outputs[1])\n","            feature = self.pool(all_hidden_states)\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(feature)\n","        return output\n","\n","# initialize layer\n","def reinit_bert(model):\n","    \"\"\"_summary_\n","\n","    Args:\n","        model (AutoModel): _description_\n","\n","    Returns:\n","        model (AutoModel): _description_\n","\n","    Usage:\n","        model = reinit_bert(model)\n","    \"\"\"\n","    for layer in model.model.encoder.layer[-1:]:\n","        for module in layer.modules():\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","    return model\n","\n","# ====================================================\n","# inference\n","# ====================================================\n","def inference_fn(test_loader, model, device):\n","    preds = []\n","    model.eval()\n","    model.to(device)\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        preds.append(y_preds.to('cpu').numpy())\n","    predictions = np.concatenate(preds)\n","    return predictions\n"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.674801Z","iopub.execute_input":"2023-10-06T16:26:53.675011Z","iopub.status.idle":"2023-10-06T16:26:53.717611Z","shell.execute_reply.started":"2023-10-06T16:26:53.674985Z","shell.execute_reply":"2023-10-06T16:26:53.71681Z"},"trusted":true,"id":"Ac1iTJZn197N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def takai_inference(df, CFG_):\n","    # ====================================================\n","    # tokenizer\n","    # ====================================================\n","    if isinstance(CFG_.path, list):\n","        CFG_.tokenizer = AutoTokenizer.from_pretrained(CFG_.path[0]+'tokenizer/')\n","    else:\n","        CFG_.tokenizer = AutoTokenizer.from_pretrained(CFG_.path+'tokenizer/')\n","    # ====================================================\n","    # Data Loading\n","    # ====================================================\n","\n","    submission = pd.read_csv('../input/commonlit-evaluate-student-summaries/sample_submission.csv')\n","\n","\n","    #     # \"text\"列の長さを計算して新しい列\"length\"に追加\n","    #     test['length'] = test['text'].apply(len)\n","    #     # \"text\"列の先頭に\"length\"列の値を結合\n","    #     test['text'] = test['length'].astype(str) + '[SEP]' + test['prompt_question'] + '[SEP]' + test['prompt_title'] + '[SEP]' + test['text']\n","    #CFG_list=[CFG2,CFG4,CFG10,CFG11,CFG12,CFG13,CFG14,CFG15,CFG16]\n","    if CFG_==CFG2 or CFG_==CFG10 or CFG_==CFG2 or CFG_==CFG10 :\n","        df['full_text'] =   df['prompt_title'] + '[SEP]' + df['prompt_question'] + ' [SEP] ' + df['text']\n","    elif CFG_==CFG4 or CFG_==CFG14 or CFG_==CFG15 or CFG_==CFG16:\n","        df['full_text'] = df['prompt_question']+' [SEP] ' + df['text']\n","    elif CFG_==CFG11 or CFG_==CFG12 or CFG_==CFG13:\n","        df['full_text'] =  df['prompt_question'] + '[SEP]' + df['prompt_title'] + ' [SEP] ' + df['text']\n","    elif CFG_==CFG22 or CFG_==CFG21:\n","        df['full_text'] = df['text']+' [SEP] '+df['prompt_text']\n","\n","\n","    #df['text'] = df['prompt_question']+' [SEP] ' + df['text']\n","    print(f\"df.shape: {df.shape}\")\n","    display(df.head())\n","    print(f\"submission.shape: {submission.shape}\")\n","    display(submission.head())\n","\n","    # sort by length to speed up inference\n","    df['tokenize_length'] = [len(CFG_.tokenizer(text)['input_ids']) for text in df['full_text'].values]\n","    df = df.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n","    display(df.head())\n","    test_dataset = TestDataset(CFG_, df)\n","    test_loader = DataLoader(test_dataset,\n","                             batch_size=CFG_.batch_size,\n","                             shuffle=False,\n","                             collate_fn=DataCollatorWithPadding(tokenizer=CFG_.tokenizer, padding='longest'),\n","                             num_workers=CFG_.num_workers, pin_memory=True, drop_last=False)\n","    predictions = []\n","    if  CFG_==CFG22 or CFG_==CFG21:\n","        model = CustomModel(CFG_, config_path=CFG_.config_path, pretrained=False)\n","        state = torch.load(CFG_.path+f\"{CFG_.model.replace('/', '-')}_full_train.pth\",\n","                           map_location=torch.device('cpu'))\n","        model.load_state_dict(state['model'],strict=False)\n","        prediction = inference_fn(test_loader, model, device)\n","\n","        predictions.append(prediction)\n","        del model, state, prediction; gc.collect()\n","        torch.cuda.empty_cache()\n","\n","    else:\n","        for fold in CFG_.trn_fold:\n","            if isinstance(CFG_.path,list):\n","                for n,path in enumerate(CFG_.path):\n","\n","                    model = CustomModel_1(CFG_, config_path=CFG_.config_path, pretrained=False)\n","                    state = torch.load(path+f\"{CFG_.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                                       map_location=torch.device('cpu'))\n","                    model.load_state_dict(state['model'],strict=False)\n","                    prediction_one = inference_fn(test_loader, model, device)\n","                    if n==0:\n","                        prediction=prediction_one\n","                    else:\n","                        prediction=np.concatenate((prediction,prediction_one),axis=1)\n","\n","            else:\n","\n","                model = CustomModel(CFG_, config_path=CFG_.config_path, pretrained=False)\n","                state = torch.load(CFG_.path+f\"{CFG_.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                                   map_location=torch.device('cpu'))\n","                model.load_state_dict(state['model'],strict=False)\n","                prediction = inference_fn(test_loader, model, device)\n","\n","            predictions.append(prediction)\n","            del model, state, prediction; gc.collect()\n","            torch.cuda.empty_cache()\n","\n","    predictions = np.mean(predictions, axis=0)\n","    torch.cuda.empty_cache()\n","\n","    student_ids = df[\"student_id\"].values\n","    return predictions, student_ids"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.719182Z","iopub.execute_input":"2023-10-06T16:26:53.719679Z","iopub.status.idle":"2023-10-06T16:26:53.736451Z","shell.execute_reply.started":"2023-10-06T16:26:53.719648Z","shell.execute_reply":"2023-10-06T16:26:53.735801Z"},"trusted":true,"id":"XOPBKaPV197O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CFG_listの数だけ推論\n","# transformers_preds_dictにモデル名をキーにして推論結果を保存\n","for idx, CFG_ in enumerate(CFG_list):\n","    model_path = CFG_.path\n","    if isinstance(model_path, list):\n","        model_path = model_path[0]\n","    model_name = \"takai_\" + Path(model_path).name\n","    predictions, student_ids = takai_inference(test.copy(), CFG_)\n","    original_sorted_predictions = pd.DataFrame(predictions, index=student_ids).reindex(test[\"student_id\"]).values\n","    del predictions\n","    gc.collect()\n","    transformer_preds_dict[model_name] = original_sorted_predictions"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:26:53.737806Z","iopub.execute_input":"2023-10-06T16:26:53.738453Z","iopub.status.idle":"2023-10-06T16:30:08.57946Z","shell.execute_reply.started":"2023-10-06T16:26:53.738422Z","shell.execute_reply":"2023-10-06T16:30:08.577478Z"},"trusted":true,"id":"aRpnBVXb197P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.580733Z","iopub.status.idle":"2023-10-06T16:30:08.581719Z","shell.execute_reply.started":"2023-10-06T16:30:08.581483Z","shell.execute_reply":"2023-10-06T16:30:08.581508Z"},"trusted":true,"id":"E-EGpisx197Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NLPDataset(Dataset):\n","    def __init__(self, df, prompt_df, tokenizer, meta_feature_cols, is_train=True, max_len=128):\n","        self.student_ids = df[\"student_id\"].tolist()\n","        self.text = df[\"text\"].tolist()\n","        # self.content = df[\"content\"].tolist()\n","        # self.wording = df[\"wording\"].tolist()\n","        self.prompt_question = prompt_df.set_index(\"prompt_id\")[\"prompt_question\"].reindex(df[\"prompt_id\"]).tolist()\n","        self.prompt_title  = prompt_df.set_index(\"prompt_id\")[\"prompt_title\"].reindex(df[\"prompt_id\"]).tolist()\n","        self.prompt_text = prompt_df.set_index(\"prompt_id\")[\"prompt_text\"].reindex(df[\"prompt_id\"]).tolist()\n","        self.meta_features = df[meta_feature_cols].values\n","        self.tokenizer = tokenizer\n","\n","        self.is_train = is_train\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.student_ids)\n","\n","    def __getitem__(self, ix):\n","        prompt_question = str(self.prompt_question[ix])\n","        prompt_text = str(self.prompt_text[ix])\n","        text = str(self.text[ix])\n","\n","        sentence = prompt_question + \" [SEP] \" + text  # + \" [SEP] \" + prompt_text\n","        meta_feature = self.meta_features[ix]\n","\n","        text_inputs = self.tokenizer(\n","            sentence,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_token_type_ids=True\n","        )\n","\n","        data = {\n","            \"input_ids\": torch.tensor(text_inputs[\"input_ids\"], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(text_inputs[\"attention_mask\"], dtype=torch.long),\n","            \"token_type_ids\": torch.tensor(text_inputs[\"token_type_ids\"], dtype=torch.long),\n","            \"meta_features\": torch.tensor(meta_feature, dtype=torch.float),\n","            # \"targets\": torch.tensor([target1, target2], dtype=torch.float),\n","        }\n","\n","        return data"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.583114Z","iopub.status.idle":"2023-10-06T16:30:08.583822Z","shell.execute_reply.started":"2023-10-06T16:30:08.583579Z","shell.execute_reply":"2023-10-06T16:30:08.583602Z"},"trusted":true,"id":"7cHUk35f197Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NLPModel046(nn.Module):\n","    def __init__(self, pretrain_path, mlm_model_path=None):\n","        super(NLPModel046, self).__init__()\n","        model_config = transformers.AutoConfig.from_pretrained(pretrain_path)\n","        model_config.attention_probs_dropout_prob=0.0\n","        model_config.hidden_dropout_prob=0.0\n","        model_config.output_hidden_states=True\n","        self.encoder = transformers.AutoModel.from_pretrained(\n","            pretrain_path,\n","            config=model_config,\n","        )\n","        self.fc = nn.Linear(1024, 2)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, meta_features):\n","        last_hidden_state = self.encoder(input_ids=input_ids,\n","                                 attention_mask=attention_mask,\n","                                 token_type_ids = token_type_ids,\n","                                 return_dict=False)[1][-1]\n","\n","        cls_token = last_hidden_state[:, 0, :]\n","        output = self.fc(cls_token)\n","        return output\n","\n","\n","class NLPModel139(nn.Module):\n","    def __init__(self, pretrain_path, meta_feature_cols, mlm_model_path=None):\n","        super(NLPModel139, self).__init__()\n","        model_config = transformers.AutoConfig.from_pretrained(pretrain_path)\n","        model_config.attention_probs_dropout_prob=0.0\n","        model_config.hidden_dropout_prob=0.0\n","        model_config.output_hidden_states=True\n","        self.encoder = transformers.AutoModel.from_pretrained(\n","            pretrain_path,\n","            config=model_config,\n","        )\n","        self.fc = nn.Linear(1024 + len(meta_feature_cols), 2)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, meta_features):\n","        last_hidden_state = self.encoder(input_ids=input_ids,\n","                                 attention_mask=attention_mask,\n","                                 token_type_ids = token_type_ids,\n","                                 return_dict=False)[1][-1]\n","\n","        cls_token = last_hidden_state[:, 0, :]\n","        feature = torch.cat([cls_token, meta_features], dim=1)\n","        output = self.fc(feature)\n","        return output\n","\n","\n","class NLPModel068(nn.Module):\n","    def __init__(self, pretrain_path, mlm_model_path=None):\n","        super(NLPModel068, self).__init__()\n","        model_config = transformers.AutoConfig.from_pretrained(pretrain_path)\n","        model_config.attention_probs_dropout_prob=0.0\n","        model_config.hidden_dropout_prob=0.0\n","        model_config.output_hidden_states=True\n","        self.encoder = transformers.AutoModel.from_pretrained(\n","            pretrain_path,\n","            config=model_config,\n","        )\n","        self.fc = nn.Linear(1536, 2)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, meta_features):\n","\n","        last_hidden_state = self.encoder(input_ids=input_ids,\n","                                 attention_mask=attention_mask,\n","                                 token_type_ids = token_type_ids,\n","                                 return_dict=False)[1][-1]\n","\n","        cls_token = last_hidden_state[:, 0, :]\n","        feature = cls_token\n","        output = self.fc(feature)\n","        return output"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.58514Z","iopub.status.idle":"2023-10-06T16:30:08.585839Z","shell.execute_reply.started":"2023-10-06T16:30:08.585598Z","shell.execute_reply":"2023-10-06T16:30:08.585621Z"},"trusted":true,"id":"M1AXgGgZ197Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EmbDataset(Dataset):\n","    def __init__(self, df, prompt_df, tokenizer, max_len=128):\n","        self.student_ids = df[\"student_id\"].tolist()\n","        self.text = df[\"text\"].tolist()\n","        self.prompt_question = prompt_df.set_index(\"prompt_id\")[\"prompt_question\"].reindex(df[\"prompt_id\"]).tolist()\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.student_ids)\n","\n","    def __getitem__(self, ix):\n","        prompt_question = str(self.prompt_question[ix])\n","        text = str(self.text[ix])\n","        sentence = prompt_question + \" [SEP] \" + text # + \" [SEP] \" + prompt_text\n","        text_inputs = self.tokenizer(\n","            sentence,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_token_type_ids=True\n","        )\n","        data = {\n","            \"input_ids\": torch.tensor(text_inputs[\"input_ids\"], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(text_inputs[\"attention_mask\"], dtype=torch.long),\n","            \"token_type_ids\": torch.tensor(text_inputs[\"token_type_ids\"], dtype=torch.long),\n","        }\n","        return data\n","\n","class EmbModel(nn.Module):\n","    def __init__(self, model_path):\n","        super(EmbModel, self).__init__()\n","        self.encoder = transformers.AutoModel.from_pretrained(\n","            model_path,\n","            attention_probs_dropout_prob=0.0, hidden_dropout_prob=0.0,\n","            output_hidden_states=True\n","        )\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        last_hidden_state = self.encoder(input_ids=input_ids,\n","                                 attention_mask=attention_mask,\n","                                 token_type_ids = token_type_ids,\n","                                 return_dict=False)[1][-1]\n","        cls_token = last_hidden_state[:, 0, :]\n","        return cls_token\n","\n","def get_embedding(model, data_loader, device):\n","    # get batch data loop\n","    epoch_loss = 0\n","    epoch_data_num = len(data_loader.dataset)\n","    embs_list = []\n","    bar = tqdm(enumerate(data_loader), total=len(data_loader))\n","    model.eval()\n","    for iter_i, batch in bar:\n","        batch = {k : v.to(device) for k, v in batch.items()}\n","        text_inputs = {\n","            \"input_ids\": batch[\"input_ids\"],\n","            \"attention_mask\": batch[\"attention_mask\"],\n","            \"token_type_ids\": batch[\"token_type_ids\"],\n","        }\n","        with torch.no_grad():\n","            embs = model(**text_inputs)\n","        embs_list.append(embs.detach().cpu().numpy())\n","    embeddings = np.concatenate(embs_list, axis=0)\n","    return embeddings\n","\n","\n","def get_transformer_embeddings(df, prompt_df, pretrain_path, model_prefix=\"\"):\n","    set_seed(CFG.seed)\n","    device = torch.device(\n","        f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","    )\n","    model = EmbModel(pretrain_path)\n","    model.to(device)\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(pretrain_path)\n","    scaler = amp.GradScaler(enabled=CFG.use_fp16)\n","    dataset = EmbDataset(df, prompt_df, tokenizer, max_len=CFG.max_length)\n","    data_loader = DataLoader(\n","        dataset,\n","        batch_size=32,\n","        num_workers=2,\n","        shuffle=False,\n","    )\n","    embeddings = get_embedding(\n","        model, data_loader, device\n","    )\n","    return embeddings"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.587149Z","iopub.status.idle":"2023-10-06T16:30:08.587846Z","shell.execute_reply.started":"2023-10-06T16:30:08.587606Z","shell.execute_reply":"2023-10-06T16:30:08.587628Z"},"trusted":true,"id":"tAkxnwV-197Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_run(test_df, p_test_df, model_path, pretrain_path, meta_feature_cols, model_cls):\n","\n","    set_seed(CFG.seed)\n","    device = torch.device(\n","        \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","    )\n","\n","    ###################################\n","    # Model and Tokenizer\n","    ###################################\n","    model = model_cls(pretrain_path, meta_feature_cols)\n","    model.to(device)\n","\n","    trained_state_dict = torch.load(\n","        model_path,\n","        map_location=lambda storage,loc: storage\n","    )[\"model_state_dict\"]\n","\n","    own_state = model.state_dict()\n","    for name, param in trained_state_dict.items():\n","        if name not in own_state:\n","             continue\n","        else:\n","            param = param.data\n","        own_state[name].copy_(param)\n","\n","    model.eval()\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(pretrain_path)\n","\n","    # token sort for fast inference\n","    # 一旦このパターンしかないのでここで固定。場合によっては設定で切り替える\n","    full_text = test_df[\"prompt_question\"] + \" [SEP] \" + test_df[\"text\"]\n","    test_df['tokenize_length'] = full_text.map(lambda text: len(tokenizer(text)['input_ids']))\n","    sorted_test_df = test_df.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n","\n","\n","    ###################################\n","    # Make data\n","    ###################################\n","\n","    test_dataset = NLPDataset(\n","        sorted_test_df, p_test_df, tokenizer, meta_feature_cols, max_len=CFG.max_length, is_train=False\n","    )\n","\n","    # data loader\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=CFG.batch_size,\n","        num_workers=CFG.num_workers,\n","        shuffle=False,\n","        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n","    )\n","\n","    ###############################\n","    # predict per batch\n","    ###############################\n","    epoch_data_num = len(test_loader.dataset)\n","    pred_list = []\n","\n","    for iter_i, input_data in enumerate(test_loader):\n","        # input\n","        input_data = {k : v.to(device) for k, v in input_data.items()}\n","        text_inputs = {\n","            \"input_ids\": input_data[\"input_ids\"],\n","            \"attention_mask\": input_data[\"attention_mask\"],\n","            \"token_type_ids\": input_data[\"token_type_ids\"],\n","            \"meta_features\": input_data[\"meta_features\"],\n","        }\n","\n","        with torch.no_grad():\n","            preds = model(**text_inputs)\n","        pred_list.append(preds.detach().cpu().numpy())\n","\n","    sorted_test_preds = np.concatenate(pred_list, axis=0)\n","    sorted_preds_df = pd.DataFrame(sorted_test_preds, columns=[\"content\", \"wording\"])\n","    sorted_preds_df[\"student_id\"] = sorted_test_df[\"student_id\"].values\n","\n","    # 元のtest_dfの並び順に戻す\n","    test_preds = sorted_preds_df.set_index(\"student_id\").reindex(test_df[\"student_id\"])[[\"content\", \"wording\"]].values\n","    return test_preds"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.589196Z","iopub.status.idle":"2023-10-06T16:30:08.589895Z","shell.execute_reply.started":"2023-10-06T16:30:08.589648Z","shell.execute_reply":"2023-10-06T16:30:08.58967Z"},"trusted":true,"id":"CCE9mbmi197R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CFG:\n","    seed = 2023\n","    num_workers = 4\n","    batch_size = 16\n","    max_length = 512\n","    n_folds = 4\n","    use_fp16 = False"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.591192Z","iopub.status.idle":"2023-10-06T16:30:08.591899Z","shell.execute_reply.started":"2023-10-06T16:30:08.591653Z","shell.execute_reply":"2023-10-06T16:30:08.591675Z"},"trusted":true,"id":"I4bRsVwB197R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {\n","#     \"exp034\": {\n","#         \"path_list\": [\n","#             \"/kaggle/input/commonlit2-034/fold0last-checkpoint.bin\",\n","#             \"/kaggle/input/commonlit2-034/fold1last-checkpoint.bin\",\n","#             \"/kaggle/input/commonlit2-034/fold2last-checkpoint.bin\",\n","#             \"/kaggle/input/commonlit2-034/fold3last-checkpoint.bin\",\n","#         ],\n","#         \"pretrain_path\": \"/kaggle/input/transformers-models/deberta-v3-large\",\n","#         \"model_module\": NLPModel034,\n","#     },\n","#     \"exp112\": {\n","#         \"path_list\": [\n","#             \"/kaggle/input/commonlit2-112/all_last-checkpoint.bin\",\n","#         ],\n","#         \"pretrain_path\": \"/kaggle/input/transformers-models/deberta-v3-large\",\n","#         \"model_module\": NLPModel046,  # same with 046 model\n","#         # \"ensemble_weight\": 0.4\n","#     },\n","    \"exp139\": {\n","        \"path_list\": [\n","            \"/kaggle/input/commonlit2-139/all_last-checkpoint.bin\",\n","        ],\n","        \"pretrain_path\": \"/kaggle/input/transformers-models/deberta-v3-large\",\n","        \"model_module\": NLPModel139,\n","        # \"ensemble_weight\": 0.28\n","    },\n","    \"exp068\": {\n","        \"path_list\": [\n","            \"/kaggle/input/commonlit2-068/all_last-checkpoint.bin\",\n","        ],\n","        \"pretrain_path\": \"/kaggle/input/transformers-models/deberta-v2-xlarge\",\n","        \"model_module\": NLPModel068,\n","        # \"ensemble_weight\": 0.32\n","    },\n","}"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.593227Z","iopub.status.idle":"2023-10-06T16:30:08.593948Z","shell.execute_reply.started":"2023-10-06T16:30:08.593706Z","shell.execute_reply":"2023-10-06T16:30:08.593728Z"},"trusted":true,"id":"5cExgVN6197R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get embedding　as meta feature of transformer\n","emb_pretrain_path = \"/kaggle/input/transformers-models/deberta-v3-large\"\n","embeddings = get_transformer_embeddings(test, prompts_test, emb_pretrain_path)\n","emb_df = pd.DataFrame(embeddings, columns=[f\"emb_{i}\" for i in range(embeddings.shape[1])])\n","meta_feature_cols = emb_df.columns.tolist()\n","test = pd.concat([test, emb_df], axis=1)\n","del emb_df\n","gc.collect()\n","for c in tqdm(meta_feature_cols):\n","    test[c] = test[c] - test.groupby(\"prompt_id\")[c].transform(\"mean\")\n","\n","# preds\n","for model_name, model_info in models.items():\n","    pretrain_path = model_info[\"pretrain_path\"]\n","    model_path_list = model_info[\"path_list\"]\n","    model = model_info[\"model_module\"]\n","    test_preds = np.zeros((len(test), 2))\n","    for model_ix, model_path in enumerate(model_path_list):\n","        print(f\"Model {model_ix}\")\n","        each_test_preds = predict_run(test, prompts_test, model_path, pretrain_path, meta_feature_cols, model)\n","        test_preds = each_test_preds\n","        break\n","    # TODO: token lengthでのsort処理及び復元処理\n","    transformer_preds_dict[model_name] = test_preds\n","\n","test.drop(meta_feature_cols, axis=1, inplace=True)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.595235Z","iopub.status.idle":"2023-10-06T16:30:08.595955Z","shell.execute_reply.started":"2023-10-06T16:30:08.59571Z","shell.execute_reply":"2023-10-06T16:30:08.595733Z"},"trusted":true,"id":"BXPDoLOV197R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ensemble_weights = {\n","    # 'exp112': 0.1,\n","    'exp139': 0.15,\n","    'exp068': 0.15,\n","    'takai_exp049': 0.3,\n","    'takai_exp050': 0.4,\n","    # 'takai_exp069': 0.5\n","}\n","\n","assert(np.sum(list(ensemble_weights.values())) == 1)\n","\n","transformer_ensemble_preds = np.zeros((len(test), 2))\n","\n","for exp_name, weight in ensemble_weights.items():\n","    pred = transformer_preds_dict[exp_name]\n","    transformer_ensemble_preds += pred * weight"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.597251Z","iopub.status.idle":"2023-10-06T16:30:08.597948Z","shell.execute_reply.started":"2023-10-06T16:30:08.597705Z","shell.execute_reply":"2023-10-06T16:30:08.597728Z"},"trusted":true,"id":"rHMtH91U197R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test[\"ensemble_pred_content\"] = transformer_ensemble_preds[:, 0]\n","test[\"ensemble_pred_wording\"] = transformer_ensemble_preds[:, 1]"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.599276Z","iopub.status.idle":"2023-10-06T16:30:08.599987Z","shell.execute_reply.started":"2023-10-06T16:30:08.599746Z","shell.execute_reply":"2023-10-06T16:30:08.599784Z"},"trusted":true,"id":"iAL8mfzz197R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(transformer_preds_dict.keys())"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.601281Z","iopub.status.idle":"2023-10-06T16:30:08.602005Z","shell.execute_reply.started":"2023-10-06T16:30:08.601777Z","shell.execute_reply":"2023-10-06T16:30:08.6018Z"},"trusted":true,"id":"SQaw72Ba197S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Engineering for lgbm"],"metadata":{"id":"daE-HRs8197S"}},{"cell_type":"code","source":["def nakama_fb_predict_feature(df):\n","\n","    class CFG:\n","        num_workers=4\n","        path=\"../input/fb3-deberta-v3-base-baseline-train/\"\n","        config_path=path+'config.pth'\n","        model=\"microsoft/deberta-v3-base\"\n","        gradient_checkpointing=False\n","        batch_size=24\n","        target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","        seed=42\n","        n_fold=4\n","        trn_fold=[0, 1, 2, 3]\n","\n","    def prepare_input(cfg, text):\n","        inputs = cfg.tokenizer.encode_plus(\n","            text,\n","            return_tensors=None,\n","            add_special_tokens=True,\n","            #max_length=CFG.max_len,\n","            #pad_to_max_length=True,\n","            #truncation=True\n","        )\n","        for k, v in inputs.items():\n","            inputs[k] = torch.tensor(v, dtype=torch.long)\n","        return inputs\n","\n","    class TestDataset(Dataset):\n","        def __init__(self, cfg, df):\n","            self.cfg = cfg\n","            self.texts = df['text'].values\n","\n","        def __len__(self):\n","            return len(self.texts)\n","\n","        def __getitem__(self, item):\n","            inputs = prepare_input(self.cfg, self.texts[item])\n","            return inputs\n","\n","\n","    class MeanPooling(nn.Module):\n","        def __init__(self):\n","            super(MeanPooling, self).__init__()\n","\n","        def forward(self, last_hidden_state, attention_mask):\n","            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","            sum_mask = input_mask_expanded.sum(1)\n","            sum_mask = torch.clamp(sum_mask, min=1e-9)\n","            mean_embeddings = sum_embeddings / sum_mask\n","            return mean_embeddings\n","\n","    class CustomFBModel(nn.Module):\n","        def __init__(self, cfg, config_path=None, pretrained=False):\n","            super().__init__()\n","            self.cfg = cfg\n","            if config_path is None:\n","                self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","                self.config.hidden_dropout = 0.\n","                self.config.hidden_dropout_prob = 0.\n","                self.config.attention_dropout = 0.\n","                self.config.attention_probs_dropout_prob = 0.\n","                # LOGGER.info(self.config)\n","            else:\n","                self.config = torch.load(config_path)\n","            if pretrained:\n","                self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","            else:\n","                self.model = AutoModel.from_config(self.config)\n","            if self.cfg.gradient_checkpointing:\n","                self.model.gradient_checkpointing_enable()\n","            self.pool = MeanPooling()\n","            self.fc = nn.Linear(self.config.hidden_size, 6)\n","            self._init_weights(self.fc)\n","\n","        def _init_weights(self, module):\n","            if isinstance(module, nn.Linear):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.Embedding):\n","                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","                if module.padding_idx is not None:\n","                    module.weight.data[module.padding_idx].zero_()\n","            elif isinstance(module, nn.LayerNorm):\n","                module.bias.data.zero_()\n","                module.weight.data.fill_(1.0)\n","\n","        def feature(self, inputs):\n","            outputs = self.model(**inputs)\n","            last_hidden_states = outputs[0]\n","            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","            return feature\n","\n","        def forward(self, inputs):\n","            feature = self.feature(inputs)\n","            output = self.fc(feature)\n","            return output\n","\n","        def inference_fn(test_loader, model, device):\n","            preds = []\n","            model.eval()\n","            model.to(device)\n","            tk0 = tqdm(test_loader, total=len(test_loader))\n","            for inputs in tk0:\n","                for k, v in inputs.items():\n","                    inputs[k] = v.to(device)\n","                with torch.no_grad():\n","                    y_preds = model(inputs)\n","                preds.append(y_preds.to('cpu').numpy())\n","            predictions = np.concatenate(preds)\n","            return predictions\n","\n","    CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n","\n","    test_dataset = TestDataset(CFG, df)\n","    test_loader = DataLoader(test_dataset,\n","                             batch_size=CFG.batch_size,\n","                             shuffle=False,\n","                             collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n","                             num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","    predictions = []\n","    for fold in CFG.trn_fold:\n","        model = CustomFBModel(CFG, config_path=CFG.config_path, pretrained=False)\n","        state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n","                           map_location=torch.device('cpu'))\n","        model.load_state_dict(state['model'])\n","        prediction = inference_fn(test_loader, model, device)\n","        predictions.append(prediction)\n","        del model, state, prediction; gc.collect()\n","        torch.cuda.empty_cache()\n","    predictions = np.mean(predictions, axis=0)\n","\n","    df[CFG.target_cols] = predictions\n","\n","    return df"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.603333Z","iopub.status.idle":"2023-10-06T16:30:08.604054Z","shell.execute_reply.started":"2023-10-06T16:30:08.603828Z","shell.execute_reply":"2023-10-06T16:30:08.603851Z"},"trusted":true,"id":"KKOIGHuj197S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spell_checker = SpellChecker()\n","\n","def calc_spell_miss(text):\n","    wordlist=text.split()\n","    amount_miss = len(list(spell_checker.unknown(wordlist)))\n","    return amount_miss\n","\n","def ngrams(token, n):\n","    # Use the zip function to help us generate n-grams\n","    # Concatentate the tokens into ngrams and return\n","    ngrams = zip(*[token[i:] for i in range(n)])\n","    return [\" \".join(ngram) for ngram in ngrams]\n","\n","def ngram_co_occurrence(row, n: int):\n","    # Tokenize the original text and summary into words\n","    original_tokens = row['prompt_tokens']\n","    summary_tokens = row['summary_tokens']\n","\n","    # Generate n-grams for the original text and summary\n","    original_ngrams = set(ngrams(original_tokens, n))\n","    summary_ngrams = set(ngrams(summary_tokens, n))\n","\n","    # Calculate the number of common n-grams\n","    common_ngrams = original_ngrams.intersection(summary_ngrams)\n","\n","    # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n","    # original_ngram_freq = Counter(ngrams(original_words, n))\n","    # summary_ngram_freq = Counter(ngrams(summary_words, n))\n","    # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n","\n","    return len(common_ngrams)\n","\n","\n","def word_overlap_count(row):\n","    prompt_words = row['prompt_tokens']\n","    summary_words = row['summary_tokens']\n","    return len(set(prompt_words).intersection(set(summary_words)))\n","\n","\n","def quotes_count(row):\n","    summary = row['text']\n","    text = row['prompt_text']\n","    quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","    if len(quotes_from_summary)>0:\n","        return [quote in text for quote in quotes_from_summary].count(True)\n","    else:\n","        return 0\n","\n","\n","def clean_text(text):\n","    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","\n","\n","def jaccard_similarity(text1, text2):\n","    set1 = set(clean_text(text1).split())\n","    set2 = set(clean_text(text2).split())\n","    intersection = len(set1.intersection(set2))\n","    union = len(set1.union(set2))\n","    return intersection / union\n","\n","def calc_jaccard_score(df):\n","    prompt_questions = df['prompt_question'].tolist()\n","    prompt_texts = df['prompt_text'].tolist()\n","    question_jaccard_scores = []\n","    text_jaccard_scores = []\n","    texts = df['text'].tolist()\n","    for ix in tqdm(range(len(df))):\n","        text = texts[ix]\n","        prompt_question = prompt_questions[ix]\n","        prompt_text = prompt_texts[ix]\n","        score1 = jaccard_similarity(text, prompt_question)\n","        score2 = jaccard_similarity(text, prompt_text)\n","        question_jaccard_scores.append(score1)\n","        text_jaccard_scores.append(score2)\n","    return question_jaccard_scores, text_jaccard_scores\n","\n","\n","def calc_sentence_embedding(df, prompt_df):\n","\n","    device = torch.device(\n","        f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","    )\n","\n","    model = SentenceTransformer('/kaggle/input/transformers-models/sentence-transformers_all-mpnet-base-v2')\n","    model.to(device)\n","\n","    # ----------------------------------\n","    # prompt questions\n","    # ----------------------------------\n","    prompt_questions = prompt_df[\"prompt_question\"].tolist()\n","    prompt_texts = prompt_df[\"prompt_text\"].tolist()\n","    prompt_questions_embeddings = {}\n","    _embeddings = model.encode(prompt_questions, show_progress_bar=False)\n","    for k, v in zip(prompt_df[\"prompt_id\"].tolist(), _embeddings):\n","        prompt_questions_embeddings[k] = v\n","\n","    # ----------------------------------\n","    # prompt texts\n","    # ----------------------------------\n","    prompt_texts_embeddings = {}\n","    _embeddings = model.encode(prompt_texts, show_progress_bar=False)\n","    for k, v in zip(prompt_df[\"prompt_id\"].tolist(), _embeddings):\n","        prompt_texts_embeddings[k] = v\n","\n","    # ----------------------------------\n","    # prompt texts split version\n","    # ----------------------------------\n","    prompt_texts_split_embeddings = {}\n","    prompt_sentences = {}\n","    prompt_ids = prompt_df[\"prompt_id\"].tolist()\n","    for prompt_ix, prompt_id in enumerate(prompt_ids):\n","        # sentences = re.split('[.\\n\\r]+', prompt_df[\"prompt_text\"].iloc[prompt_ix])\n","        sentences = sent_tokenize(prompt_df[\"prompt_text\"].iloc[prompt_ix])\n","        # sentences = [s for s in sentences if len(s.split()) > 3]\n","        _embeddings = model.encode(sentences, show_progress_bar=False)\n","        prompt_texts_split_embeddings[prompt_id] = _embeddings\n","        prompt_sentences[prompt_id] = sentences\n","\n","    # ----------------------------------\n","    # summary texts\n","    # ----------------------------------\n","    # create text embeddings of df\n","    batch_size = 32\n","    texts = df['text'].tolist()\n","    texts_embeddings = []\n","    for batch_ix in tqdm(range(len(df) // batch_size + 1)):\n","        batch_from = batch_ix * batch_size\n","        batch_to = (batch_ix + 1) * batch_size\n","        batch_to = min(batch_to, len(df))\n","        batch_texts = texts[batch_from:batch_to]\n","        batch_embeddings = model.encode(batch_texts)\n","        texts_embeddings.append(batch_embeddings)\n","    texts_embeddings = np.concatenate(texts_embeddings, axis=0)\n","\n","    # ----------------------------------\n","    # summary texts split version\n","    # ----------------------------------\n","    texts = df['text'].tolist()\n","    texts_split_embeddings = []\n","    for row_text in tqdm(texts):\n","        sentences = sent_tokenize(row_text)\n","        _embeddings = model.encode(sentences, show_progress_bar=False)\n","        texts_split_embeddings.append(_embeddings)\n","\n","    return prompt_questions_embeddings, prompt_texts_embeddings, prompt_texts_split_embeddings, prompt_sentences, texts_embeddings, texts_split_embeddings\n","\n","\n","def create_meta_feature(df, prompt_df):\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/transformers-models/deberta-v3-large\")\n","\n","    df[\"summary_word_len\"] = df[\"text\"].apply(lambda x: len(x.split()))\n","\n","    df[\"summary_length\"] = df[\"text\"].apply(\n","        lambda x: len(tokenizer.encode(x))\n","    )\n","    df[\"prompt_length\"] = df[\"prompt_text\"].apply(\n","        lambda x: len(tokenizer.encode(x))\n","    )\n","    df[\"summary_tokens\"] = df[\"text\"].apply(\n","        lambda x: tokenizer.convert_ids_to_tokens(\n","            tokenizer.encode(x),\n","            skip_special_tokens=True\n","        )\n","    )\n","    df[\"prompt_tokens\"] = df[\"prompt_text\"].apply(\n","        lambda x: tokenizer.convert_ids_to_tokens(\n","            tokenizer.encode(x),\n","            skip_special_tokens=True\n","        )\n","    )\n","    # df['length_ratio'] = df['summary_length'] / df['prompt_length']\n","    df[\"spell_err_num\"] = df[\"text\"].map(calc_spell_miss)\n","\n","    df['bigram_overlap_count'] = df.apply(\n","        ngram_co_occurrence,args=(2,), axis=1\n","    )\n","    df['trigram_overlap_count'] = df.apply(\n","        ngram_co_occurrence, args=(3,), axis=1\n","    )\n","    df['bigram_overlap_ratio'] = df['bigram_overlap_count'] / (df['summary_length'] - 1)\n","    df['trigram_overlap_ratio'] = df['trigram_overlap_count'] / (df['summary_length'] - 1)\n","\n","    df[\"summary_length\"] = df[\"summary_length\"] - df[\"prompt_id\"].map(df.groupby(\"prompt_id\")[\"summary_length\"].mean())\n","\n","    df['word_overlap_count'] = df.apply(word_overlap_count, axis=1)\n","    df['quotes_count'] = df.apply(quotes_count, axis=1)\n","\n","    question_jaccard_scores, text_jaccard_scores = calc_jaccard_score(df)\n","    df['question_jaccard'] = question_jaccard_scores\n","    df['text_jaccard'] = text_jaccard_scores\n","\n","    (\n","        prompt_questions_embeddings,\n","        prompt_texts_embeddings,\n","        prompt_texts_split_embeddings,\n","        prompt_sentences,\n","        texts_embeddings,\n","        texts_split_embeddings\n","    ) = calc_sentence_embedding(df, prompt_df)\n","\n","    prompt_ids = df['prompt_id'].tolist()\n","    questions_cossim_feature = []\n","    texts_cossim_feature = []\n","    for ix in tqdm(range(len(df))):\n","        prompt_id = prompt_ids[ix]\n","        prompt_questions_cosine_sim = cosine_similarity(texts_embeddings[ix].reshape(1, -1), prompt_questions_embeddings[prompt_id].reshape(1, -1))[0][0]\n","        prompt_texts_cosine_sim = cosine_similarity(texts_embeddings[ix].reshape(1, -1), prompt_texts_embeddings[prompt_id].reshape(1, -1))[0][0]\n","        questions_cossim_feature.append(prompt_questions_cosine_sim)\n","        texts_cossim_feature.append(prompt_texts_cosine_sim)\n","\n","    df['questions_cossim'] = questions_cossim_feature\n","    df['texts_cossim'] = texts_cossim_feature\n","\n","    emb_cols = [f\"emb_{i}\" for i in range(texts_embeddings.shape[1])]\n","    emb_df = pd.DataFrame(texts_embeddings, columns=emb_cols)\n","    emb_df[\"prompt_id\"] = df[\"prompt_id\"].values\n","    prompt_mean = emb_df.groupby(\"prompt_id\").mean()\n","    emb_arr = emb_df[emb_cols].values\n","    idx2prompt = emb_df[\"prompt_id\"].values\n","    mean_cossim = emb_df.index.map(lambda idx: np.sum(emb_arr[idx] * prompt_mean.loc[idx2prompt[idx]]))\n","    df[\"sentence_embedding_prompt_mean_cossim\"] = mean_cossim\n","    mean_diff = emb_df.index.map(lambda idx: np.mean(np.square(emb_arr[idx] - prompt_mean.loc[idx2prompt[idx]])))\n","    df[\"sentence_embedding_prompt_mean_diff\"] = mean_diff\n","\n","    # knn\n","    for p_id in df[\"prompt_id\"].unique():\n","        p_id_indices = df[df[\"prompt_id\"] == p_id].index\n","        knn = NearestNeighbors(n_neighbors=min(101, len(p_id_indices)))\n","        knn.fit(texts_embeddings[p_id_indices])\n","        neighbors_dists, neighbors_indices = knn.kneighbors(texts_embeddings[p_id_indices])\n","        content_arr = df[\"ensemble_pred_content\"].values[p_id_indices]\n","        wording_arr = df[\"ensemble_pred_wording\"].values[p_id_indices]\n","        for thresh in [100]:\n","            content_mean = content_arr[neighbors_indices[:, 1:thresh+1]].mean(axis=1)\n","            wording_mean = wording_arr[neighbors_indices[:, 1:thresh+1]].mean(axis=1)\n","            df.loc[p_id_indices, f\"knn{thresh}_pred_content_mean\"] = content_mean\n","            df.loc[p_id_indices, f\"knn{thresh}_pred_wording_mean\"] = wording_mean\n","            df.loc[p_id_indices, f\"knn{thresh}_dists_mean\"] = neighbors_dists[:, 1:thresh+1].mean(axis=1)\n","\n","    for thresh in [100]:\n","        df[f\"diff_knn{thresh}_pred_content_mean\"] = df[\"ensemble_pred_content\"] - df[f\"knn{thresh}_pred_content_mean\"]\n","        df[f\"diff_knn{thresh}_pred_wording_mean\"] = df[\"ensemble_pred_wording\"] - df[f\"knn{thresh}_pred_wording_mean\"]\n","\n","\n","    for p_id in df[\"prompt_id\"].unique():\n","        p_id_indices = df[df[\"prompt_id\"] == p_id].index\n","        tfidf = TfidfVectorizer(lowercase=False)\n","        tfidf.fit(df.loc[p_id_indices, \"text\"])\n","        tfidf_train = tfidf.transform(df.loc[p_id_indices, \"text\"])\n","        df.loc[p_id_indices, \"tfidf_mean\"] = tfidf_train.toarray().mean(axis=1)\n","        df.loc[p_id_indices, \"tfidf_nonzero_mean\"] = np.nanmean(np.where(0, np.nan, tfidf_train.toarray()), axis=1)\n","\n","        tfidf_texts_embeddings = tfidf_train.toarray()\n","        emb_cols = [f\"emb_{i}\" for i in range(tfidf_texts_embeddings.shape[1])]\n","        emb_df = pd.DataFrame(tfidf_texts_embeddings, columns=emb_cols)\n","        emb_mean = emb_df.mean(axis=0).values\n","        scalar = (np.linalg.norm(tfidf_texts_embeddings, axis=1) * np.linalg.norm(emb_mean))\n","        cossim = ((tfidf_texts_embeddings * emb_mean).sum(axis=1)) / scalar\n","        df.loc[p_id_indices, \"tfidf_emb_prompt_mean_cossim\"] = cossim\n","\n","        bow = CountVectorizer()\n","        bow.fit(df.loc[p_id_indices, \"text\"])\n","        bow_train = bow.transform(df.loc[p_id_indices, \"text\"])\n","        tfidf_texts_embeddings = bow_train.toarray()\n","        emb_cols = [f\"emb_{i}\" for i in range(tfidf_texts_embeddings.shape[1])]\n","        emb_df = pd.DataFrame(tfidf_texts_embeddings, columns=emb_cols)\n","        emb_mean = emb_df.mean(axis=0).values\n","        scalar = (np.linalg.norm(tfidf_texts_embeddings, axis=1) * np.linalg.norm(emb_mean))\n","        cossim = ((tfidf_texts_embeddings * emb_mean).sum(axis=1)) / scalar\n","        df.loc[p_id_indices, \"bow_emb_prompt_mean_cossim\"] = cossim\n","\n","\n","\n","    # ----------------------------------\n","    # split text cossim agg features\n","    # ----------------------------------\n","    # prompt text split version\n","    prompt_ids = df['prompt_id'].tolist()\n","    texts_cossim_feature = []\n","    cosine_sim_features = []\n","    for ix in tqdm(range(len(df))):\n","        prompt_id = prompt_ids[ix]\n","        prompt_texts_cosine_sim = cosine_similarity(texts_embeddings[ix].reshape(1, -1), prompt_texts_split_embeddings[prompt_id])\n","        cosine_sim_features.append({\n","            # \"prompt_cossim_mean\": prompt_texts_cosine_sim[0].mean(),\n","            # \"prompt_cossim_max\": prompt_texts_cosine_sim[0].max(),\n","            \"prompt_cossim_std\": prompt_texts_cosine_sim[0].std(),\n","        })\n","    cosine_sim_features_df = pd.DataFrame(cosine_sim_features)\n","    df = pd.concat([df, cosine_sim_features_df], axis=1)\n","\n","    # summary texts split version\n","    prompt_ids = df['prompt_id'].tolist()\n","    texts_cossim_feature = []\n","    cosine_sim_features = []\n","    for ix in tqdm(range(len(df))):\n","        prompt_id = prompt_ids[ix]\n","        prompt_texts_cosine_sim = cosine_similarity(prompt_texts_embeddings[prompt_id].reshape(1, -1), texts_split_embeddings[ix])\n","        cosine_sim_features.append({\n","            # \"per_text_sentence_prompt_cossim_mean\": prompt_texts_cosine_sim[0].mean(),\n","            # \"per_text_sentence_prompt_cossim_max\": prompt_texts_cosine_sim[0].max(),\n","            \"per_text_sentence_prompt_cossim_std\": prompt_texts_cosine_sim[0].std(),\n","        })\n","    cosine_sim_features_df = pd.DataFrame(cosine_sim_features)\n","    df = pd.concat([df, cosine_sim_features_df], axis=1)\n","\n","    del texts_embeddings, prompt_texts_embeddings, prompt_sentences, texts_split_embeddings, prompt_texts_split_embeddings\n","    gc.collect()\n","\n","    readability_scores = []\n","    for text in tqdm(df[\"text\"].tolist()):\n","        score = {}\n","        try:\n","            r = Readability(text)\n","        except:\n","            readability_scores.append(score)\n","            continue\n","        try:\n","            score[\"flesch_kincaid\"] = r.flesch_kincaid().score\n","        except:\n","            score[\"flesch_kincaid\"] = np.nan\n","            pass\n","        try:\n","            score[\"flesch\"] = r.flesch().score\n","        except:\n","            score[\"flesch\"] = np.nan\n","            pass\n","        try:\n","            score[\"gunning_fog\"] = r.gunning_fog().score\n","        except:\n","            score[\"gunning_fog\"] = np.nan\n","            pass\n","        try:\n","            score[\"coleman_liau\"] = r.coleman_liau().score\n","        except:\n","            score[\"coleman_liau\"] = np.nan\n","            pass\n","        try:\n","            score[\"dale_chall\"] = r.dale_chall().score\n","        except:\n","            score[\"dale_chall\"] = np.nan\n","            pass\n","        try:\n","            score[\"ari\"] = r.ari().score\n","        except:\n","            score[\"ari\"] = np.nan\n","            pass\n","        try:\n","            score[\"linsear_write\"] = r.linsear_write().score\n","        except:\n","            score[\"linsear_write\"] = np.nan\n","            pass\n","        try:\n","            score[\"spache\"] = r.spache().score\n","        except:\n","            score[\"spache\"] = np.nan\n","            pass\n","        readability_scores.append(score)\n","\n","    readability_df = pd.DataFrame(readability_scores)\n","    df = pd.concat([df, readability_df], axis=1)\n","\n","    def count_words_within_quotes(text):\n","        # 正規表現を使って二重引用符で囲まれたテキストを抽出\n","        quoted_texts = re.findall(r'\"([^\"]+)\"', text)\n","        # 引用符が2つ以上存在しない場合は0を返す\n","        if len(quoted_texts) == 0:\n","            return 0\n","\n","        # 引用符で囲まれたテキストの中で最も単語数の多いものを求める\n","        max_word_count = max(len(quoted_text) for quoted_text in quoted_texts)\n","        return max_word_count\n","\n","    df[\"mean_num_words\"] = df[\"text\"].map(lambda x: np.mean([len(e.split()) for e in x.split('.')]))\n","    df['refer_cnt'] = df[\"text\"].map(count_words_within_quotes)\n","\n","    return df"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.605542Z","iopub.status.idle":"2023-10-06T16:30:08.606262Z","shell.execute_reply.started":"2023-10-06T16:30:08.606036Z","shell.execute_reply":"2023-10-06T16:30:08.60606Z"},"trusted":true,"id":"WGagPvOG197S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = create_meta_feature(test, prompts_test)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.607537Z","iopub.status.idle":"2023-10-06T16:30:08.608238Z","shell.execute_reply.started":"2023-10-06T16:30:08.608016Z","shell.execute_reply":"2023-10-06T16:30:08.608039Z"},"trusted":true,"id":"wj_k-_eQ197T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = nakama_fb_predict_feature(test)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.609519Z","iopub.status.idle":"2023-10-06T16:30:08.610207Z","shell.execute_reply.started":"2023-10-06T16:30:08.60998Z","shell.execute_reply":"2023-10-06T16:30:08.610003Z"},"trusted":true,"id":"AWpBN1xM197X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LightGBM inference"],"metadata":{"id":"f-asnT5u197Y"}},{"cell_type":"code","source":["LGBM_DIRS = [\n","    # model_name, lgbm_dir, weight_original, weight_long_prompt, seed_num, transformer_name\n","    (\"exp405\", \"/kaggle/input/commonlit2-405-lgb\", 0.24, 0.27, 10, \"exp139\"),\n","    (\"exp406\", \"/kaggle/input/commonlit2-406-lgb\", 0.11, 0.14, 10, \"exp068\"),\n","    (\"exp407\", \"/kaggle/input/commonlit2-407-lgb\", 0.16, 0.19, 10, \"takai_exp049\"),\n","    (\"exp408\", \"/kaggle/input/commonlit2-408-lgb\", 0.12, 0.15, 10, \"takai_exp050\"),\n","    (\"exp409\", \"/kaggle/input/commonlit2-409-lgb\", 0.20, 0.23, 10, \"takai_exp108_full_train\"),\n","]\n","\n","\n","for model_name, preds in transformer_preds_dict.items():\n","    test[f\"{model_name}_pred_content\"] = preds[:, 0]\n","    test[f\"{model_name}_pred_wording\"] = preds[:, 1]\n","\n","test[f\"takai_exp069_pred_content\"] = test[f\"takai_exp108_full_train_pred_content\"]\n","test[f\"takai_exp069_pred_wording\"] = test[f\"takai_exp108_full_train_pred_wording\"]\n","\n","\n","\n","lgbm_test_preds = np.zeros((len(test), 2))\n","lgbm_test_preds_for_long_prompt = np.zeros((len(test), 2))\n","for model_name, lgbm_dir, weight, weight_long_prompt, seed_num, transformer_model_name in LGBM_DIRS:\n","\n","    lgb_preds_list = []\n","    for seed_ix in tqdm(range(seed_num)):\n","        for fold_ix in range(CFG.n_folds):\n","\n","#             ensemble_preds = np.zeros((len(test_df), 2))\n","#             for model_name, preds in transformer_preds_dict.items():\n","#                 test_df[f\"{model_name}_pred_content\"] = preds[:, 0]\n","#                 test_df[f\"{model_name}_pred_wording\"] = preds[:, 1]\n","#                 ensemble_preds[:, 0] += test_df[f\"{model_name}_pred_content\"].values * models[model_name][\"ensemble_weight\"]\n","#                 ensemble_preds[:, 1] += test_df[f\"{model_name}_pred_wording\"].values * models[model_name][\"ensemble_weight\"]\n","\n","#             test_df[\"ensemble_pred_content\"] = ensemble_preds[:, 0]\n","#             test_df[\"ensemble_pred_wording\"] = ensemble_preds[:, 1]\n","            test[\"ensemble_pred_content\"] = transformer_preds_dict[transformer_model_name][:, 0]\n","            test[\"ensemble_pred_wording\"] = transformer_preds_dict[transformer_model_name][:, 1]\n","\n","            with open(Path(lgbm_dir) / f\"use_feats_fold{fold_ix}.txt\") as f:\n","                use_feats = f.read().splitlines()\n","\n","            lgb_preds = np.zeros((len(test), 2))\n","            for target_ix, target_col in enumerate([\"content\", \"wording\"]):\n","                lgb_model = lgb.Booster(model_file=Path(lgbm_dir) / f\"{target_col}_fold{fold_ix}_{seed_ix}.txt\")\n","                lgb_preds[:, target_ix] = lgb_model.predict(test[use_feats])\n","\n","            lgb_preds_list.append(lgb_preds)\n","\n","    _test_preds = np.mean(lgb_preds_list, axis=0)\n","    lgbm_test_preds += _test_preds * weight\n","    lgbm_test_preds_for_long_prompt += _test_preds * weight_long_prompt"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.611522Z","iopub.status.idle":"2023-10-06T16:30:08.612206Z","shell.execute_reply.started":"2023-10-06T16:30:08.611984Z","shell.execute_reply":"2023-10-06T16:30:08.612006Z"},"trusted":true,"id":"X8kXWxac197Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Add Deberta(with prompt text) predict"],"metadata":{"id":"-L6-jlWy197Y"}},{"cell_type":"code","source":["weight_exp069 = 0.17\n","weight_exp069_for_long_prompt = 0.02\n","assert(round(sum([w for _, _, w, _, _, _ in LGBM_DIRS]) + weight_exp069, 1) == 1.0)\n","assert(round(sum([w for _, _, _, w, _, _ in LGBM_DIRS]) + weight_exp069_for_long_prompt, 1) == 1.0)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.613723Z","iopub.status.idle":"2023-10-06T16:30:08.614436Z","shell.execute_reply.started":"2023-10-06T16:30:08.6142Z","shell.execute_reply":"2023-10-06T16:30:08.614223Z"},"trusted":true,"id":"mdYX8kmd197Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_preds = lgbm_test_preds + transformer_preds_dict[\"takai_exp108_full_train\"] * weight_exp069\n","test_preds_for_long_prompt = lgbm_test_preds_for_long_prompt + transformer_preds_dict[\"takai_exp108_full_train\"] * weight_exp069_for_long_prompt\n","\n","del transformer_preds_dict\n","gc.collect()"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.61573Z","iopub.status.idle":"2023-10-06T16:30:08.616448Z","shell.execute_reply.started":"2023-10-06T16:30:08.61622Z","shell.execute_reply":"2023-10-06T16:30:08.616244Z"},"trusted":true,"id":"OKW0ImMY197Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/transformers-models/deberta-v3-large\")\n","test['prompt_text_tokenize_length'] = test[\"prompt_text\"].map(lambda text: len(tokenizer(text)['input_ids']))\n","test['text_tokenize_length'] = test[\"text\"].map(lambda text: len(tokenizer(text)['input_ids']))\n","test_tokenize_length = test['prompt_text_tokenize_length'] + test['text_tokenize_length']\n","\n","LONG_THRESH1 = 1800\n","LONG_THRESH2 = 3000\n","weights_long = (test_tokenize_length - LONG_THRESH1) / (LONG_THRESH2 - LONG_THRESH1)\n","weights_long[weights_long < 0] = 0\n","weights_long[weights_long > 1] = 1\n","\n","weights_base = np.ones(len(test)) - weights_long"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.617735Z","iopub.status.idle":"2023-10-06T16:30:08.618469Z","shell.execute_reply.started":"2023-10-06T16:30:08.618222Z","shell.execute_reply":"2023-10-06T16:30:08.618245Z"},"trusted":true,"id":"8bHiaGrJ197Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_df = pd.read_csv(\"/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv\")\n","sub_df[\"content\"] = test_preds[:, 0] * weights_base + test_preds_for_long_prompt[:, 0] * weights_long\n","sub_df[\"wording\"] = test_preds[:, 1] * weights_base + test_preds_for_long_prompt[:, 1] * weights_long\n","\n","sub_df.to_csv(\"submission.csv\", index=False)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-06T16:30:08.619778Z","iopub.status.idle":"2023-10-06T16:30:08.620455Z","shell.execute_reply.started":"2023-10-06T16:30:08.620217Z","shell.execute_reply":"2023-10-06T16:30:08.62024Z"},"trusted":true,"id":"0vCYEwCu197Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"de_qKcYd197Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fwMy8CH3197Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kS27RGas197Z"},"execution_count":null,"outputs":[]}]}